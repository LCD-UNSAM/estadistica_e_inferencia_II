<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="es" xml:lang="es"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.4.554">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>Estadística a Inferencia II - 1&nbsp; Inferencia Bayesiana</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js" integrity="sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==" crossorigin="anonymous"></script><script src="site_libs/quarto-nav/quarto-nav.js"></script>
<script src="site_libs/quarto-nav/headroom.min.js"></script>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="site_libs/quarto-search/fuse.min.js"></script>
<script src="site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="./">
<link href="./02_Programación_probabilística.html" rel="next">
<link href="./index.html" rel="prev">
<script src="site_libs/quarto-html/quarto.js"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="site_libs/quarto-html/anchor.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" class="quarto-color-scheme" id="quarto-text-highlighting-styles">
<link href="site_libs/quarto-html/quarto-syntax-highlighting-dark.css" rel="prefetch" class="quarto-color-scheme quarto-color-alternate" id="quarto-text-highlighting-styles">
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" class="quarto-color-scheme" id="quarto-bootstrap" data-mode="light">
<link href="site_libs/bootstrap/bootstrap-dark.min.css" rel="prefetch" class="quarto-color-scheme quarto-color-alternate" id="quarto-bootstrap" data-mode="dark">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "Sin resultados",
    "search-matching-documents-text": "documentos encontrados",
    "search-copy-link-title": "Copiar el enlace en la búsqueda",
    "search-hide-matches-text": "Ocultar resultados adicionales",
    "search-more-match-text": "resultado adicional en este documento",
    "search-more-matches-text": "resultados adicionales en este documento",
    "search-clear-button-title": "Borrar",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancelar",
    "search-submit-button-title": "Enviar",
    "search-label": "Buscar"
  }
}</script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" integrity="sha512-c3Nl8+7g4LMSTdrm621y7kf9v3SDPnhxLNhcjFJbKECVnmZHTdo+IRO05sNLTH/D3vA6u1X32ehoLC7WFVdheg==" crossorigin="anonymous"></script>

<script type="application/javascript">define('jquery', [],function() {return window.jQuery;})</script>
<script src="https://unpkg.com/@jupyter-widgets/html-manager@*/dist/embed-amd.js" crossorigin="anonymous"></script>

  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

</head>

<body class="nav-sidebar docked">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Alternar barra lateral" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="./01_Inferencia_Bayesiana.html"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Inferencia Bayesiana</span></a></li></ol></nav>
        <a class="flex-grow-1" role="button" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Alternar barra lateral" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
      <button type="button" class="btn quarto-search-button" aria-label="" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation docked overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header">
    <div class="sidebar-title mb-0 py-0">
      <a href="./">Estadística a Inferencia II</a> 
        <div class="sidebar-tools-main tools-wide">
    <div class="dropdown">
      <a href="" title="github" id="quarto-navigation-tool-dropdown-0" class="quarto-navigation-tool dropdown-toggle px-1" data-bs-toggle="dropdown" aria-expanded="false" aria-label="github"><i class="bi bi-github"></i></a>
      <ul class="dropdown-menu" aria-labelledby="quarto-navigation-tool-dropdown-0">
          <li>
            <a class="dropdown-item sidebar-tools-main-item" href="https://github.com/LCD-UNSAM/estadistica_e_inferencia_II">
            Fuente
            </a>
          </li>
          <li>
            <a class="dropdown-item sidebar-tools-main-item" href="https://github.com/LCD-UNSAM/estadistica_e_inferencia_II/issues/new">
            Reportar errores
            </a>
          </li>
      </ul>
    </div>
  <a href="" class="quarto-color-scheme-toggle quarto-navigation-tool  px-1" onclick="window.quartoToggleColorScheme(); return false;" title="Modo claro/oscuro"><i class="bi"></i></a>
  <a href="" class="quarto-reader-toggle quarto-navigation-tool px-1" onclick="window.quartoToggleReader(); return false;" title="Modo sin distracciones">
  <div class="quarto-reader-toggle-btn">
  <i class="bi"></i>
  </div>
</a>
</div>
    </div>
      </div>
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Buscar"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">‎</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./01_Inferencia_Bayesiana.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Inferencia Bayesiana</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./02_Programación_probabilística.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Programación probabilista</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./03_Modelos_jerárquicos.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Modelado Jerárquico</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./04_Flujo_de_trabajo_bayesiano.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Flujo de trabajo Bayesiano</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./05_GLMS.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Modelos lineales y generalizaciones</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./06_modelos_de_mezcla.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Modelos de mezcla</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./08_BART.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">Árboles de regresión aditivos Bayesianos</span></span></a>
  </div>
</li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Tabla de contenidos</h2>
   
  <ul>
  <li><a href="#el-universo-bayesiano" id="toc-el-universo-bayesiano" class="nav-link active" data-scroll-target="#el-universo-bayesiano"><span class="header-section-number">1.1</span> El universo Bayesiano</a>
  <ul class="collapse">
  <li><a href="#teorema-de-bayes" id="toc-teorema-de-bayes" class="nav-link" data-scroll-target="#teorema-de-bayes"><span class="header-section-number">1.1.1</span> Teorema de Bayes</a></li>
  <li><a href="#el-a-posteriori-como-único-estimador" id="toc-el-a-posteriori-como-único-estimador" class="nav-link" data-scroll-target="#el-a-posteriori-como-único-estimador"><span class="header-section-number">1.1.2</span> El <em>a posteriori</em> como único estimador</a></li>
  <li><a href="#estadística-bayesiana-en-tres-pasos" id="toc-estadística-bayesiana-en-tres-pasos" class="nav-link" data-scroll-target="#estadística-bayesiana-en-tres-pasos"><span class="header-section-number">1.1.3</span> Estadística Bayesiana en tres pasos</a></li>
  </ul></li>
  <li><a href="#inferencia-bayesiana" id="toc-inferencia-bayesiana" class="nav-link" data-scroll-target="#inferencia-bayesiana"><span class="header-section-number">1.2</span> Inferencia Bayesiana</a>
  <ul class="collapse">
  <li><a href="#el-problema-de-la-moneda" id="toc-el-problema-de-la-moneda" class="nav-link" data-scroll-target="#el-problema-de-la-moneda"><span class="header-section-number">1.2.1</span> El problema de la moneda</a></li>
  <li><a href="#definiendo-el-a-priori" id="toc-definiendo-el-a-priori" class="nav-link" data-scroll-target="#definiendo-el-a-priori"><span class="header-section-number">1.2.2</span> Definiendo el <em>a priori</em></a></li>
  <li><a href="#definiendo-el-likelihood" id="toc-definiendo-el-likelihood" class="nav-link" data-scroll-target="#definiendo-el-likelihood"><span class="header-section-number">1.2.3</span> Definiendo el <em>likelihood</em></a></li>
  <li><a href="#obteniendo-el-a-posteriori" id="toc-obteniendo-el-a-posteriori" class="nav-link" data-scroll-target="#obteniendo-el-a-posteriori"><span class="header-section-number">1.2.4</span> Obteniendo el <em>a posteriori</em></a></li>
  <li><a href="#notación-y-visualización-de-modelos-bayesianos" id="toc-notación-y-visualización-de-modelos-bayesianos" class="nav-link" data-scroll-target="#notación-y-visualización-de-modelos-bayesianos"><span class="header-section-number">1.2.5</span> Notación y visualización de modelos Bayesianos</a></li>
  <li><a href="#obteniendo-los-datos" id="toc-obteniendo-los-datos" class="nav-link" data-scroll-target="#obteniendo-los-datos"><span class="header-section-number">1.2.6</span> Obteniendo los datos</a></li>
  <li><a href="#calculando-el-a-posteriori" id="toc-calculando-el-a-posteriori" class="nav-link" data-scroll-target="#calculando-el-a-posteriori"><span class="header-section-number">1.2.7</span> Calculando el <em>a posteriori</em></a></li>
  </ul></li>
  <li><a href="#analizando-los-resultados" id="toc-analizando-los-resultados" class="nav-link" data-scroll-target="#analizando-los-resultados"><span class="header-section-number">1.3</span> Analizando los resultados</a>
  <ul class="collapse">
  <li><a href="#influencia-y-elección-del-a-priori" id="toc-influencia-y-elección-del-a-priori" class="nav-link" data-scroll-target="#influencia-y-elección-del-a-priori"><span class="header-section-number">1.3.1</span> Influencia y elección del <em>a priori</em></a></li>
  <li><a href="#cuantificando-el-peso-del-a-priori" id="toc-cuantificando-el-peso-del-a-priori" class="nav-link" data-scroll-target="#cuantificando-el-peso-del-a-priori"><span class="header-section-number">1.3.2</span> Cuantificando el peso del <em>a priori</em></a></li>
  <li><a href="#resumiendo-el-a-posteriori" id="toc-resumiendo-el-a-posteriori" class="nav-link" data-scroll-target="#resumiendo-el-a-posteriori"><span class="header-section-number">1.3.3</span> Resumiendo el <em>a posteriori</em></a></li>
  </ul></li>
  <li><a href="#distribución-predictivas" id="toc-distribución-predictivas" class="nav-link" data-scroll-target="#distribución-predictivas"><span class="header-section-number">1.4</span> Distribución predictivas</a>
  <ul class="collapse">
  <li><a href="#distribución-predictivas-a-posteriori" id="toc-distribución-predictivas-a-posteriori" class="nav-link" data-scroll-target="#distribución-predictivas-a-posteriori"><span class="header-section-number">1.4.1</span> Distribución predictivas <em>a posteriori</em></a></li>
  <li><a href="#distribución-predictiva-a-priori" id="toc-distribución-predictiva-a-priori" class="nav-link" data-scroll-target="#distribución-predictiva-a-priori"><span class="header-section-number">1.4.2</span> Distribución predictiva <em>a priori</em></a></li>
  <li><a href="#distribución-predictiva-a-priori-y-a-posterior-para-el-problema-de-la-moneda." id="toc-distribución-predictiva-a-priori-y-a-posterior-para-el-problema-de-la-moneda." class="nav-link" data-scroll-target="#distribución-predictiva-a-priori-y-a-posterior-para-el-problema-de-la-moneda."><span class="header-section-number">1.4.3</span> Distribución predictiva <em>a priori</em> y a posterior para el problema de la moneda.</a></li>
  <li><a href="#cuarteto-bayesiano" id="toc-cuarteto-bayesiano" class="nav-link" data-scroll-target="#cuarteto-bayesiano"><span class="header-section-number">1.4.4</span> Cuarteto Bayesiano</a></li>
  </ul></li>
  <li><a href="#ejercicios" id="toc-ejercicios" class="nav-link" data-scroll-target="#ejercicios"><span class="header-section-number">1.5</span> Ejercicios</a></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Inferencia Bayesiana</span></h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  


</header>


<div id="cell-1" class="cell" data-execution_count="1">
<details class="code-fold">
<summary>Mostrar Código</summary>
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1"></a><span class="im">import</span> arviz <span class="im">as</span> az</span>
<span id="cb1-2"><a href="#cb1-2"></a><span class="im">from</span> ipywidgets <span class="im">import</span> interact</span>
<span id="cb1-3"><a href="#cb1-3"></a><span class="im">import</span> ipywidgets <span class="im">as</span> ipyw</span>
<span id="cb1-4"><a href="#cb1-4"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb1-5"><a href="#cb1-5"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb1-6"><a href="#cb1-6"></a><span class="im">import</span> preliz <span class="im">as</span> pz</span></code><button title="Copiar al portapapeles" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<div id="cell-2" class="cell" data-execution_count="2">
<details class="code-fold">
<summary>Mostrar Código</summary>
<div class="sourceCode cell-code" id="cb2"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1"></a>az.style.use(<span class="st">'arviz-doc'</span>)</span></code><button title="Copiar al portapapeles" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<p>Los objetivos de este capítulo son:</p>
<ul>
<li>Revisitar conceptos básicos de estadística Bayesiana
<ul>
<li>Qué implica hacer estadística Bayesiana</li>
<li>Interpretación de probabilidades en estadística Bayesiana</li>
</ul></li>
<li>Entender las distribuciones a priori, a posteriori, a priori predictiva y a posteriori predictiva</li>
</ul>
<section id="el-universo-bayesiano" class="level2" data-number="1.1">
<h2 data-number="1.1" class="anchored" data-anchor-id="el-universo-bayesiano"><span class="header-section-number">1.1</span> El universo Bayesiano</h2>
<p>En este curso aprenderemos sobre una forma de hacer estadística llamada usualmente estadística Bayesiana. El nombre se debe a Thomas Bayes (1702-1761) un ministro presbiteriano, y matemático aficionado, quien derivó por primera vez lo que ahora conocemos como el <strong>teorema de Bayes</strong>, el cual fue publicado (postumanente) en 1763. Sin embargo una de las primeras personas en realmente desarrollar métodos Bayesianos, fue Pierre-Simon Laplace (1749-1827), por lo que tal vez sería un poco más correcto hablar de <em>Estadística Laplaciana</em> y no Bayesiana.</p>
<p>Hay dos ideas centrales que hacen que un método sea Bayesiano:</p>
<ul>
<li>Toda cantidad desconocida es modelada utilizando una distribución de probabilidad de algún tipo.</li>
<li>El teorema de Bayes es usado para actualizar dicha distribución a la luz de los datos.</li>
</ul>
<p>En el universo Bayesiano las cantidades conocidas son consideradas fijas y usualmente les llamamos <strong>datos</strong>. Por el contrario toda cantidad desconocida es considerada como una variable aleatoria y es considerada un <strong>parámetros</strong> dentro de un modelo Bayesiano.</p>
<section id="teorema-de-bayes" class="level3" data-number="1.1.1">
<h3 data-number="1.1.1" class="anchored" data-anchor-id="teorema-de-bayes"><span class="header-section-number">1.1.1</span> Teorema de Bayes</h3>
<p>El teorema de Bayes es una consecuencia directa de la regla del producto, veamos.</p>
<p><span class="math display">\[\begin{align}
p(\theta, Y) = p(\theta \mid Y)\; p(Y) \\
p(\theta, Y) = p(Y \mid \theta)\; p(\theta)
\end{align}\]</span> Dado que los dos términos a la derecha de la igualdad son iguales entre si podemos escribir que:</p>
<p><span class="math display">\[
p(\theta \mid Y) \; p(Y) = p(Y \mid \theta)\; p(\theta)
\]</span></p>
<p>Reordenando llegamos al Teorema de Bayes!</p>
<p><span class="math display">\[
p(\theta \mid Y) = \frac{p(Y \mid \theta) p(\theta)}{p(Y)}
\]</span></p>
<p>El cual también suele ser escrito de la siguiente forma:</p>
<p><span class="math display">\[
\overbrace{p(\theta \mid Y)}^{\text{posterior}} = \frac{\overbrace{p(Y \mid \theta)}^{\text{likelihood}} \overbrace{p(\theta)}^{\text{prior}}}{\underbrace{\int_{\Theta} p(Y \mid \theta) p(\theta) \text{d}\theta}_{\text{likelihood marginal}}}
\]</span></p>
<p>El <strong><em>a priori</em></strong> es la forma de introducir conocimiento previo sobre los valores que pueden tomar los parámetros. A veces cuando no sabemos demasiado se suelen usar <em>a prioris</em> que asignan igual probabilidad a todos los valores de los parámetros, otras veces se puede elegir <em>a prioris</em> que restrinjan los valores de los parámetros a rangos razonables, algo que se conoce como regularización, por ejemplo solo valores positivos. Muchas veces contamos con información mucho más precisa como medidas experimentales previas o límites impuesto por alguna teoría.</p>
<p>El <strong><em>likelihood</em></strong> es la forma de incluir nuestros datos en el análisis. Es una expresión matemática que especifica la plausibilidad de los datos. El <em>likelihood</em> es central tanto en estadística Bayesiana como en estadística no-Bayesiana. A medida que la cantidad de datos aumenta el <em>likelihood</em> tiene cada vez más peso en los resultados, esto explica el porqué a veces los resultados de la estadística Bayesiana y frecuentista coinciden cuando la muestra es <em>grande</em>.</p>
<p>El <strong><em>a posteriori</em></strong> es la distribución de probabilidad para los parámetros. Es la consecuencia lógica de haber usado un conjunto de datos, un <em>likelihood</em> y un <em>a priori</em>. Se lo suele pensar como la versión actualizada del <em>a priori</em>. De hecho un <em>a posteriori</em> puede ser un <em>a priori</em> de un análisis a futuro.</p>
<p>La <strong><em>likelihood marginal</em></strong> (también llamado <em>evidencia</em>) es el likelihood promediado sobre todas los posibles hipótesis (o conjunto de parámetros) <span class="math inline">\(\theta\)</span>, esto es equivalente a <span class="math inline">\(p(Y)\)</span>. En general, la <em>evidencia</em> puede ser vista como una simple constante de normalización que en la mayoría de los problemas prácticos puede (y suele) omitirse. Por lo que el teorema de Bayes suele aparecer escrito como:</p>
<p><span class="math display">\[
p(\theta \mid Y) \propto p(Y \mid \theta) p(\theta)
\]</span></p>
<p>El rol de todos estos términos irá quedando más claro a medida que avancemos.</p>
</section>
<section id="el-a-posteriori-como-único-estimador" class="level3" data-number="1.1.2">
<h3 data-number="1.1.2" class="anchored" data-anchor-id="el-a-posteriori-como-único-estimador"><span class="header-section-number">1.1.2</span> El <em>a posteriori</em> como único estimador</h3>
<p>El <em>a posteriori</em> representa todo lo que sabemos de un problema, dado un modelo y un conjunto de datos. Y por lo tanto cualquier cantidad que nos interese sobre el problema puede deducirse a partir de él. Típicamente esto toma la forma de integrales como la siguiente.</p>
<p><span class="math display">\[
J = \int \varphi(\theta) \ \ p(\theta \mid Y) d\theta
\]</span></p>
<p>Por ejemplo, para calcular la media de <span class="math inline">\(\theta\)</span> deberíamos reemplazar <span class="math inline">\(\varphi(\theta)\)</span>, por <span class="math inline">\(\theta\)</span>:</p>
<p><span class="math display">\[
\bar \theta = \int \theta \ \ p(\theta \mid Y) d\theta
\]</span></p>
<p>Esto no es más que la definición de un promedio pesado, donde cada valor de <span class="math inline">\(\theta\)</span> es <em>pesado</em> según la probabilidad asignada por el <em>a posteriori</em>.</p>
<p>En la práctica, y al usar métodos computacionales como los usados en este curso, estas integrales pueden aproximarse usando sumas.</p>
</section>
<section id="estadística-bayesiana-en-tres-pasos" class="level3" data-number="1.1.3">
<h3 data-number="1.1.3" class="anchored" data-anchor-id="estadística-bayesiana-en-tres-pasos"><span class="header-section-number">1.1.3</span> Estadística Bayesiana en tres pasos</h3>
<p>El teorema de Bayes es el único estimador usado en estadística Bayesiana. Por lo que conceptualmente la estadística Bayesiana resulta muy simple. Según George Box y <a href="https://www.crcpress.com/Bayesian-Data-Analysis-Third-Edition/Gelman-Carlin-Stern-Dunson-Vehtari-Rubin/p/book/9781439840955">Andrew Gelman et al.&nbsp;(2013)</a> la estadística Bayesiana se reduce a tres pasos:</p>
<ol type="1">
<li><p><strong>Crear un modelo probabilístico</strong>. Los modelos probabilísticos son <em>historias</em> que dan cuenta de como se generan los datos observados (o por observar). Los modelos se expresan usando distribuciones de probabilidad.</p></li>
<li><p><strong>Condicionar el modelo a los datos observados a fin de obtener el <em>a posteriori</em></strong>. Usando el teorema de Bayes se actualizan las probabilidades asignadas <em>a priori</em> de acuerdo a los datos observados obteniéndose las probabilidades <em>a posteriori</em>.</p></li>
<li><p><strong>Criticar el ajuste del modelo generado a los datos y evaluar las consecuencias del modelo</strong>. Se puede demostrar que dada la información previa y los datos observados no existe otro mecanismo capaz de generar una <em>mejor</em> inferencia que la estadística Bayesiana. Esto parece maravilloso, pero hay un problema, sólo es cierto si se asumen que los datos y el modelo son correctos. En la práctica, los datos pueden contener errores y los modelos son <em>a duras penas</em> aproximaciones de fenómenos <em>reales</em>. Por lo tanto es necesario realizar varias evaluaciones, incluyendo si las predicciones generadas por el modelo se ajustan a los datos observados, si las conclusiones obtenidas tienen sentido dado el marco conceptual en el que uno trabaja, la sensibilidad de los resultados a los <em>detalles</em> del modelo (sobre todo a detalles para los cuales no tenemos demasiada información), etc. Además, es posible que realizar inferencia Bayesiana sea demasiado costosa en la práctica por lo que sea conveniente realizar aproximaciones.</p></li>
</ol>
</section>
</section>
<section id="inferencia-bayesiana" class="level2" data-number="1.2">
<h2 data-number="1.2" class="anchored" data-anchor-id="inferencia-bayesiana"><span class="header-section-number">1.2</span> Inferencia Bayesiana</h2>
<p>En la práctica la mayoría de los modelos tendrán más de un parámetro, pero empecemos con un modelo con un solo parámetro.</p>
<section id="el-problema-de-la-moneda" class="level3" data-number="1.2.1">
<h3 data-number="1.2.1" class="anchored" data-anchor-id="el-problema-de-la-moneda"><span class="header-section-number">1.2.1</span> El problema de la moneda</h3>
<p>A juzgar por la cantidad de ejemplos sobre monedas arrojadas al aires en libros de estadística y probabilidad, pareciera que las monedas son uno de los objetos de estudio centrales de estas disciplinas.</p>
<p>Una de las razones detrás de la ubiquidad de este ejemplo es que las monedas son objetos familiares que facilitan discutir conceptos que de otra forma podrían sonar demasiado abstractos. De todas formas quizá la razón más importante sea que el problema puede ser modelado de forma simple y que muchos problemas <em>reales</em> son conceptualmente similares, de hecho cualquier problema en donde obtengamos resultados binarios (0/1, enfermo/sano, spam/no-spam, etc) puede ser pensado como si estuviéramos hablando de monedas. En definitiva el modelo que veremos a continuación (ejemplificado con monedas) sirve para cualquier situación en la cual los datos observados solo pueden tomar dos valores mutuamente excluyentes. Debido a que estos valores son nominales y son dos, a este modelo se le llama binomial.</p>
<p>En el siguiente ejemplo trataremos de determinar el grado en que una moneda está sesgada. En general cuando se habla de sesgo se hace referencia a la desviación de algún valor (por ejemplo, igual proporción de caras y cecas), pero aquí usaremos el termino <em>sesgo</em> de forma más general. Diremos que el sesgo es un valor en el intervalo [0, 1], siendo 0 para una moneda que siempre cae ceca y 1 para una moneda que siempre cae cara y lo representaremos con la variable <span class="math inline">\(\theta\)</span>. A fin de cuantificar <span class="math inline">\(\theta\)</span> arrojaremos una moneda al aire repetidas veces, por practicidad arrojaremos la moneda de forma computacional (¡pero nada nos impide hacerlo manualmente!). Llevaremos registro del resultado en la variable <span class="math inline">\(y\)</span>. Siendo <span class="math inline">\(y\)</span> la cantidad de caras obtenidas en un experimento.</p>
<p>Habiendo definido nuestro problema debemos expresarlo en términos del teorema de Bayes,</p>
<p><span class="math display">\[
p(\theta \mid Y) \propto p(Y \mid  \theta) p(\theta)
\]</span></p>
<p>Donde, como dijimos <span class="math inline">\(\theta = 1\)</span> quiere decir 100% cara y <span class="math inline">\(\theta = 0\)</span> 100% ceca.</p>
<p>Ahora sólo restar reemplazar los dos términos a la derecha de la igualdad, el <em>a priori</em> y el <em>likelihood</em>, por distribuciones de probabilidad <em>adecuadas</em> y luego multiplicarlas para obtener el término a la izquierda, el <em>a posteriori</em>. Como es la primera vez que haremos ésto, lo haremos paso a paso y analíticamente. En el próximo capítulo veremos cómo hacerlo computacionalmente.</p>
</section>
<section id="definiendo-el-a-priori" class="level3" data-number="1.2.2">
<h3 data-number="1.2.2" class="anchored" data-anchor-id="definiendo-el-a-priori"><span class="header-section-number">1.2.2</span> Definiendo el <em>a priori</em></h3>
<p>El <em>a priori</em> lo modelaremos usando una distribución Beta, que es una distribución muy usada en estadística Bayesiana. La <span class="math inline">\(pdf\)</span> de esta distribución es:</p>
<p><span class="math display">\[
p(\theta)= \frac{\Gamma(\alpha+\beta)}{\Gamma(\alpha)\Gamma(\beta)}\, \theta^{\alpha-1}(1-\theta)^{\beta-1}
\]</span></p>
<p>El primer término es una constante de normalización. Por suerte para nuestro problema nos basta con establecer una proporcionalidad, por lo que podemos simplificar esta expresión y escribir la distribución Beta de la siguiente forma.</p>
<p><span class="math display">\[
p(\theta) \propto  \theta^{\alpha-1}(1-\theta)^{\beta-1}
\]</span></p>
<p>Hay varias razones para usar una distribución Beta para este y otros problemas:</p>
<ul>
<li>La distribución Beta varía entre 0 y 1, de igual forma que lo hace <span class="math inline">\(\theta\)</span> en nuestro modelo.</li>
<li>Esta distribución combinada con la que elegiremos como <em>likelihood</em> (ver más adelante), nos permitirá resolver el problema de forma analítica.</li>
<li>Es una distribución versátil para expresar distintas situaciones.</li>
</ul>
<p>Respecto al último punto, veamos un ejemplo. Supongamos que el experimento de la moneda es realizado por tres personas. Una de ellas dice no saber nada de la moneda por lo tanto <em>a priori</em> todos los valores de <span class="math inline">\(\theta\)</span> son igualmente probables. La segunda persona desconfía de la moneda, ya que sospecha que es una moneda trucada, por lo tanto considera que está sesgada, pero no sabe para cual de las dos opciones. Por último, la tercer persona asegura que lo más probable es que <span class="math inline">\(\theta\)</span> tome un valor alrededor de 0.5 ya que así lo indican experimentos previos y análisis teóricos sobre tiradas de monedas. Todas estas situaciones pueden ser modeladas por la distribución Beta, como se ve a continuación.</p>
<div id="cell-11" class="cell" data-nbpresent="{&quot;id&quot;:&quot;0862c2ab-669f-4578-b565-33fbf856f1e5&quot;}" data-scrolled="true" data-execution_count="3">
<div class="sourceCode cell-code" id="cb3"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1"></a>_, axes <span class="op">=</span> plt.subplots(<span class="dv">1</span>, <span class="dv">3</span>, figsize<span class="op">=</span>(<span class="dv">12</span>, <span class="dv">3</span>), sharey<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb3-2"><a href="#cb3-2"></a></span>
<span id="cb3-3"><a href="#cb3-3"></a>params <span class="op">=</span> [(<span class="dv">1</span>, <span class="dv">1</span>), (<span class="fl">0.5</span>, <span class="fl">0.5</span>), (<span class="dv">20</span>, <span class="dv">20</span>)]</span>
<span id="cb3-4"><a href="#cb3-4"></a></span>
<span id="cb3-5"><a href="#cb3-5"></a><span class="cf">for</span> (a, b), ax  <span class="kw">in</span> <span class="bu">zip</span>(params, axes):</span>
<span id="cb3-6"><a href="#cb3-6"></a>    ax <span class="op">=</span> pz.Beta(a, b).plot_pdf(ax<span class="op">=</span>ax, legend<span class="op">=</span><span class="st">"title"</span>)</span>
<span id="cb3-7"><a href="#cb3-7"></a>    ax.set_ylim(<span class="dv">0</span>, <span class="dv">7</span>)</span></code><button title="Copiar al portapapeles" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="01_Inferencia_Bayesiana_files/figure-html/cell-4-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<div id="cell-12" class="cell" data-execution_count="4">
<div class="sourceCode cell-code" id="cb4"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1"></a>pz.Beta().plot_interactive(xy_lim<span class="op">=</span>(<span class="va">None</span>, <span class="va">None</span>, <span class="va">None</span>, <span class="dv">10</span>))</span></code><button title="Copiar al portapapeles" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="4">
<script type="application/vnd.jupyter.widget-view+json">
{"model_id":"02faf8a3a2724559aeb00ac0ca6e6e63","version_major":2,"version_minor":0,"quarto_mimetype":"application/vnd.jupyter.widget-view+json"}
</script>
</div>
</div>
</section>
<section id="definiendo-el-likelihood" class="level3" data-number="1.2.3">
<h3 data-number="1.2.3" class="anchored" data-anchor-id="definiendo-el-likelihood"><span class="header-section-number">1.2.3</span> Definiendo el <em>likelihood</em></h3>
<p>Habiendo definido el <em>a priori</em> veamos ahora el likelihood. Asumiendo que el resultado obtenido al arrojar una moneda no influye en el resultado de posteriores experimentos (es decir los experimentos son independientes entre sí) es razonable utilizar como likelihood la distribución binomial.</p>
<p><span class="math display">\[
p(y \mid \theta) = \frac{N!}{y!(N-y)!} \theta^y (1 - \theta)^{N−y}
\]</span></p>
<p>Donde N es la cantidad total de experimentos (monedas arrojadas al aire) e <span class="math inline">\(y\)</span> es la cantidad de caras obtenidas. A los fines prácticos podríamos simplificar la igualdad anterior y convertirla en una proporcionalidad, eliminando el término <span class="math inline">\(\frac{N!}{y!(N-y)!}\)</span> ya que ese término no depende de <span class="math inline">\(\theta\)</span> que es lo que nos interesa averiguar. Por lo que podríamos establecer que:</p>
<p><span class="math display">\[
p(y \mid \theta) \propto \theta^y (1 - \theta)^{N−y}
\]</span></p>
<p>La elección de esta distribución para modelar nuestro problema es razonable ya que <span class="math inline">\(\theta\)</span> es la chance de obtener una cara al arrojar una moneda y ese hecho ha ocurrido <span class="math inline">\(y\)</span> veces, de la misma forma <span class="math inline">\(1-\theta\)</span> es la chance de obtener ceca lo cual ha sido observado <span class="math inline">\(N-y\)</span> veces.</p>
<div id="cell-14" class="cell" data-nbpresent="{&quot;id&quot;:&quot;8f7f8e91-ae8d-4f01-b29b-0f7e2bb0d8b1&quot;}" data-execution_count="5">
<div class="sourceCode cell-code" id="cb5"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1"></a>pz.Binomial(<span class="dv">1</span>, <span class="fl">0.5</span>).plot_interactive(pointinterval<span class="op">=</span><span class="va">False</span>, xy_lim<span class="op">=</span>(<span class="va">None</span>, <span class="va">None</span>, <span class="va">None</span>, <span class="dv">1</span>))</span></code><button title="Copiar al portapapeles" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="5">
<script type="application/vnd.jupyter.widget-view+json">
{"model_id":"f16743ebb0414dcfb5d3c2cbd428f18d","version_major":2,"version_minor":0,"quarto_mimetype":"application/vnd.jupyter.widget-view+json"}
</script>
</div>
</div>
</section>
<section id="obteniendo-el-a-posteriori" class="level3" data-number="1.2.4">
<h3 data-number="1.2.4" class="anchored" data-anchor-id="obteniendo-el-a-posteriori"><span class="header-section-number">1.2.4</span> Obteniendo el <em>a posteriori</em></h3>
<p>Se puede demostrar que siempre que usemos como <em>prior</em> una función Beta y como <em>likelihood</em> una distribución binomial obtendremos como resultado una distribución <em>a posteriori</em>, la cual será una Beta con los siguientes parámetros:</p>
<p><span class="math display">\[
p(\theta \mid y) = \operatorname{Beta}(\alpha_{a priori} + y, \beta_{a priori} + N - y)
\]</span></p>
<p>Veamos de donde surge este resultado, según el teorema de Bayes la distribución <em>a posteriori</em> es el producto del <em>likelihood</em> y la distribución <em>a priori</em>.</p>
<p><span class="math display">\[
p(\theta \mid y) = p(y \mid \theta) p(\theta) * c
\]</span></p>
<p>Por lo tanto, en nuestro caso tendremos que:</p>
<p><span class="math display">\[
p(\theta \mid y) \propto \underbrace{{\color{gray}{\frac{N!}{y!(N-y)!}}} \theta^y (1 - \theta)^{N−y}}_{\text{likelihood}} \underbrace{{\color{gray}{\frac{\Gamma(\alpha+\beta)}{\Gamma(\alpha)\Gamma(\beta)}}}\, \theta^{\alpha-1}(1-\theta)^{\beta-1}}_{\text{prior}}
\]</span></p>
<p>Si omitimos las constantes de normalizando del likelihood y prior, obtenemos que el <em>a posteriori</em> es:</p>
<p><span class="math display">\[
p(\theta \mid y) \propto \theta^{\alpha-1+y}(1-\theta)^{\beta-1+N−y}
\]</span></p>
<p>Podemos ver que la expresión a la derecha de la proporcionalidad tiene la misma forma funcional (sin considerar la constante de proporcionalidad) que una distribución Beta. Como la distribución a posterior debe intergrar a 1 entonces podemos escribir.</p>
<p><span class="math display">\[
p(\theta \mid y) = \frac{\Gamma(\alpha + y +\beta + N -y)}{\Gamma(\alpha + y)\Gamma(\beta + N - y)}\, \theta^{\alpha-1+y}(1-\theta)^{\beta-1+n-y}
\]</span></p>
<p>Lo que es equivalente a una distribución Beta con parámetros <span class="math inline">\(\alpha_{\text{a posteriori}} = \alpha_{\text{a priori}} + y \quad \beta_{\text{a posteriori}} = \beta_{\text{a priori}} + N - y\)</span>.</p>
<p>Cuando se cumple que para un cierto <em>likelihood</em> la forma funcional del <em>a priori</em> y la del <em>a posteriori</em> coinciden se dice que el <em>a priori</em> es conjugado con el <em>likelihood</em>. Históricamente los problemas en estadística Bayesiana estuvieron restringidos al uso de <em>a prioris</em> conjugados, ya que estos garantizan la tratabilidad matemática del problema, es decir garantizan que es posible obtener una expresión analítica para nuestro problema. En el próximo capítulo veremos técnicas computacionales modernas que permiten calcular la distribución <em>a posteriori</em> incluso cuando no se usan <em>a prioris</em> conjugados. Estas técnicas computacionales han permitido el resurgimiento de la estadística Bayesiana en las últimas décadas.</p>
</section>
<section id="notación-y-visualización-de-modelos-bayesianos" class="level3" data-number="1.2.5">
<h3 data-number="1.2.5" class="anchored" data-anchor-id="notación-y-visualización-de-modelos-bayesianos"><span class="header-section-number">1.2.5</span> Notación y visualización de modelos Bayesianos</h3>
<p>Para representar modelos en estadística Bayesiana (y en probabilidad en general) se suele utilizar la siguiente notación</p>
<p><span class="math display">\[
\begin{align}
\theta \sim &amp; \operatorname{Beta}(\alpha, \beta) \\
Y \sim &amp; \operatorname{Bin}(n=1, p=\theta)
\end{align}
\]</span></p>
<p>El símbolo <span class="math inline">\(\sim\)</span> indica que la variable a la izquierda se distribuye según la distribución a la derecha. Entonces podríamos decir que <span class="math inline">\(\mathbf{\theta}\)</span> es una variable aleatoria con distribución <span class="math inline">\(\operatorname{Beta}\)</span>, y que <span class="math inline">\(\operatorname{Beta}\)</span> está definida por los parámetros <span class="math inline">\(\alpha\)</span> y <span class="math inline">\(\beta\)</span>, este es nuestro <em>a priori</em>. En la siguiente línea tenemos el <em>likelihood</em> el cual está definido por una distribución binomial con parámetros <span class="math inline">\(n=1\)</span> y <span class="math inline">\(p=\theta\)</span>.</p>
<p>Gráficamente esto se puede representar usando los diagramas de Kruschke:</p>
<center>
<p><img src="img/modelo_1_moneda.png" width="400"></p>
<p>En el primer nivel (de arriba hacia abajo) se observa el <em>a priori</em>, luego el likelihood, y por último los datos. Las flechas indican la vinculación entre las partes del modelo y el signo <span class="math inline">\(\sim\)</span> la naturaleza estocástica de las variables.</p>
</center></section>
<section id="obteniendo-los-datos" class="level3" data-number="1.2.6">
<h3 data-number="1.2.6" class="anchored" data-anchor-id="obteniendo-los-datos"><span class="header-section-number">1.2.6</span> Obteniendo los datos</h3>
<p>Bien, ahora que sabemos cómo calcular el <em>a posteriori</em>, lo único que resta es conseguir los datos. En este ejemplo los datos son sintéticos, es decir los obtuve computacionalmente mediante un generador de números (pseudo)aleatorios, pero bien podrían haber surgido de un experimento con una moneda <em>real</em>.</p>
</section>
<section id="calculando-el-a-posteriori" class="level3" data-number="1.2.7">
<h3 data-number="1.2.7" class="anchored" data-anchor-id="calculando-el-a-posteriori"><span class="header-section-number">1.2.7</span> Calculando el <em>a posteriori</em></h3>
<p>En el próximo capítulo veremos cómo usar métodos computacionales para computar un <em>a posteriori</em> sin necesidad de derivarlo analíticamente. Esto es lo que haremos para resolver el resto de los problemas del curso. Pero dado que ya nos tomamos el trabajo de derivar analíticamente la expresión para el <em>a posteriori</em> vamos a usar esa expresión. Si miran el código de la siguiente celda verán que la mayoría de las lineas se encargan de dibujar los resultados y no de calcularlos. El cálculo del <em>a posteriori</em> ocurre en la línea 20. Cada una de estas lineas computa el <em>a posteriori</em> para cada uno de los <em>a prioris</em> que vimos antes. El cálculo es simple, tan solo se computa el valor del <em>a posteriori</em> (usando la función <em>pdf</em> de la distribución Beta provista por PreliZ) para 2000 puntos igualmente espaciados entre 0 y 1 (linea 9). El loop que empieza en la linea 11 se debe a que exploraremos cómo cambian las distribuciones <em>a posteriori</em> para distinta cantidad de datos (<em>n_intentos</em>). Con un círculo negro de contorno blanco se indica el valor real de <span class="math inline">\(\theta\)</span>, valor que por supuesto es desconocido en una situación real, pero conocido para mí, ya que los datos son sintéticos.</p>
<div id="cell-19" class="cell" data-nbpresent="{&quot;id&quot;:&quot;bf1b519c-69dd-42d5-b637-80ef31d70d7f&quot;}" data-execution_count="6">
<div class="sourceCode cell-code" id="cb6"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">12</span>, <span class="dv">9</span>))</span>
<span id="cb6-2"><a href="#cb6-2"></a></span>
<span id="cb6-3"><a href="#cb6-3"></a>n_trials <span class="op">=</span> [<span class="dv">0</span>, <span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">3</span>, <span class="dv">4</span>, <span class="dv">8</span>, <span class="dv">16</span>, <span class="dv">32</span>, <span class="dv">50</span>, <span class="dv">150</span>]</span>
<span id="cb6-4"><a href="#cb6-4"></a>data <span class="op">=</span> [<span class="dv">0</span>, <span class="dv">1</span>, <span class="dv">1</span>, <span class="dv">1</span>, <span class="dv">1</span>, <span class="dv">4</span>, <span class="dv">6</span>, <span class="dv">9</span>, <span class="dv">13</span>, <span class="dv">48</span>]</span>
<span id="cb6-5"><a href="#cb6-5"></a>theta_real <span class="op">=</span> <span class="fl">0.35</span></span>
<span id="cb6-6"><a href="#cb6-6"></a></span>
<span id="cb6-7"><a href="#cb6-7"></a>beta_params <span class="op">=</span> [(<span class="dv">1</span>, <span class="dv">1</span>), (<span class="fl">0.5</span>, <span class="fl">0.5</span>), (<span class="dv">20</span>, <span class="dv">20</span>)]</span>
<span id="cb6-8"><a href="#cb6-8"></a>dist <span class="op">=</span> pz.Beta</span>
<span id="cb6-9"><a href="#cb6-9"></a>x <span class="op">=</span> np.linspace(<span class="dv">0</span>, <span class="dv">1</span>, <span class="dv">2000</span>)</span>
<span id="cb6-10"><a href="#cb6-10"></a></span>
<span id="cb6-11"><a href="#cb6-11"></a><span class="cf">for</span> idx, N <span class="kw">in</span> <span class="bu">enumerate</span>(n_trials):</span>
<span id="cb6-12"><a href="#cb6-12"></a>    <span class="cf">if</span> idx <span class="op">==</span> <span class="dv">0</span>:</span>
<span id="cb6-13"><a href="#cb6-13"></a>        plt.subplot(<span class="dv">4</span>, <span class="dv">3</span>, <span class="dv">2</span>)</span>
<span id="cb6-14"><a href="#cb6-14"></a>        plt.xlabel(<span class="st">'θ'</span>)</span>
<span id="cb6-15"><a href="#cb6-15"></a>    <span class="cf">else</span>:</span>
<span id="cb6-16"><a href="#cb6-16"></a>        plt.subplot(<span class="dv">4</span>, <span class="dv">3</span>, idx<span class="op">+</span><span class="dv">3</span>)</span>
<span id="cb6-17"><a href="#cb6-17"></a>        plt.xticks([])</span>
<span id="cb6-18"><a href="#cb6-18"></a>    y <span class="op">=</span> data[idx]</span>
<span id="cb6-19"><a href="#cb6-19"></a>    <span class="cf">for</span> (a_prior, b_prior) <span class="kw">in</span> beta_params:</span>
<span id="cb6-20"><a href="#cb6-20"></a>        posterior <span class="op">=</span> dist(a_prior <span class="op">+</span> y, b_prior <span class="op">+</span> N <span class="op">-</span> y).pdf(x)</span>
<span id="cb6-21"><a href="#cb6-21"></a>        plt.fill_between(x, <span class="dv">0</span>, posterior, alpha<span class="op">=</span><span class="fl">0.7</span>)</span>
<span id="cb6-22"><a href="#cb6-22"></a></span>
<span id="cb6-23"><a href="#cb6-23"></a>    plt.plot(theta_real, <span class="dv">0</span>, ms<span class="op">=</span><span class="dv">9</span>, marker<span class="op">=</span><span class="st">'o'</span>, mec<span class="op">=</span><span class="st">'w'</span>, mfc<span class="op">=</span><span class="st">'k'</span>)</span>
<span id="cb6-24"><a href="#cb6-24"></a>    plt.plot(<span class="dv">0</span>, <span class="dv">0</span>, label<span class="op">=</span><span class="ss">f'</span><span class="sc">{</span>N<span class="sc">:4d}</span><span class="ss"> experimentos</span><span class="ch">\n</span><span class="sc">{</span>y<span class="sc">:4d}</span><span class="ss"> caras'</span>, alpha<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb6-25"><a href="#cb6-25"></a>    plt.xlim(<span class="dv">0</span>, <span class="dv">1</span>)</span>
<span id="cb6-26"><a href="#cb6-26"></a>    plt.ylim(<span class="dv">0</span>, <span class="dv">12</span>)</span>
<span id="cb6-27"><a href="#cb6-27"></a>    plt.legend()</span>
<span id="cb6-28"><a href="#cb6-28"></a>    plt.yticks([])</span></code><button title="Copiar al portapapeles" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stderr">
<pre><code>/home/osvaldo/anaconda3/envs/EI2/lib/python3.11/site-packages/numba/np/ufunc/dufunc.py:287: RuntimeWarning: divide by zero encountered in nb_logpdf
  return super().__call__(*args, **kws)</code></pre>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="01_Inferencia_Bayesiana_files/figure-html/cell-7-output-2.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
</section>
</section>
<section id="analizando-los-resultados" class="level2" data-number="1.3">
<h2 data-number="1.3" class="anchored" data-anchor-id="analizando-los-resultados"><span class="header-section-number">1.3</span> Analizando los resultados</h2>
<p>La primer figura del panel muestra los <em>a priori</em>, nuestra estimación de <span class="math inline">\(\theta\)</span> dado que no hemos realizado ningún experimento. Las sucesivas nueve figuras muestran las distribuciones <em>a posteriori</em> y se indica la cantidad de experimentos y de caras obtenidas. Además se puede ver un círculo negro de contorno blanco en 0.35, la cual representa el valor verdadero de <span class="math inline">\(\theta\)</span>. Por supuesto que en problemas reales este valor es desconocido.</p>
<p>Este ejemplo es realmente ilustrativo en varios aspectos.</p>
<ul>
<li>El resultado de un análisis Bayesiano NO es un solo valor, si no una distribución (<em>a posteriori</em>) de los valores plausibles de los parámetros (dado los datos y el modelo).</li>
<li>La dispersión o ancho de las curvas es una medida de la incertidumbre sobre los valores.</li>
<li>El valor más probable viene dado por la moda de la distribución (el <em>pico</em> de la distribución).</li>
<li>Aún cuando <span class="math inline">\(\frac{2}{1} = \frac{8}{4}\)</span> son numéricamente iguales tenemos menor incertidumbre en un resultado cuando el número de experimentos es mayor.</li>
<li>Dada una cantidad <em>suficiente</em> de datos los resultados tienden a converger sin importar el <em>a priori</em> usado.</li>
<li>La rapidez con la que los resultados convergen varía. En este ejemplo las curvas azul y turquesa parecen converger con tan solo 8 experimentos, pero se necesitan más de 50 experimentos para que las tres curvas se muestren similares. Aún con 150 experimentos se observan ligeras diferencias.</li>
<li>Partiendo de los <em>a priori</em> uniforme (azul) o <em>sesgado</em> (turquesa) y habiendo realizado un solo experimento y observado una sola cara, lo más razonable es pensar que estamos frente a una moneda con dos caras!</li>
<li>La situación cambia drásticamente al ver por primera vez una moneda caer ceca. Ahora lo más probable (dado cualquiera de los tres <em>a prioris</em>) es inferir que <span class="math inline">\(\theta=0.5\)</span>. Los valores de <span class="math inline">\(\theta\)</span> exactamente 0 o 1 se vuelven imposibles.</li>
<li>El <em>a priori</em> naranja es más informativo que los otros dos (la distribución esta más concentrada), por ello se requiere de un número mas grande de experimentos para “moverlo”.</li>
<li>El <em>a priori</em> uniforme (azul) es lo que se conoce como no informativo. El resultado de un análisis Bayesiano usando un <em>a priori</em> no-informativos en general coinciden con los resultados de análisis frecuentistas (en este caso el valor esperado de <span class="math inline">\(\theta = \frac{y}{N}\)</span>).</li>
</ul>
<section id="influencia-y-elección-del-a-priori" class="level3" data-number="1.3.1">
<h3 data-number="1.3.1" class="anchored" data-anchor-id="influencia-y-elección-del-a-priori"><span class="header-section-number">1.3.1</span> Influencia y elección del <em>a priori</em></h3>
<p>De los ejemplos anteriores debería quedar claro que los <em>a priori</em> influencian los resultados de nuestros cálculos. Esto tiene total sentido si no fuese así no haría falta incluirlos en el análisis y todo sería más simple (aunque nos perderíamos la oportunidad de usar información previa). De los ejemplos anteriores también debería quedar claro que a medida que aumentan los datos (como las tiradas de monedas) los resultados son cada vez menos sensibles al <em>a priori</em>. De hecho, para una cantidad infinita de datos el <em>a priori</em> no tiene ningún efecto. Exactamente cuantos datos son necesarios para que el efecto del <em>a priori</em> sea despreciable varía según el problema y los modelos usados. En el ejemplo de la moneda se puede ver que 50 experimentos bastan para hacer que dos de los resultados sean prácticamente indistinguibles, pero hacen falta más de 150 experimentos para que los 3 resultados se vuelvan <em>practicamente</em> independientes del <em>a priori</em>. Esto es así por que los dos primeros <em>a prioris</em> son relativamente <em>planos</em>, mientras que el tercer <em>a priori</em> concentra casi toda la probabilidad en una región relativamente pequeña. El tercer a priori no solo considera que el valor más probable de <span class="math inline">\(\theta\)</span> es 0.5, si no que considera que la mayoría de los otros valores son muy poco probables. ¿Cómo cambiarían los resultados si hubiéramos usado como <em>a priori</em> <span class="math inline">\(\operatorname{Beta}(\alpha=2, \beta=2)\)</span>?</p>
<p>La elección de los <em>a priori</em> puede poner nervioso a quienes se inician en el análisis Bayesiano (o a los detractores de este paradigma). ¡El temor es que los <em>a prioris</em> censuren a los datos y no les permitan <em>hablar por sí mismos</em>! Eso está muy bien, pero el punto es que los datos no saben hablar, con suerte murmuran. Los datos solo tienen sentido a la luz de los modelos (matemáticos y mentales) usados para interpretarlos, y los <em>a prioris</em> son parte de esos modelos.</p>
<p>Hay quienes prefieren usar <em>a priori</em> no-informativos (también conocidos como <em>a priori</em> planos, vagos, o difusos). Estos <em>a priori</em> aportan la menor cantidad posible de información y por lo tanto tienen el menor impacto posible en el análisis. Si bien es posible usarlos, en general hay razones prácticas para no preferirlos. En este curso usaremos <em>a priori ligeramente informativos</em> siguendo las recomendaciones de Gelman, McElreath, Kruschke, y otros. En muchos problemas sabemos al menos algo de los valores posibles que pueden tomar nuestros parámetros, por ejemplo que solo pueden ser positivos, o que están restringidos a sumar 1 o el rango aproximado, etc. En esos casos podemos usar <em>a prioris</em> que introduzcan esta <em>ligera</em> información. En estos casos podemos pensar que la función del <em>a priori</em> es la de mantener las inferencias dentro de límites razonables. Estos <em>a priori</em> se suelen llamar regularizadores.</p>
<p>Por supuesto que también es posible usar <em>a prioris informativos</em> (o <em>fuertes</em>). Hacer esto es razonable solo si contamos con información previa confiable. Esto puede ser ventajoso en casos en que los datos contengan poca información sobre el problema. Si la información no viene por el <em>likelihood</em> (datos), entonces puede venir por el <em>a priori</em>. A modo de ejemplo, en bioinformática estructural es común usar toda la información previa posible (de forma Bayesiana y no-Bayesiana) para resolver problemas. Esto es posible por la existencia de bases de datos que almacenan los resultados de cientos o miles experimentos realizados a lo largo de décadas de esfuerzo (¡No usar esta información sería casi absurdo!). En resumen, si contás con información confiable no hay razón para descartarla, menos si el <em>argumento</em> es algo relacionado con pretender ser <em>objetivo</em> (¡No hay objetividad en negar lo que se sabe!).</p>
<p>Hasta ahora hemos visto que es posible clasificar, aunque sea de forma vaga o aproximada, a los <em>a priori</em> en función de la información que contienen. Pero saber esta clasificación no necesariamente hace las cosas más simples a la hora de elegir un <em>a priori</em>. ¿Acaso no sería mejor eliminar los <em>a prioris</em> de nuestro análisis? Eso haría el asunto mucho mas simple. Bueno, el punto es que desde una perspectiva Bayesiana todos los modelos tienen <em>a prioris</em>, aun cuando no sean explícitos. De hecho muchos resultados de la estadística frecuentista pueden considerarse casos especiales de modelos Bayesianos usando <em>a prioris planos</em>. Volviendo a la figura anterior se puede ver que la moda del <em>a posteriori</em> para la curva azul. Coincide con la estimación (puntual) frecuentista para el valor de <span class="math inline">\(\theta\)</span></p>
<p><span class="math display">\[
\hat \theta = {{y} \over {N}}
\]</span></p>
<p>Notar que <span class="math inline">\(\hat \theta\)</span> es una estimación puntual (un número) y no una distribución.</p>
<p>Este ejemplo nos muestra que no es posible hacer análisis estadísticos y sacarse los <em>a prioris</em> de encima. Un posible corolario es que es más flexible y transparente especificar los <em>a prioris</em> de forma explícita que esconderlos bajo la cama. Al hacerlo ganamos mayor control sobre nuestro modelo, mayor transparencia y por el mismo precio la estimación de la incertidumbre con la que se estima cada parámetro.</p>
<p>Por último, hay que recordar que el modelado estadístico (como otras formas de modelado) es un proceso iterativo e interactivo. Nada nos impide usar más de un <em>a priori</em> (o un likelihood) si así lo quisiéramos. Una parte importante del modelado es la de cuestionar los supuestos y los <em>a prioris</em> son simplemente un tipo de supuestos (como lo son los <em>likelihoods</em>). Si tuviéramos más de un <em>a priori</em> razonable podríamos realizar un <em>análisis de sensibilidad</em>, es decir evaluar como cambian los resultados con los <em>a prioris</em>, podríamos llegar a la conclusión que para un rango amplio de <em>a prioris</em> ¡los resultados no varían! Más adelante veremos varias herramientas para comparar distintos modelos.</p>
<p>Dado que los <em>a prioris</em> tienen un papel central en la estadística Bayesiana, seguiremos discutiéndolos a medida que vayamos viendo problemas concretos. Por lo que si esta discusión no ha aclarado todas tus dudas y seguís algo confundido, mejor mantener la calma y no preocuparse demasiado, este tema ha sido motivo de discusión y confusión durante décadas ¡y la discusión todavía continua!</p>
</section>
<section id="cuantificando-el-peso-del-a-priori" class="level3" data-number="1.3.2">
<h3 data-number="1.3.2" class="anchored" data-anchor-id="cuantificando-el-peso-del-a-priori"><span class="header-section-number">1.3.2</span> Cuantificando el peso del <em>a priori</em></h3>
<p>En general la distribución más familiar para la mayoría de las personas es la distribución Gaussiana, como esta distribución está definida por dos parámetros, la media y la dispersión de ese valor medio, suele resultarnos <em>natural</em> pensar las distribuciones en esos términos. Además, resulta relativamente sencillo (o al menos más sencillo que otras alternativas) pensar en un valor medio y una desviación o ruido simétrico alrededor de esa media.</p>
<p>Si queremos expresar la distribución Beta en función de la media y la dispersión podemos hacerlo de la siguiente forma:</p>
<p><span class="math display">\[\begin{align}
\alpha &amp;= \mu \kappa \\
\beta &amp;= (1 - \mu) \kappa
\end{align}\]</span></p>
<p>donde <span class="math inline">\(\mu\)</span> es la media y <span class="math inline">\(\kappa\)</span> es un parámetro llamado concentración. Por ejemplo si <span class="math inline">\(\mu=0.5\)</span> y <span class="math inline">\(\kappa=40\)</span>, tenemos que:</p>
<p><span class="math display">\[\begin{align}
\alpha = 0.5 \times 40 &amp;= 20 \\
\beta = (1-0.5) \times 40 &amp;= 20
\end{align}\]</span></p>
<p><span class="math inline">\(\kappa\)</span> se puede interpretar como la cantidad de experimentos si/no que realizamos dándonos como resultado la media <span class="math inline">\(\mu\)</span>. Es decir el <em>a priori</em> no sesgado (naranja) equivale a haber arrojado una moneda 40 veces y haber obtenido como media 0.5. Es decir que si usamos ese <em>a priori</em> recién al observar 40 experimentos si/no, los datos tendrán el mismo peso relativo que el <em>a priori</em>, por debajo de este número el <em>a priori</em> contribuye más que los datos al resultado final y por encima menos. El <em>a priori</em> azul (uniforme) equivale a haber observado a la moneda caer una vez cara y otra vez ceca (<span class="math inline">\(\kappa = 2\)</span>). Cuando <span class="math inline">\(\kappa &lt; 2\)</span>, la cosa se pone un poco extraña, por ejemplo el <em>a priori</em> sesgado (turquesa) equivale a haber observado una sola moneda (<span class="math inline">\(\kappa = 1\)</span>) pero en una especie de, a falta de mejor analogía, ¡¿<em>superposición cuántica de estados</em>?!</p>
</section>
<section id="resumiendo-el-a-posteriori" class="level3" data-number="1.3.3">
<h3 data-number="1.3.3" class="anchored" data-anchor-id="resumiendo-el-a-posteriori"><span class="header-section-number">1.3.3</span> Resumiendo el <em>a posteriori</em></h3>
<p>El resultado de un análisis Bayesiano es siempre una distribución de probabilidad.</p>
<p>A la hora de comunicar los resultados de un análisis Bayesiano, lo más informativo es reportar la distribución completa, aunque esto no siempre es posible o deseable, por ejemplo el <em>a posteriori</em> de una distribución multidimensional es imposible de visualizar de forma directa. Por lo tanto, es común recurrir a distintas medidas que resumen el <em>a posteriori</em>, por ejemplo la media, mediana, la desviación estándar, etc. También es común, e informativo, reportar un intervalo de credibilidad. Existen varios criterios para definir intervalos de credibilidad, el que usaremos en este curso (y que también es ampliamente usado en la literatura) es lo que se conoce como intervalo de más alta densidad y nos referiremos a él por su sigla en ingles, HDI (<em>Highest Posterior Density interval</em>). Un HDI es el intervalo, más corto, que contiene una porción fija de la densidad de probabilidad, generalmente el 95% (aunque otros valores como 90% o 50% son comunes). Cualquier punto dentro de este intervalo tiene mayor densidad que cualquier punto fuera del intervalo. Para una distribución unimodal, el HDI 95 es simplemente el intervalo entre los percentiles 2,5 y 97,5.</p>
<p>ArviZ es un paquete de Python para análisis exploratorio de modelos Bayesianos. ArviZ provee de funciones que facilitan analizar y resumir el <em>a posteriori</em>. Por ejemplo <code>plot_posterior</code> puede ser usado para generar un gráfico con la media y el HDI. En el siguiente ejemplo en vez de un <em>a posteriori</em> “real” estamos usando datos sintéticos generados de una distribución Beta.</p>
<div id="cell-24" class="cell" data-nbpresent="{&quot;id&quot;:&quot;da32677d-9227-420b-98f4-c488eda85a59&quot;}" data-execution_count="7">
<div class="sourceCode cell-code" id="cb8"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb8-1"><a href="#cb8-1"></a>mock_posterior <span class="op">=</span> pz.Beta(<span class="dv">5</span>, <span class="dv">11</span>).rvs(size<span class="op">=</span><span class="dv">1000</span>)</span>
<span id="cb8-2"><a href="#cb8-2"></a>az.plot_posterior(mock_posterior, figsize<span class="op">=</span>(<span class="dv">8</span>, <span class="dv">4</span>))<span class="op">;</span></span></code><button title="Copiar al portapapeles" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="01_Inferencia_Bayesiana_files/figure-html/cell-8-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<p>Ahora que estamos aprendiendo que es un HDI por primera vez y antes de que automaticemos el concepto conviene aclarar un par de puntos.</p>
<ol type="1">
<li><p><strong>La elección automática de 95% (o cualquier otro valor) es totalmente arbitraria</strong>. En principio no hay ninguna razón para pensar que describir el <em>a posteriori</em> con un HDI 95 sea mejor que describirlo con un HDI 98 o que no podamos usar valores como 87% o 66%. El valor de 95% es tan solo un accidente histórico. Como un sutil recordatorio de esto ArviZ usa por defecto el valor 94%!</p></li>
<li><p><strong>Un intervalo de credibilidad (que es Bayesiano) no es lo mismo que un intervalo de confianza (que es frecuentista)</strong>. Un intervalo de confianza es un intervalo que se define según un nivel de confianza, en general del 95%. Un intervalo de confianza se construye de tal forma que si repitiéramos infinitas veces un experimento obtendríamos que la proporción de intervalos que contienen el valor <em>verdadero</em> del parámetro que nos interesa coincide con el nivel de confianza estipulado. Contra-intuitivamente esto no es lo mismo que decir que un intervalo en particular tiene una probabilidad <span class="math inline">\(x\)</span> de contener el parámetro (esto sería la definición de un intervalo de credibilidad, que es Bayesiano). De hecho, un intervalo de confianza en particular contiene o no contiene al valor, la teoría frecuentista no nos deja hablar de probabilidades de los parámetros, ya que estos tienen valores fijos. Si no queda clara la diferencia no te hagas problema, la diferencia entre estos dos conceptos suele ser tan difícil de entender que en la práctica estudiantes y científicos por igual interpretan los intervalos de confianza (frecuentistas) como intervalos de credibilidad (Bayesianos).</p></li>
</ol>
<blockquote class="blockquote">
<p>Si bien desde la perspectiva Bayesiana podemos afirmar que un intervalo de credibilidad nos permite asegurar que la probabilidad de un parámetro está acotado en cierto rango. Siempre hay que tener presente que dicha afirmación es correcta SOLO en sentido teórico. Es decir, solo si todos los supuestos contenidos en el modelo son ciertos. Una inferencia es siempre dependiente de los datos y modelos usados.</p>
</blockquote>
</section>
</section>
<section id="distribución-predictivas" class="level2" data-number="1.4">
<h2 data-number="1.4" class="anchored" data-anchor-id="distribución-predictivas"><span class="header-section-number">1.4</span> Distribución predictivas</h2>
<p>Si bien el objeto central de la estadística Bayesiana es la distribución <em>a posteriori</em>. Existen otras distribuciones muy importantes. Una de ellas es la distribución predictiva <em>a posteriori</em>, otra es la distribución predictiva <em>a priori</em>.</p>
<section id="distribución-predictivas-a-posteriori" class="level3" data-number="1.4.1">
<h3 data-number="1.4.1" class="anchored" data-anchor-id="distribución-predictivas-a-posteriori"><span class="header-section-number">1.4.1</span> Distribución predictivas <em>a posteriori</em></h3>
<p>Esta distribución representa las predicciones <span class="math inline">\(\tilde{y}\)</span> de un modelo una vez obtenido el <em>a posteriori</em>. Se calcula de la siguiente manera:</p>
<p><span class="math display">\[
p(\tilde{y}  \mid  y) = \int p(\tilde{y} \mid \theta) p(\theta \mid y) d\theta
\]</span></p>
<p>Es decir integramos <span class="math inline">\(\theta\)</span> de acuerdo a la distribución <em>a posteriori</em>.</p>
<p>Computacionalmente podemos generar muestras de esta distribución según el siguiente procedimiento:</p>
<ol type="1">
<li>Elegimos un valor de <span class="math inline">\(\theta\)</span> de acuerdo a la distribución a posteriori <span class="math inline">\(p(\theta \mid y)\)</span></li>
<li>Fijamos <span class="math inline">\(\theta\)</span> en la distribución que usamos como likelihood <span class="math inline">\(p(\tilde{y} \mid \theta)\)</span> y generamos una muestra aleatoria</li>
<li>Repetimos desde 1, tantas veces como muestras necesitemos</li>
</ol>
<p>Los datos generados son predictivos ya que son los datos que se esperaría ver por ejemplo en un futuro experimento, es decir son variables no observadas pero potencialmente observables. Como veremos en el siguiente capítulo un uso muy común para la distribución predictiva <em>a posteriori</em> es compararla con los datos observados y así evaluar si el posterior calculado es razonable.</p>
</section>
<section id="distribución-predictiva-a-priori" class="level3" data-number="1.4.2">
<h3 data-number="1.4.2" class="anchored" data-anchor-id="distribución-predictiva-a-priori"><span class="header-section-number">1.4.2</span> Distribución predictiva <em>a priori</em></h3>
<p>Asi como es posible generar datos sintéticos desde el <em>a posteriori</em>. Es posible hacerlo desde el prior. En este caso la distribución se llama distribución predictiva <em>a priori</em>. Y representa los datos <span class="math inline">\(p(y^\ast)\)</span> que el modelo <em>espera</em> ver antes de haber visto los datos. O más formalmente antes de haber sido condicionado a los datos. Se calcula como:</p>
<p><span class="math display">\[
p(y^\ast) =  \int p(y^\ast \mid \theta) \; p(\theta) \; d\theta
\]</span></p>
<p>Es importante notar que la definición es muy similar a la distribución predictiva a posteriori, solo que ahora integramos a lo largo del prior en vez del posterior.</p>
<p>Los datos generados son predictivos ya que son los datos que el modelo esperara ver, es decir son datos no observados pero potencialmente observables. Como veremos en el siguiente capítulo un uso muy común para la distribución predictiva <em>a priori</em> es compararla con nuestro conocimiento previo y así evaluar si el modelo es capaz de generar resultados razonable, incluso antes de haber incorporado los datos.</p>
</section>
<section id="distribución-predictiva-a-priori-y-a-posterior-para-el-problema-de-la-moneda." class="level3" data-number="1.4.3">
<h3 data-number="1.4.3" class="anchored" data-anchor-id="distribución-predictiva-a-priori-y-a-posterior-para-el-problema-de-la-moneda."><span class="header-section-number">1.4.3</span> Distribución predictiva <em>a priori</em> y a posterior para el problema de la moneda.</h3>
<p>En el caso del modelo beta-binomial es posible obtener analíticamente tanto la distribución predictiva a priori como a posteriori y estas son:</p>
<p><span class="math display">\[
p(y^\ast) \propto \operatorname{Beta-binomial}(n=N, \alpha_{a priori}, \beta_{a priori})
\]</span></p>
<p><span class="math display">\[
p(\tilde{y}  \mid  y)  \propto \operatorname{Beta-binomial}(n=N, \alpha_{a priori} + y, \beta_{a priori} + N - y)
\]</span></p>
<p>Omitiremos la discusión de como se obtienen estas distribuciones</p>
</section>
<section id="cuarteto-bayesiano" class="level3" data-number="1.4.4">
<h3 data-number="1.4.4" class="anchored" data-anchor-id="cuarteto-bayesiano"><span class="header-section-number">1.4.4</span> Cuarteto Bayesiano</h3>
<p>El siguiente bloque de código computa las distribuciones <em>a priori</em>, <em>a posteriori</em>, predictiva <em>a priori</em> y predictiva <em>a posteriori</em>. En vez de usar la distribución <span class="math inline">\(\operatorname{Beta-binomial}\)</span> para las distribuciones predictivas hemos optado por usar una aproximación más computacional y muestrear primero de la distribuciones beta y luego de la binomial. Esperamos que esta decisión contribuya a comprender mejor que representan estas distribuciones.</p>
<p>Es importante notar que mientras la distribuciones <em>a priori</em> y <em>a posteriori</em> son distribución <strong>sobre los parámetros en un modelo</strong>, la distribución predictivas <em>a priori</em> y <em>a posteriori</em> son distribuciones <strong>sobre los datos</strong> (predichos).</p>
<div id="cell-30" class="cell" data-execution_count="8">
<div class="sourceCode cell-code" id="cb9"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb9-1"><a href="#cb9-1"></a>fig, axes <span class="op">=</span> plt.subplots(<span class="dv">2</span>, <span class="dv">2</span>, figsize<span class="op">=</span>(<span class="dv">10</span>, <span class="dv">8</span>), sharex<span class="op">=</span><span class="st">"row"</span>, sharey<span class="op">=</span><span class="st">"row"</span>)</span>
<span id="cb9-2"><a href="#cb9-2"></a>axes <span class="op">=</span> np.ravel(axes)</span>
<span id="cb9-3"><a href="#cb9-3"></a>dist <span class="op">=</span> pz.Beta</span>
<span id="cb9-4"><a href="#cb9-4"></a>a_prior <span class="op">=</span> <span class="dv">1</span></span>
<span id="cb9-5"><a href="#cb9-5"></a>b_prior <span class="op">=</span> <span class="dv">1</span></span>
<span id="cb9-6"><a href="#cb9-6"></a>N <span class="op">=</span> <span class="dv">12</span></span>
<span id="cb9-7"><a href="#cb9-7"></a>y <span class="op">=</span> <span class="dv">3</span></span>
<span id="cb9-8"><a href="#cb9-8"></a>eps <span class="op">=</span> np.finfo(<span class="bu">float</span>).eps</span>
<span id="cb9-9"><a href="#cb9-9"></a>x <span class="op">=</span> np.linspace(eps, <span class="dv">1</span><span class="op">-</span>eps, <span class="dv">100</span>)</span>
<span id="cb9-10"><a href="#cb9-10"></a></span>
<span id="cb9-11"><a href="#cb9-11"></a></span>
<span id="cb9-12"><a href="#cb9-12"></a>prior <span class="op">=</span> dist(a_prior, b_prior).pdf(x)</span>
<span id="cb9-13"><a href="#cb9-13"></a>axes[<span class="dv">0</span>].fill_between(x, <span class="dv">0</span>, prior)</span>
<span id="cb9-14"><a href="#cb9-14"></a>axes[<span class="dv">0</span>].set_title(<span class="st">"Prior"</span>)</span>
<span id="cb9-15"><a href="#cb9-15"></a>axes[<span class="dv">0</span>].set_yticks([])</span>
<span id="cb9-16"><a href="#cb9-16"></a></span>
<span id="cb9-17"><a href="#cb9-17"></a></span>
<span id="cb9-18"><a href="#cb9-18"></a>posterior <span class="op">=</span> dist(a_prior <span class="op">+</span> y, b_prior <span class="op">+</span> N <span class="op">-</span> y).pdf(x)</span>
<span id="cb9-19"><a href="#cb9-19"></a>axes[<span class="dv">1</span>].fill_between(x, <span class="dv">0</span>, posterior)</span>
<span id="cb9-20"><a href="#cb9-20"></a>axes[<span class="dv">1</span>].set_title(<span class="st">"Posterior"</span>)</span>
<span id="cb9-21"><a href="#cb9-21"></a></span>
<span id="cb9-22"><a href="#cb9-22"></a></span>
<span id="cb9-23"><a href="#cb9-23"></a>prior <span class="op">=</span> dist(a_prior, b_prior).rvs(<span class="dv">500</span>)</span>
<span id="cb9-24"><a href="#cb9-24"></a>prior_predictive <span class="op">=</span> np.hstack([pz.Binomial(n<span class="op">=</span>N, p<span class="op">=</span>p).rvs(N) <span class="cf">for</span> p <span class="kw">in</span> prior])</span>
<span id="cb9-25"><a href="#cb9-25"></a>axes[<span class="dv">2</span>].hist(prior_predictive, bins<span class="op">=</span><span class="bu">range</span>(<span class="dv">0</span>, N<span class="op">+</span><span class="dv">2</span>), rwidth<span class="op">=</span><span class="fl">0.9</span>, align<span class="op">=</span><span class="st">"left"</span>, density<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb9-26"><a href="#cb9-26"></a>axes[<span class="dv">2</span>].set_xlim(<span class="op">-</span><span class="fl">0.5</span>, N<span class="op">+</span><span class="fl">0.5</span>)  </span>
<span id="cb9-27"><a href="#cb9-27"></a>axes[<span class="dv">2</span>].set_title(<span class="st">"Predictiva a priori"</span>)</span>
<span id="cb9-28"><a href="#cb9-28"></a></span>
<span id="cb9-29"><a href="#cb9-29"></a>posterior <span class="op">=</span> dist(a_prior <span class="op">+</span> y, b_prior <span class="op">+</span> N <span class="op">-</span> y).rvs(<span class="dv">500</span>)</span>
<span id="cb9-30"><a href="#cb9-30"></a>prior_predictive <span class="op">=</span> np.hstack([pz.Binomial(n<span class="op">=</span>N, p<span class="op">=</span>p).rvs(N) <span class="cf">for</span> p <span class="kw">in</span> posterior])</span>
<span id="cb9-31"><a href="#cb9-31"></a>axes[<span class="dv">3</span>].hist(prior_predictive, bins<span class="op">=</span><span class="bu">range</span>(<span class="dv">0</span>, N<span class="op">+</span><span class="dv">2</span>), rwidth<span class="op">=</span><span class="fl">0.9</span>, align<span class="op">=</span><span class="st">"left"</span>, density<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb9-32"><a href="#cb9-32"></a>axes[<span class="dv">3</span>].set_xlim(<span class="op">-</span><span class="fl">0.5</span>, N<span class="op">+</span><span class="fl">0.5</span>)  </span>
<span id="cb9-33"><a href="#cb9-33"></a>axes[<span class="dv">3</span>].set_title(<span class="st">"Predictiva a posteriori"</span>)<span class="op">;</span></span>
<span id="cb9-34"><a href="#cb9-34"></a></span>
<span id="cb9-35"><a href="#cb9-35"></a>fig.suptitle(<span class="st">"Cuarteto Bayesiano"</span>, fontweight<span class="op">=</span><span class="st">"bold"</span>, fontsize<span class="op">=</span><span class="dv">16</span>)<span class="op">;</span></span></code><button title="Copiar al portapapeles" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="01_Inferencia_Bayesiana_files/figure-html/cell-9-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
</section>
</section>
<section id="ejercicios" class="level2" data-number="1.5">
<h2 data-number="1.5" class="anchored" data-anchor-id="ejercicios"><span class="header-section-number">1.5</span> Ejercicios</h2>
<ol type="1">
<li><p>El estadístico Bruno de Finetti declaró que “Las probabilidades no existen”. Cómo se vincula este enunciado con el modelo BetaBinomial que hemos visto?</p></li>
<li><p>Edwin Jaynes, físico, declaró que la teoría de probabilidad es la lógica de la ciencia. Discutí este enunciado.</p></li>
<li><p>Usá pz.Beta().plot_interactive() para explorar distintas combinaciones de parámetros de la distribución Beta. Cuál es el efecto de los parámetros <span class="math inline">\(\alpha\)</span> y <span class="math inline">\(\beta\)</span>?</p></li>
<li><p>Interpretá los parámetros de una distribución Beta, <span class="math inline">\(\alpha\)</span> y <span class="math inline">\(\beta\)</span> en términos de su PDF (probability density function). Analizá solo el caso de <span class="math inline">\(\alpha &gt;= 1\)</span> y <span class="math inline">\(\beta &gt;= 1\)</span></p></li>
<li><p>La media de la distribución Beta es <span class="math inline">\(\frac{\alpha}{\alpha+\beta}\)</span>. Cuál es la media de la distribución a posteriori para un modelo Beta-Binomial, con prior Beta(2, 5) y 10 experimentos con 6 caras?</p></li>
<li><p>La varianza de la distribución Beta es <span class="math inline">\(\frac{\alpha \beta}{(\alpha+\beta)^2(\alpha+\beta+1)}\)</span>. Cuál es la varianza de la distribución a posteriori para un modelo Beta-Binomial, con prior Beta(2, 5) y 10 experimentos con 6 caras?</p></li>
<li><p>Contrastá los resultados de los puntos anteriores usando la función <code>mean</code> y <code>var</code> de la distribución Beta de PreliZ.</p></li>
<li><p>PreliZ tiene una función llamada <a href="https://preliz.readthedocs.io/en/latest/api_reference.html#preliz.unidimensional.maxent">maxent</a>. Explicá que hace.</p></li>
<li><p>Conocimiento experto indica que un parámetro debe ser positivo y que el 90% puede estar entre 2 y 20. Usá maxent para definir ese prior.</p></li>
<li><p>Conocimiento experto indica que un parámetro debe ser positivo con media 6 y 90% puede estar entre 2 y 20. Usá maxent para definir ese prior.</p></li>
<li><p>Usá la siguiente función para explorar diversas combinaciones de <em>priors</em> y <em>likelihoods</em>. Enunciá las conclusiones que consideres más relevantes.</p></li>
</ol>
<div id="cell-32" class="cell" data-execution_count="9">
<div class="sourceCode cell-code" id="cb10"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb10-1"><a href="#cb10-1"></a><span class="kw">def</span> a_posteriori_grilla(grilla<span class="op">=</span><span class="dv">10</span>, a<span class="op">=</span><span class="dv">1</span>, b<span class="op">=</span><span class="dv">1</span>, caras<span class="op">=</span><span class="dv">6</span>, tiradas<span class="op">=</span><span class="dv">9</span>):</span>
<span id="cb10-2"><a href="#cb10-2"></a>    grid <span class="op">=</span> np.linspace(<span class="dv">0</span>, <span class="dv">1</span>, grilla)</span>
<span id="cb10-3"><a href="#cb10-3"></a>    prior <span class="op">=</span> pz.Beta(a, b).pdf(grid)</span>
<span id="cb10-4"><a href="#cb10-4"></a>    likelihood <span class="op">=</span> pz.Binomial(n<span class="op">=</span>tiradas, p<span class="op">=</span>grid).pdf(caras)</span>
<span id="cb10-5"><a href="#cb10-5"></a>    posterior <span class="op">=</span> likelihood <span class="op">*</span> prior</span>
<span id="cb10-6"><a href="#cb10-6"></a>    posterior <span class="op">/=</span> posterior.<span class="bu">sum</span>()</span>
<span id="cb10-7"><a href="#cb10-7"></a>    _, ax <span class="op">=</span> plt.subplots(<span class="dv">1</span>, <span class="dv">3</span>, sharex<span class="op">=</span><span class="va">True</span>, figsize<span class="op">=</span>(<span class="dv">16</span>, <span class="dv">4</span>))</span>
<span id="cb10-8"><a href="#cb10-8"></a>    ax[<span class="dv">0</span>].set_title(<span class="st">'caras = </span><span class="sc">{}</span><span class="ch">\n</span><span class="st">tiradas = </span><span class="sc">{}</span><span class="st">'</span>.<span class="bu">format</span>(caras, tiradas))</span>
<span id="cb10-9"><a href="#cb10-9"></a>    <span class="cf">for</span> i, (e, e_n) <span class="kw">in</span> <span class="bu">enumerate</span>(<span class="bu">zip</span>([prior, likelihood, posterior], [<span class="st">'a priori'</span>, <span class="st">'likelihood'</span>, <span class="st">'a posteriori'</span>])):</span>
<span id="cb10-10"><a href="#cb10-10"></a>        ax[i].set_yticks([])</span>
<span id="cb10-11"><a href="#cb10-11"></a>        ax[i].plot(grid, e, <span class="st">'o-'</span>, label<span class="op">=</span>e_n)</span>
<span id="cb10-12"><a href="#cb10-12"></a>        ax[i].legend(fontsize<span class="op">=</span><span class="dv">14</span>)</span>
<span id="cb10-13"><a href="#cb10-13"></a></span>
<span id="cb10-14"><a href="#cb10-14"></a></span>
<span id="cb10-15"><a href="#cb10-15"></a>interact(a_posteriori_grilla, grilla<span class="op">=</span>ipyw.IntSlider(<span class="bu">min</span><span class="op">=</span><span class="dv">2</span>, <span class="bu">max</span><span class="op">=</span><span class="dv">100</span>, step<span class="op">=</span><span class="dv">1</span>, value<span class="op">=</span><span class="dv">15</span>), a<span class="op">=</span>ipyw.FloatSlider(<span class="bu">min</span><span class="op">=</span><span class="dv">1</span>, <span class="bu">max</span><span class="op">=</span><span class="dv">7</span>, step<span class="op">=</span><span class="dv">1</span>, value<span class="op">=</span><span class="dv">1</span>), b<span class="op">=</span>ipyw.FloatSlider(</span>
<span id="cb10-16"><a href="#cb10-16"></a>    <span class="bu">min</span><span class="op">=</span><span class="dv">1</span>, <span class="bu">max</span><span class="op">=</span><span class="dv">7</span>, step<span class="op">=</span><span class="dv">1</span>, value<span class="op">=</span><span class="dv">1</span>), caras<span class="op">=</span>ipyw.IntSlider(<span class="bu">min</span><span class="op">=</span><span class="dv">0</span>, <span class="bu">max</span><span class="op">=</span><span class="dv">20</span>, step<span class="op">=</span><span class="dv">1</span>, value<span class="op">=</span><span class="dv">6</span>), tiradas<span class="op">=</span>ipyw.IntSlider(<span class="bu">min</span><span class="op">=</span><span class="dv">0</span>, <span class="bu">max</span><span class="op">=</span><span class="dv">20</span>, step<span class="op">=</span><span class="dv">1</span>, value<span class="op">=</span><span class="dv">9</span>))<span class="op">;</span></span></code><button title="Copiar al portapapeles" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<script type="application/vnd.jupyter.widget-view+json">
{"model_id":"b3c9951a864b4993b102cd0ea5b9c188","version_major":2,"version_minor":0,"quarto_mimetype":"application/vnd.jupyter.widget-view+json"}
</script>
</div>
</div>


</section>

</main> <!-- /main -->
<script type="application/vnd.jupyter.widget-state+json">
{"state":{},"version_major":2,"version_minor":0}
</script>
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const disableStylesheet = (stylesheets) => {
    for (let i=0; i < stylesheets.length; i++) {
      const stylesheet = stylesheets[i];
      stylesheet.rel = 'prefetch';
    }
  }
  const enableStylesheet = (stylesheets) => {
    for (let i=0; i < stylesheets.length; i++) {
      const stylesheet = stylesheets[i];
      stylesheet.rel = 'stylesheet';
    }
  }
  const manageTransitions = (selector, allowTransitions) => {
    const els = window.document.querySelectorAll(selector);
    for (let i=0; i < els.length; i++) {
      const el = els[i];
      if (allowTransitions) {
        el.classList.remove('notransition');
      } else {
        el.classList.add('notransition');
      }
    }
  }
  const toggleGiscusIfUsed = (isAlternate, darkModeDefault) => {
    const baseTheme = document.querySelector('#giscus-base-theme')?.value ?? 'light';
    const alternateTheme = document.querySelector('#giscus-alt-theme')?.value ?? 'dark';
    let newTheme = '';
    if(darkModeDefault) {
      newTheme = isAlternate ? baseTheme : alternateTheme;
    } else {
      newTheme = isAlternate ? alternateTheme : baseTheme;
    }
    const changeGiscusTheme = () => {
      // From: https://github.com/giscus/giscus/issues/336
      const sendMessage = (message) => {
        const iframe = document.querySelector('iframe.giscus-frame');
        if (!iframe) return;
        iframe.contentWindow.postMessage({ giscus: message }, 'https://giscus.app');
      }
      sendMessage({
        setConfig: {
          theme: newTheme
        }
      });
    }
    const isGiscussLoaded = window.document.querySelector('iframe.giscus-frame') !== null;
    if (isGiscussLoaded) {
      changeGiscusTheme();
    }
  }
  const toggleColorMode = (alternate) => {
    // Switch the stylesheets
    const alternateStylesheets = window.document.querySelectorAll('link.quarto-color-scheme.quarto-color-alternate');
    manageTransitions('#quarto-margin-sidebar .nav-link', false);
    if (alternate) {
      enableStylesheet(alternateStylesheets);
      for (const sheetNode of alternateStylesheets) {
        if (sheetNode.id === "quarto-bootstrap") {
          toggleBodyColorMode(sheetNode);
        }
      }
    } else {
      disableStylesheet(alternateStylesheets);
      toggleBodyColorPrimary();
    }
    manageTransitions('#quarto-margin-sidebar .nav-link', true);
    // Switch the toggles
    const toggles = window.document.querySelectorAll('.quarto-color-scheme-toggle');
    for (let i=0; i < toggles.length; i++) {
      const toggle = toggles[i];
      if (toggle) {
        if (alternate) {
          toggle.classList.add("alternate");     
        } else {
          toggle.classList.remove("alternate");
        }
      }
    }
    // Hack to workaround the fact that safari doesn't
    // properly recolor the scrollbar when toggling (#1455)
    if (navigator.userAgent.indexOf('Safari') > 0 && navigator.userAgent.indexOf('Chrome') == -1) {
      manageTransitions("body", false);
      window.scrollTo(0, 1);
      setTimeout(() => {
        window.scrollTo(0, 0);
        manageTransitions("body", true);
      }, 40);  
    }
  }
  const isFileUrl = () => { 
    return window.location.protocol === 'file:';
  }
  const hasAlternateSentinel = () => {  
    let styleSentinel = getColorSchemeSentinel();
    if (styleSentinel !== null) {
      return styleSentinel === "alternate";
    } else {
      return false;
    }
  }
  const setStyleSentinel = (alternate) => {
    const value = alternate ? "alternate" : "default";
    if (!isFileUrl()) {
      window.localStorage.setItem("quarto-color-scheme", value);
    } else {
      localAlternateSentinel = value;
    }
  }
  const getColorSchemeSentinel = () => {
    if (!isFileUrl()) {
      const storageValue = window.localStorage.getItem("quarto-color-scheme");
      return storageValue != null ? storageValue : localAlternateSentinel;
    } else {
      return localAlternateSentinel;
    }
  }
  const darkModeDefault = false;
  let localAlternateSentinel = darkModeDefault ? 'alternate' : 'default';
  // Dark / light mode switch
  window.quartoToggleColorScheme = () => {
    // Read the current dark / light value 
    let toAlternate = !hasAlternateSentinel();
    toggleColorMode(toAlternate);
    setStyleSentinel(toAlternate);
    toggleGiscusIfUsed(toAlternate, darkModeDefault);
  };
  // Ensure there is a toggle, if there isn't float one in the top right
  if (window.document.querySelector('.quarto-color-scheme-toggle') === null) {
    const a = window.document.createElement('a');
    a.classList.add('top-right');
    a.classList.add('quarto-color-scheme-toggle');
    a.href = "";
    a.onclick = function() { try { window.quartoToggleColorScheme(); } catch {} return false; };
    const i = window.document.createElement("i");
    i.classList.add('bi');
    a.appendChild(i);
    window.document.body.appendChild(a);
  }
  // Switch to dark mode if need be
  if (hasAlternateSentinel()) {
    toggleColorMode(true);
  } else {
    toggleColorMode(false);
  }
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    text: function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copiado");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copiado");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp('/' + window.location.host + '/');
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      // TODO in 1.5, we should make sure this works without a callout special case
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="./index.html" class="pagination-link" aria-label="‎">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text">‎</span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="./02_Programación_probabilística.html" class="pagination-link" aria-label="Programación probabilista">
        <span class="nav-page-text"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Programación probabilista</span></span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav>
</div> <!-- /content -->
<footer class="footer">
  <div class="nav-footer">
    <div class="nav-footer-left">
      &nbsp;
    </div>   
    <div class="nav-footer-center">
<p><a rel="license" href="http://creativecommons.org/licenses/by/4.0/"><img alt="Licencia Creative Commons" style="border-width:0" src="https://i.creativecommons.org/l/by/4.0/80x15.png"></a><br>Este obra está bajo <a rel="license" href="http://creativecommons.org/licenses/by/4.0/">licencia Creative Commons Reconocimiento 4.0 Internacional</a>.</p>
</div>
    <div class="nav-footer-right">
      &nbsp;
    </div>
  </div>
</footer>




</body></html>
[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Estadística a Inferencia II",
    "section": "",
    "text": "‎\nLa estadística trata sobre la recolección, organización, análisis e interpretación de datos, es por ello que la estadística es esencial para el correcto análisis de datos.\nExisten dos grandes conjuntos de herramientas para analizar datos:\nAnálisis Exploratorio de Datos (EDA): Consiste en resúmenes numéricos como la media, moda, desviación estándar, rangos intercuartiles, etc (esto se conoce también como estadística descriptiva). Además hace énfasis en el uso de métodos visuales para inspeccionar los datos, como por ejemplo histogramas y gráficos de dispersión.\nEstadística Inferencial: Consiste en usar datos para generar enunciados que exceden los propios datos. A veces esto implica realizar predicciones, a veces entender los detalles de algún fenómeno en particular o elegir entre varias explicaciones plausibles.\nMuchos de los cursos y libros sobre estadística, principalmente aquellos dirigidos a no-estadísticos, enseñan una serie de recetas que más o menos tienen la siguiente forma.\nLa principal meta de estos cursos es la de enseñar a usar la lata adecuada y con suerte alguna que otra discusión sobre el emplatado. Esta aproximación pedagógica, dificulta entender conceptualmente la unidad de los diferentes métodos enseñados y tiene como resultado la reproducción de prácticas poco transparentes y/o útiles.\nEn este curso se intenta una aproximación totalmente diferente. También aprenderemos recetas, pero intentaremos que los platos tengan un sabor más casero y menos enlatado, aprenderemos a mezclar ingredientes frescos que se acomoden a diferentes situaciones gastronómicas.\nEste enfoque es posible por dos razones:",
    "crumbs": [
      "‎"
    ]
  },
  {
    "objectID": "index.html#a-quienes-está-dirijido",
    "href": "index.html#a-quienes-está-dirijido",
    "title": "Estadística a Inferencia II",
    "section": "A quienes está dirijido?",
    "text": "A quienes está dirijido?\nEste es un curso introductorio para personas sin conocimiento previo de estadística o ciencia de datos. Se asume familiaridad con Python y librerías de Python usadas en análisis de datos como Numpy, matplotlib, Pandas, etc.\nQuienes no sepan Python, pero tengan familiaridad con otros lenguajes de programación también podrán aprovechar el curso, aunque puede que experimente un poco más de fricción.\nPor último quienes no tengan interés en aprender a usar código para analisis de datos pueden aún aprovechar parte del material para obtener una visión a vuelo de pájaro de los métodos Bayesianos.",
    "crumbs": [
      "‎"
    ]
  },
  {
    "objectID": "index.html#cómo-usar-este-material",
    "href": "index.html#cómo-usar-este-material",
    "title": "Estadística a Inferencia II",
    "section": "Cómo usar este material",
    "text": "Cómo usar este material\n\nVersión estática: Esta página contiene una versión estática del material. Es decir podrás ver el texto y las figuras pero no podrás modificarlos, ni interactuar con el material.\nVersión interactiva online: . Esta versión permite interactuar con el material, modificarlo y ejecutarlo en tu navegador.\nVersión interactiva local: También es posible descargar el material y ejecutarlo en tu propia computadora. Para ello hacé click y seguí las instrucciones de la próxima sección (Instalación).",
    "crumbs": [
      "‎"
    ]
  },
  {
    "objectID": "index.html#instalación",
    "href": "index.html#instalación",
    "title": "Estadística a Inferencia II",
    "section": "Instalación",
    "text": "Instalación\nPara usar este material es necesario tener instalado Python. Se recomienda la versión 3.9 o superior. Además es necesario instalar los siguientes paquetes:\n\nPyMC 5.8.2\nArviZ 0.16.1\nPreliZ 0.3.3\ngraphviz (una dependencia opcional de PyMC)\n\nSe recomienda instalar primero Anaconda. Luego instalar el resto de los paquetes con los comandos:\nconda install pip\npip install pymc==5.8.3 arviz==0.16.1 preliz==0.3.3 graphviz\nComo alternativa pueden crear un ambiente con los paquetes necesario descargando el archivo y ejecutando el comando\nconda env create -f environment.yml",
    "crumbs": [
      "‎"
    ]
  },
  {
    "objectID": "index.html#contribuciones",
    "href": "index.html#contribuciones",
    "title": "Estadística a Inferencia II",
    "section": "Contribuciones",
    "text": "Contribuciones\nTodo el contenido de este repositorio es abierto, esto quiere decir que cualquier persona interesada puede contribuir al mismo. Todas las contribuciones serán bien recibidas incluyendo:\n\nCorrecciones ortográficas\nNuevas figuras\nCorrecciones en el código Python, incluidas mejoras de estilo\nMejores ejemplos\nMejores explicaciones\nCorrecciones de errores conceptuales\n\nLa forma de contribuir es vía Github, es decir los cambios deberán ser hechos en forma de pull requests y los problemas/bugs deberán reportarse como Issues.",
    "crumbs": [
      "‎"
    ]
  },
  {
    "objectID": "01_Inferencia_Bayesiana.html",
    "href": "01_Inferencia_Bayesiana.html",
    "title": "1  Inferencia Bayesiana",
    "section": "",
    "text": "1.1 El universo Bayesiano\nEn este curso aprenderemos sobre una forma de hacer estadística llamada usualmente estadística Bayesiana. El nombre se debe a Thomas Bayes (1702-1761) un ministro presbiteriano, y matemático aficionado, quien derivó por primera vez lo que ahora conocemos como el teorema de Bayes, el cual fue publicado (postumanente) en 1763. Sin embargo una de las primeras personas en realmente desarrollar métodos Bayesianos, fue Pierre-Simon Laplace (1749-1827), por lo que tal vez sería un poco más correcto hablar de Estadística Laplaciana y no Bayesiana.\nHay dos ideas centrales que hacen que un método sea Bayesiano:\nEn el universo Bayesiano las cantidades conocidas son consideradas fijas y usualmente les llamamos datos. Por el contrario toda cantidad desconocida es considerada como una variable aleatoria y es considerada un parámetros dentro de un modelo Bayesiano.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Inferencia Bayesiana</span>"
    ]
  },
  {
    "objectID": "01_Inferencia_Bayesiana.html#el-universo-bayesiano",
    "href": "01_Inferencia_Bayesiana.html#el-universo-bayesiano",
    "title": "1  Inferencia Bayesiana",
    "section": "",
    "text": "Toda cantidad desconocida es modelada utilizando una distribución de probabilidad de algún tipo.\nEl teorema de Bayes es usado para actualizar dicha distribución a la luz de los datos.\n\n\n\n1.1.1 Teorema de Bayes\nEl teorema de Bayes es una consecuencia directa de la regla del producto, veamos.\n\\[\\begin{align}\np(\\theta, Y) = p(\\theta \\mid Y)\\; p(Y) \\\\\np(\\theta, Y) = p(Y \\mid \\theta)\\; p(\\theta)\n\\end{align}\\] Dado que los dos términos a la derecha de la igualdad son iguales entre si podemos escribir que:\n\\[\np(\\theta \\mid Y) \\; p(Y) = p(Y \\mid \\theta)\\; p(\\theta)\n\\]\nReordenando llegamos al Teorema de Bayes!\n\\[\np(\\theta \\mid Y) = \\frac{p(Y \\mid \\theta) p(\\theta)}{p(Y)}\n\\]\nEl cual también suele ser escrito de la siguiente forma:\n\\[\n\\overbrace{p(\\theta \\mid Y)}^{\\text{posterior}} = \\frac{\\overbrace{p(Y \\mid \\theta)}^{\\text{likelihood}} \\overbrace{p(\\theta)}^{\\text{prior}}}{\\underbrace{\\int_{\\Theta} p(Y \\mid \\theta) p(\\theta) \\text{d}\\theta}_{\\text{likelihood marginal}}}\n\\]\nEl a priori es la forma de introducir conocimiento previo sobre los valores que pueden tomar los parámetros. A veces cuando no sabemos demasiado se suelen usar a prioris que asignan igual probabilidad a todos los valores de los parámetros, otras veces se puede elegir a prioris que restrinjan los valores de los parámetros a rangos razonables, algo que se conoce como regularización, por ejemplo solo valores positivos. Muchas veces contamos con información mucho más precisa como medidas experimentales previas o límites impuesto por alguna teoría.\nEl likelihood es la forma de incluir nuestros datos en el análisis. Es una expresión matemática que especifica la plausibilidad de los datos. El likelihood es central tanto en estadística Bayesiana como en estadística no-Bayesiana. A medida que la cantidad de datos aumenta el likelihood tiene cada vez más peso en los resultados, esto explica el porqué a veces los resultados de la estadística Bayesiana y frecuentista coinciden cuando la muestra es grande.\nEl a posteriori es la distribución de probabilidad para los parámetros. Es la consecuencia lógica de haber usado un conjunto de datos, un likelihood y un a priori. Se lo suele pensar como la versión actualizada del a priori. De hecho un a posteriori puede ser un a priori de un análisis a futuro.\nLa likelihood marginal (también llamado evidencia) es el likelihood promediado sobre todas los posibles hipótesis (o conjunto de parámetros) \\(\\theta\\), esto es equivalente a \\(p(Y)\\). En general, la evidencia puede ser vista como una simple constante de normalización que en la mayoría de los problemas prácticos puede (y suele) omitirse. Por lo que el teorema de Bayes suele aparecer escrito como:\n\\[\np(\\theta \\mid Y) \\propto p(Y \\mid \\theta) p(\\theta)\n\\]\nEl rol de todos estos términos irá quedando más claro a medida que avancemos.\n\n\n1.1.2 El a posteriori como único estimador\nEl a posteriori representa todo lo que sabemos de un problema, dado un modelo y un conjunto de datos. Y por lo tanto cualquier cantidad que nos interese sobre el problema puede deducirse a partir de él. Típicamente esto toma la forma de integrales como la siguiente.\n\\[\nJ = \\int \\varphi(\\theta) \\ \\ p(\\theta \\mid Y) d\\theta\n\\]\nPor ejemplo, para calcular la media de \\(\\theta\\) deberíamos reemplazar \\(\\varphi(\\theta)\\), por \\(\\theta\\):\n\\[\n\\bar \\theta = \\int \\theta \\ \\ p(\\theta \\mid Y) d\\theta\n\\]\nEsto no es más que la definición de un promedio pesado, donde cada valor de \\(\\theta\\) es pesado según la probabilidad asignada por el a posteriori.\nEn la práctica, y al usar métodos computacionales como los usados en este curso, estas integrales pueden aproximarse usando sumas.\n\n\n1.1.3 Estadística Bayesiana en tres pasos\nEl teorema de Bayes es el único estimador usado en estadística Bayesiana. Por lo que conceptualmente la estadística Bayesiana resulta muy simple. Según George Box y Andrew Gelman et al. (2013) la estadística Bayesiana se reduce a tres pasos:\n\nCrear un modelo probabilístico. Los modelos probabilísticos son historias que dan cuenta de como se generan los datos observados (o por observar). Los modelos se expresan usando distribuciones de probabilidad.\nCondicionar el modelo a los datos observados a fin de obtener el a posteriori. Usando el teorema de Bayes se actualizan las probabilidades asignadas a priori de acuerdo a los datos observados obteniéndose las probabilidades a posteriori.\nCriticar el ajuste del modelo generado a los datos y evaluar las consecuencias del modelo. Se puede demostrar que dada la información previa y los datos observados no existe otro mecanismo capaz de generar una mejor inferencia que la estadística Bayesiana. Esto parece maravilloso, pero hay un problema, sólo es cierto si se asumen que los datos y el modelo son correctos. En la práctica, los datos pueden contener errores y los modelos son a duras penas aproximaciones de fenómenos reales. Por lo tanto es necesario realizar varias evaluaciones, incluyendo si las predicciones generadas por el modelo se ajustan a los datos observados, si las conclusiones obtenidas tienen sentido dado el marco conceptual en el que uno trabaja, la sensibilidad de los resultados a los detalles del modelo (sobre todo a detalles para los cuales no tenemos demasiada información), etc. Además, es posible que realizar inferencia Bayesiana sea demasiado costosa en la práctica por lo que sea conveniente realizar aproximaciones.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Inferencia Bayesiana</span>"
    ]
  },
  {
    "objectID": "01_Inferencia_Bayesiana.html#inferencia-bayesiana",
    "href": "01_Inferencia_Bayesiana.html#inferencia-bayesiana",
    "title": "1  Inferencia Bayesiana",
    "section": "1.2 Inferencia Bayesiana",
    "text": "1.2 Inferencia Bayesiana\nEn la práctica la mayoría de los modelos tendrán más de un parámetro, pero empecemos con un modelo con un solo parámetro.\n\n1.2.1 El problema de la moneda\nA juzgar por la cantidad de ejemplos sobre monedas arrojadas al aires en libros de estadística y probabilidad, pareciera que las monedas son uno de los objetos de estudio centrales de estas disciplinas.\nUna de las razones detrás de la ubiquidad de este ejemplo es que las monedas son objetos familiares que facilitan discutir conceptos que de otra forma podrían sonar demasiado abstractos. De todas formas quizá la razón más importante sea que el problema puede ser modelado de forma simple y que muchos problemas reales son conceptualmente similares, de hecho cualquier problema en donde obtengamos resultados binarios (0/1, enfermo/sano, spam/no-spam, etc) puede ser pensado como si estuviéramos hablando de monedas. En definitiva el modelo que veremos a continuación (ejemplificado con monedas) sirve para cualquier situación en la cual los datos observados solo pueden tomar dos valores mutuamente excluyentes. Debido a que estos valores son nominales y son dos, a este modelo se le llama binomial.\nEn el siguiente ejemplo trataremos de determinar el grado en que una moneda está sesgada. En general cuando se habla de sesgo se hace referencia a la desviación de algún valor (por ejemplo, igual proporción de caras y cecas), pero aquí usaremos el termino sesgo de forma más general. Diremos que el sesgo es un valor en el intervalo [0, 1], siendo 0 para una moneda que siempre cae ceca y 1 para una moneda que siempre cae cara y lo representaremos con la variable \\(\\theta\\). A fin de cuantificar \\(\\theta\\) arrojaremos una moneda al aire repetidas veces, por practicidad arrojaremos la moneda de forma computacional (¡pero nada nos impide hacerlo manualmente!). Llevaremos registro del resultado en la variable \\(y\\). Siendo \\(y\\) la cantidad de caras obtenidas en un experimento.\nHabiendo definido nuestro problema debemos expresarlo en términos del teorema de Bayes,\n\\[\np(\\theta \\mid Y) \\propto p(Y \\mid  \\theta) p(\\theta)\n\\]\nDonde, como dijimos \\(\\theta = 1\\) quiere decir 100% cara y \\(\\theta = 0\\) 100% ceca.\nAhora sólo restar reemplazar los dos términos a la derecha de la igualdad, el a priori y el likelihood, por distribuciones de probabilidad adecuadas y luego multiplicarlas para obtener el término a la izquierda, el a posteriori. Como es la primera vez que haremos ésto, lo haremos paso a paso y analíticamente. En el próximo capítulo veremos cómo hacerlo computacionalmente.\n\n\n1.2.2 Definiendo el a priori\nEl a priori lo modelaremos usando una distribución Beta, que es una distribución muy usada en estadística Bayesiana. La \\(pdf\\) de esta distribución es:\n\\[\np(\\theta)= \\frac{\\Gamma(\\alpha+\\beta)}{\\Gamma(\\alpha)\\Gamma(\\beta)}\\, \\theta^{\\alpha-1}(1-\\theta)^{\\beta-1}\n\\]\nEl primer término es una constante de normalización. Por suerte para nuestro problema nos basta con establecer una proporcionalidad, por lo que podemos simplificar esta expresión y escribir la distribución Beta de la siguiente forma.\n\\[\np(\\theta) \\propto  \\theta^{\\alpha-1}(1-\\theta)^{\\beta-1}\n\\]\nHay varias razones para usar una distribución Beta para este y otros problemas:\n\nLa distribución Beta varía entre 0 y 1, de igual forma que lo hace \\(\\theta\\) en nuestro modelo.\nEsta distribución combinada con la que elegiremos como likelihood (ver más adelante), nos permitirá resolver el problema de forma analítica.\nEs una distribución versátil para expresar distintas situaciones.\n\nRespecto al último punto, veamos un ejemplo. Supongamos que el experimento de la moneda es realizado por tres personas. Una de ellas dice no saber nada de la moneda por lo tanto a priori todos los valores de \\(\\theta\\) son igualmente probables. La segunda persona desconfía de la moneda, ya que sospecha que es una moneda trucada, por lo tanto considera que está sesgada, pero no sabe para cual de las dos opciones. Por último, la tercer persona asegura que lo más probable es que \\(\\theta\\) tome un valor alrededor de 0.5 ya que así lo indican experimentos previos y análisis teóricos sobre tiradas de monedas. Todas estas situaciones pueden ser modeladas por la distribución Beta, como se ve a continuación.\n\n_, axes = plt.subplots(1, 3, figsize=(12, 3), sharey=True)\n\nparams = [(1, 1), (0.5, 0.5), (20, 20)]\n\nfor (a, b), ax  in zip(params, axes):\n    ax = pz.Beta(a, b).plot_pdf(ax=ax, legend=\"title\")\n    ax.set_ylim(0, 7)\n\n\n\n\n\n\n\n\n\npz.Beta().plot_interactive(xy_lim=(None, None, None, 10))\n\n\n\n\n\n\n1.2.3 Definiendo el likelihood\nHabiendo definido el a priori veamos ahora el likelihood. Asumiendo que el resultado obtenido al arrojar una moneda no influye en el resultado de posteriores experimentos (es decir los experimentos son independientes entre sí) es razonable utilizar como likelihood la distribución binomial.\n\\[\np(y \\mid \\theta) = \\frac{N!}{y!(N-y)!} \\theta^y (1 - \\theta)^{N−y}\n\\]\nDonde N es la cantidad total de experimentos (monedas arrojadas al aire) e \\(y\\) es la cantidad de caras obtenidas. A los fines prácticos podríamos simplificar la igualdad anterior y convertirla en una proporcionalidad, eliminando el término \\(\\frac{N!}{y!(N-y)!}\\) ya que ese término no depende de \\(\\theta\\) que es lo que nos interesa averiguar. Por lo que podríamos establecer que:\n\\[\np(y \\mid \\theta) \\propto \\theta^y (1 - \\theta)^{N−y}\n\\]\nLa elección de esta distribución para modelar nuestro problema es razonable ya que \\(\\theta\\) es la chance de obtener una cara al arrojar una moneda y ese hecho ha ocurrido \\(y\\) veces, de la misma forma \\(1-\\theta\\) es la chance de obtener ceca lo cual ha sido observado \\(N-y\\) veces.\n\npz.Binomial(1, 0.5).plot_interactive(pointinterval=False, xy_lim=(None, None, None, 1))\n\n\n\n\n\n\n1.2.4 Obteniendo el a posteriori\nSe puede demostrar que siempre que usemos como prior una función Beta y como likelihood una distribución binomial obtendremos como resultado una distribución a posteriori, la cual será una Beta con los siguientes parámetros:\n\\[\np(\\theta \\mid y) = \\operatorname{Beta}(\\alpha_{a priori} + y, \\beta_{a priori} + N - y)\n\\]\nVeamos de donde surge este resultado, según el teorema de Bayes la distribución a posteriori es el producto del likelihood y la distribución a priori.\n\\[\np(\\theta \\mid y) = p(y \\mid \\theta) p(\\theta) * c\n\\]\nPor lo tanto, en nuestro caso tendremos que:\n\\[\np(\\theta \\mid y) \\propto \\underbrace{\\frac{N!}{y!(N-y)!} \\theta^y (1 - \\theta)^{N−y}}_{\\text{likelihood}} \\underbrace{\\frac{\\Gamma(\\alpha+\\beta)}{\\Gamma(\\alpha)\\Gamma(\\beta)}\\, \\theta^{\\alpha-1}(1-\\theta)^{\\beta-1}}_{\\text{prior}}\n\\]\nSi omitimos las constantes de normalizando del likelihood y prior, obtenemos que el a posteriori es:\n\\[\np(\\theta \\mid y) \\propto \\theta^{\\alpha-1+y}(1-\\theta)^{\\beta-1+N−y}\n\\]\nPodemos ver que la expresión a la derecha de la proporcionalidad tiene la misma forma funcional (sin considerar la constante de proporcionalidad) que una distribución Beta con parámetros \\(\\alpha_{\\text{a posteriori}} = \\alpha_{\\text{a priori}} + y \\quad \\beta_{\\text{a posteriori}} = \\beta_{\\text{a priori}} + N - y\\).\nCuando se cumple que para un cierto likelihood la forma funcional del a priori y la del a posteriori coinciden se dice que el a priori es conjugado con el likelihood. Históricamente los problemas en estadística Bayesiana estuvieron restringidos al uso de a prioris conjugados, ya que estos garantizan la tratabilidad matemática del problema, es decir garantizan que es posible obtener una expresión analítica para nuestro problema. En el próximo capítulo veremos técnicas computacionales modernas que permiten calcular la distribución a posteriori incluso cuando no se usan a prioris conjugados. Estas técnicas computacionales han permitido el resurgimiento de la estadística Bayesiana en las últimas décadas.\n\n\n1.2.5 Notación y visualización de modelos Bayesianos\nPara representar modelos en estadística Bayesiana (y en probabilidad en general) se suele utilizar la siguiente notación\n\\[\n\\begin{align}\n\\theta \\sim & \\operatorname{Beta}(\\alpha, \\beta) \\\\\nY \\sim & \\operatorname{Bin}(n=1, p=\\theta)\n\\end{align}\n\\]\nEl símbolo \\(\\sim\\) indica que la variable a la izquierda se distribuye según la distribución a la derecha. Entonces podríamos decir que \\(\\mathbf{\\theta}\\) es una variable aleatoria con distribución \\(\\operatorname{Beta}\\), y que \\(\\operatorname{Beta}\\) está definida por los parámetros \\(\\alpha\\) y \\(\\beta\\), este es nuestro a priori. En la siguiente línea tenemos el likelihood el cual está definido por una distribución binomial con parámetros \\(n=1\\) y \\(p=\\theta\\).\nGráficamente esto se puede representar usando los diagramas de Kruschke:\n\nEn el primer nivel (de arriba hacia abajo) se observa el a priori, luego el likelihood, y por último los datos. Las flechas indican la vinculación entre las partes del modelo y el signo \\(\\sim\\) la naturaleza estocástica de las variables.\n\n\n1.2.6 Obteniendo los datos\nBien, ahora que sabemos cómo calcular el a posteriori, lo único que resta es conseguir los datos. En este ejemplo los datos son sintéticos, es decir los obtuve computacionalmente mediante un generador de números (pseudo)aleatorios, pero bien podrían haber surgido de un experimento con una moneda real.\n\n\n1.2.7 Calculando el a posteriori\nEn el próximo capítulo veremos cómo usar métodos computacionales para computar un a posteriori sin necesidad de derivarlo analíticamente. Esto es lo que haremos para resolver el resto de los problemas del curso. Pero dado que ya nos tomamos el trabajo de derivar analíticamente la expresión para el a posteriori vamos a usar esa expresión. Si miran el código de la siguiente celda verán que la mayoría de las lineas se encargan de dibujar los resultados y no de calcularlos. El cálculo del a posteriori ocurre en la línea 20. Cada una de estas lineas computa el a posteriori para cada uno de los a prioris que vimos antes. El cálculo es simple, tan solo se computa el valor del a posteriori (usando la función pdf de la distribución Beta provista por PreliZ) para 2000 puntos igualmente espaciados entre 0 y 1 (linea 9). El loop que empieza en la linea 11 se debe a que exploraremos cómo cambian las distribuciones a posteriori para distinta cantidad de datos (n_intentos). Con un círculo negro de contorno blanco se indica el valor real de \\(\\theta\\), valor que por supuesto es desconocido en una situación real, pero conocido para mí, ya que los datos son sintéticos.\n\nplt.figure(figsize=(12, 9))\n\nn_trials = [0, 1, 2, 3, 4, 8, 16, 32, 50, 150]\ndata = [0, 1, 1, 1, 1, 4, 6, 9, 13, 48]\ntheta_real = 0.35\n\nbeta_params = [(1, 1), (0.5, 0.5), (20, 20)]\ndist = pz.Beta\nx = np.linspace(0, 1, 2000)\n\nfor idx, N in enumerate(n_trials):\n    if idx == 0:\n        plt.subplot(4, 3, 2)\n        plt.xlabel('θ')\n    else:\n        plt.subplot(4, 3, idx+3)\n        plt.xticks([])\n    y = data[idx]\n    for (a_prior, b_prior) in beta_params:\n        posterior = dist(a_prior + y, b_prior + N - y).pdf(x)\n        plt.fill_between(x, 0, posterior, alpha=0.7)\n\n    plt.plot(theta_real, 0, ms=9, marker='o', mec='w', mfc='k')\n    plt.plot(0, 0, label=f'{N:4d} experimentos\\n{y:4d} caras', alpha=0)\n    plt.xlim(0, 1)\n    plt.ylim(0, 12)\n    plt.legend()\n    plt.yticks([])\n\n/home/tcicchini/anaconda3/envs/estadistica_e_inferencia_II/lib/python3.12/site-packages/numba/np/ufunc/dufunc.py:202: RuntimeWarning: divide by zero encountered in nb_logpdf\n  return super().__call__(*args, **kws)",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Inferencia Bayesiana</span>"
    ]
  },
  {
    "objectID": "01_Inferencia_Bayesiana.html#analizando-los-resultados",
    "href": "01_Inferencia_Bayesiana.html#analizando-los-resultados",
    "title": "1  Inferencia Bayesiana",
    "section": "1.3 Analizando los resultados",
    "text": "1.3 Analizando los resultados\nLa primer figura del panel muestra los a priori, nuestra estimación de \\(\\theta\\) dado que no hemos realizado ningún experimento. Las sucesivas nueve figuras muestran las distribuciones a posteriori y se indica la cantidad de experimentos y de caras obtenidas. Además se puede ver un círculo negro de contorno blanco en 0.35, la cual representa el valor verdadero de \\(\\theta\\). Por supuesto que en problemas reales este valor es desconocido.\nEste ejemplo es realmente ilustrativo en varios aspectos.\n\nEl resultado de un análisis Bayesiano NO es un solo valor, si no una distribución (a posteriori) de los valores plausibles de los parámetros (dado los datos y el modelo).\nLa dispersión o ancho de las curvas es una medida de la incertidumbre sobre los valores.\nEl valor más probable viene dado por la moda de la distribución (el pico de la distribución).\nAún cuando \\(\\frac{2}{1} = \\frac{8}{4}\\) son numéricamente iguales tenemos menor incertidumbre en un resultado cuando el número de experimentos es mayor.\nDada una cantidad suficiente de datos los resultados tienden a converger sin importar el a priori usado.\nLa rapidez con la que los resultados convergen varía. En este ejemplo las curvas azul y turquesa parecen converger con tan solo 8 experimentos, pero se necesitan más de 50 experimentos para que las tres curvas se muestren similares. Aún con 150 experimentos se observan ligeras diferencias.\nPartiendo de los a priori uniforme (azul) o sesgado (turquesa) y habiendo realizado un solo experimento y observado una sola cara, lo más razonable es pensar que estamos frente a una moneda con dos caras!\nLa situación cambia drásticamente al ver por primera vez una moneda caer ceca. Ahora lo más probable (dado cualquiera de los tres a prioris) es inferir que \\(\\theta=0.5\\). Los valores de \\(\\theta\\) exactamente 0 o 1 se vuelven imposibles.\nEl a priori naranja es más informativo que los otros dos (la distribución esta más concentrada), por ello se requiere de un número mas grande de experimentos para “moverlo”.\nEl a priori uniforme (azul) es lo que se conoce como no informativo. El resultado de un análisis Bayesiano usando un a priori no-informativos en general coinciden con los resultados de análisis frecuentistas (en este caso el valor esperado de \\(\\theta = \\frac{y}{N}\\)).\n\n\n1.3.1 Influencia y elección del a priori\nDe los ejemplos anteriores debería quedar claro que los a priori influencian los resultados de nuestros cálculos. Esto tiene total sentido si no fuese así no haría falta incluirlos en el análisis y todo sería más simple (aunque nos perderíamos la oportunidad de usar información previa). De los ejemplos anteriores también debería quedar claro que a medida que aumentan los datos (como las tiradas de monedas) los resultados son cada vez menos sensibles al a priori. De hecho, para una cantidad infinita de datos el a priori no tiene ningún efecto. Exactamente cuantos datos son necesarios para que el efecto del a priori sea despreciable varía según el problema y los modelos usados. En el ejemplo de la moneda se puede ver que 50 experimentos bastan para hacer que dos de los resultados sean prácticamente indistinguibles, pero hacen falta más de 150 experimentos para que los 3 resultados se vuelvan practicamente independientes del a priori. Esto es así por que los dos primeros a prioris son relativamente planos, mientras que el tercer a priori concentra casi toda la probabilidad en una región relativamente pequeña. El tercer a priori no solo considera que el valor más probable de \\(\\theta\\) es 0.5, si no que considera que la mayoría de los otros valores son muy poco probables. ¿Cómo cambiarían los resultados si hubiéramos usado como a priori \\(\\operatorname{Beta}(\\alpha=2, \\beta=2)\\)?\nLa elección de los a priori puede poner nervioso a quienes se inician en el análisis Bayesiano (o a los detractores de este paradigma). ¡El temor es que los a prioris censuren a los datos y no les permitan hablar por sí mismos! Eso está muy bien, pero el punto es que los datos no saben hablar, con suerte murmuran. Los datos solo tienen sentido a la luz de los modelos (matemáticos y mentales) usados para interpretarlos, y los a prioris son parte de esos modelos.\nHay quienes prefieren usar a priori no-informativos (también conocidos como a priori planos, vagos, o difusos). Estos a priori aportan la menor cantidad posible de información y por lo tanto tienen el menor impacto posible en el análisis. Si bien es posible usarlos, en general hay razones prácticas para no preferirlos. En este curso usaremos a priori ligeramente informativos siguendo las recomendaciones de Gelman, McElreath, Kruschke, y otros. En muchos problemas sabemos al menos algo de los valores posibles que pueden tomar nuestros parámetros, por ejemplo que solo pueden ser positivos, o que están restringidos a sumar 1 o el rango aproximado, etc. En esos casos podemos usar a prioris que introduzcan esta ligera información. En estos casos podemos pensar que la función del a priori es la de mantener las inferencias dentro de límites razonables. Estos a priori se suelen llamar regularizadores.\nPor supuesto que también es posible usar a prioris informativos (o fuertes). Hacer esto es razonable solo si contamos con información previa confiable. Esto puede ser ventajoso en casos en que los datos contengan poca información sobre el problema. Si la información no viene por el likelihood (datos), entonces puede venir por el a priori. A modo de ejemplo, en bioinformática estructural es común usar toda la información previa posible (de forma Bayesiana y no-Bayesiana) para resolver problemas. Esto es posible por la existencia de bases de datos que almacenan los resultados de cientos o miles experimentos realizados a lo largo de décadas de esfuerzo (¡No usar esta información sería casi absurdo!). En resumen, si contás con información confiable no hay razón para descartarla, menos si el argumento es algo relacionado con pretender ser objetivo (¡No hay objetividad en negar lo que se sabe!).\nHasta ahora hemos visto que es posible clasificar, aunque sea de forma vaga o aproximada, a los a priori en función de la información que contienen. Pero saber esta clasificación no necesariamente hace las cosas más simples a la hora de elegir un a priori. ¿Acaso no sería mejor eliminar los a prioris de nuestro análisis? Eso haría el asunto mucho mas simple. Bueno, el punto es que desde una perspectiva Bayesiana todos los modelos tienen a prioris, aun cuando no sean explícitos. De hecho muchos resultados de la estadística frecuentista pueden considerarse casos especiales de modelos Bayesianos usando a prioris planos. Volviendo a la figura anterior se puede ver que la moda del a posteriori para la curva azul. Coincide con la estimación (puntual) frecuentista para el valor de \\(\\theta\\)\n\\[\n\\hat \\theta = {{y} \\over {N}}\n\\]\nNotar que \\(\\hat \\theta\\) es una estimación puntual (un número) y no una distribución.\nEste ejemplo nos muestra que no es posible hacer análisis estadísticos y sacarse los a prioris de encima. Un posible corolario es que es más flexible y transparente especificar los a prioris de forma explícita que esconderlos bajo la cama. Al hacerlo ganamos mayor control sobre nuestro modelo, mayor transparencia y por el mismo precio la estimación de la incertidumbre con la que se estima cada parámetro.\nPor último, hay que recordar que el modelado estadístico (como otras formas de modelado) es un proceso iterativo e interactivo. Nada nos impide usar más de un a priori (o un likelihood) si así lo quisiéramos. Una parte importante del modelado es la de cuestionar los supuestos y los a prioris son simplemente un tipo de supuestos (como lo son los likelihoods). Si tuviéramos más de un a priori razonable podríamos realizar un análisis de sensibilidad, es decir evaluar como cambian los resultados con los a prioris, podríamos llegar a la conclusión que para un rango amplio de a prioris ¡los resultados no varían! Más adelante veremos varias herramientas para comparar distintos modelos.\nDado que los a prioris tienen un papel central en la estadística Bayesiana, seguiremos discutiéndolos a medida que vayamos viendo problemas concretos. Por lo que si esta discusión no ha aclarado todas tus dudas y seguís algo confundido, mejor mantener la calma y no preocuparse demasiado, este tema ha sido motivo de discusión y confusión durante décadas ¡y la discusión todavía continua!\n\n\n1.3.2 Cuantificando el peso del a priori\nEn general la distribución más familiar para la mayoría de las personas es la distribución Gaussiana, como esta distribución está definida por dos parámetros, la media y la dispersión de ese valor medio, suele resultarnos natural pensar las distribuciones en esos términos. Si queremos expresar la distribución Beta en función de la media y la dispersión podemos hacerlo de la siguiente forma:\n\\[\\begin{align}\n\\alpha &= \\mu \\kappa \\\\\n\\beta &= (1 - \\mu) \\kappa\n\\end{align}\\]\ndonde \\(\\mu\\) es la media y \\(\\kappa\\) es un parámetro llamado concentración. Por ejemplo si \\(\\mu=0.5\\) y \\(\\kappa=40\\), tenemos que:\n\\[\\begin{align}\n\\alpha = 0.5 \\times 40 &= 20 \\\\\n\\beta = (1-0.5) \\times 40 &= 20\n\\end{align}\\]\n\\(\\kappa\\) se puede interpretar como la cantidad de experimentos si/no que realizamos dándonos como resultado la media \\(\\mu\\). Es decir el a priori no sesgado (naranja) equivale a haber arrojado una moneda 40 veces y haber obtenido como media 0.5. Es decir que si usamos ese a priori recién al observar 40 experimentos si/no, los datos tendrán el mismo peso relativo que el a priori, por debajo de este número el a priori contribuye más que los datos al resultado final y por encima menos. El a priori azul (uniforme) equivale a haber observado a la moneda caer una vez cara y otra vez ceca (\\(\\kappa = 2\\)). Cuando \\(\\kappa &lt; 2\\), la cosa se pone un poco extraña, por ejemplo el a priori sesgado (turquesa) equivale a haber observado una sola moneda (\\(\\kappa = 1\\)) pero en una especie de, a falta de mejor analogía, ¡¿superposición cuántica de estados?!\n\n\n1.3.3 Resumiendo el a posteriori\nEl resultado de un análisis Bayesiano es siempre una distribución de probabilidad.\nA la hora de comunicar los resultados de un análisis Bayesiano, lo más informativo es reportar la distribución completa, aunque esto no siempre es posible o deseable, por ejemplo el a posteriori de una distribución multidimensional es imposible de visualizar de forma directa. Por lo tanto, es común recurrir a distintas medidas que resumen el a posteriori, por ejemplo la media, mediana, la desviación estándar, etc. También es común, e informativo, reportar un intervalo de credibilidad. Existen varios criterios para definir intervalos de credibilidad, el que usaremos en este curso (y que también es ampliamente usado en la literatura) es lo que se conoce como intervalo de más alta densidad y nos referiremos a él por su sigla en ingles, HDI (Highest Posterior Density interval). Un HDI es el intervalo, más corto, que contiene una porción fija de la densidad de probabilidad, generalmente el 95% (aunque otros valores como 90% o 50% son comunes). Cualquier punto dentro de este intervalo tiene mayor densidad que cualquier punto fuera del intervalo. Para una distribución unimodal, el HDI 95 es simplemente el intervalo entre los percentiles 2,5 y 97,5.\nArviZ es un paquete de Python para análisis exploratorio de modelos Bayesianos. ArviZ provee de funciones que facilitan analizar y resumir el a posteriori. Por ejemplo plot_posterior puede ser usado para generar un gráfico con la media y el HDI. En el siguiente ejemplo en vez de un a posteriori “real” estamos usando datos sintéticos generados de una distribución Beta.\n\nmock_posterior = pz.Beta(5, 11).rvs(size=1000)\naz.plot_posterior(mock_posterior, figsize=(8, 4));\n\n\n\n\n\n\n\n\nAhora que estamos aprendiendo que es un HDI por primera vez y antes de que automaticemos el concepto conviene aclarar un par de puntos.\n\nLa elección automática de 95% (o cualquier otro valor) es totalmente arbitraria. En principio no hay ninguna razón para pensar que describir el a posteriori con un HDI 95 sea mejor que describirlo con un HDI 98 o que no podamos usar valores como 87% o 66%. El valor de 95% es tan solo un accidente histórico. Como un sutil recordatorio de esto ArviZ usa por defecto el valor 94%!\nUn intervalo de credibilidad (que es Bayesiano) no es lo mismo que un intervalo de confianza (que es frecuentista). Un intervalo de confianza es un intervalo que se define según un nivel de confianza, en general del 95%. Un intervalo de confianza se construye de tal forma que si repitiéramos infinitas veces un experimento obtendríamos que la proporción de intervalos que contienen el valor verdadero del parámetro que nos interesa coincide con el nivel de confianza estipulado. Contra-intuitivamente esto no es lo mismo que decir que un intervalo en particular tiene una probabilidad \\(x\\) de contener el parámetro (esto sería la definición de un intervalo de credibilidad, que es Bayesiano). De hecho, un intervalo de confianza en particular contiene o no contiene al valor, la teoría frecuentista no nos deja hablar de probabilidades de los parámetros, ya que estos tienen valores fijos. Si no queda clara la diferencia no te hagas problema, la diferencia entre estos dos conceptos suele ser tan difícil de entender que en la práctica estudiantes y científicos por igual interpretan los intervalos de confianza (frecuentistas) como intervalos de credibilidad (Bayesianos).\n\n\nSi bien desde la perspectiva Bayesiana podemos afirmar que un intervalo de credibilidad nos permite asegurar que la probabilidad de un parámetro está acotado en cierto rango. Siempre hay que tener presente que dicha afirmación es correcta SOLO en sentido teórico. Es decir, solo si todos los supuestos contenidos en el modelo son ciertos. Una inferencia es siempre dependiente de los datos y modelos usados.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Inferencia Bayesiana</span>"
    ]
  },
  {
    "objectID": "01_Inferencia_Bayesiana.html#distribución-predictivas",
    "href": "01_Inferencia_Bayesiana.html#distribución-predictivas",
    "title": "1  Inferencia Bayesiana",
    "section": "1.4 Distribución predictivas",
    "text": "1.4 Distribución predictivas\nSi bien el objeto central de la estadística Bayesiana es la distribución a posteriori. Existen otras distribuciones muy importantes. Una de ellas es la distribución predictiva a posteriori, otra es la distribución predictiva a priori.\n\n1.4.1 Distribución predictivas a posteriori\nEsta distribución representa las predicciones \\(\\tilde{y}\\) de un modelo una vez obtenido el a posteriori. Se calcula de la siguiente manera:\n\\[\np(\\tilde{y}  \\mid  y) = \\int p(\\tilde{y} \\mid \\theta) p(\\theta \\mid y) d\\theta\n\\]\nEs decir integramos \\(\\theta\\) de acuerdo a la distribución a posteriori.\nComputacionalmente podemos generar muestras de esta distribución según el siguiente procedimiento:\n\nElegimos un valor de \\(\\theta\\) de acuerdo a la distribución a posteriori \\(p(\\theta \\mid y)\\)\nFijamos \\(\\theta\\) en la distribución que usamos como likelihood \\(p(\\tilde{y} \\mid \\theta)\\) y generamos una muestra aleatoria\nRepetimos desde 1, tantas veces como muestras necesitemos\n\nLos datos generados son predictivos ya que son los datos que se esperaría ver por ejemplo en un futuro experimento, es decir son variables no observadas pero potencialmente observables. Como veremos en el siguiente capítulo un uso muy común para la distribución predictiva a posteriori es compararla con los datos observados y así evaluar si el posterior calculado es razonable.\n\n\n1.4.2 Distribución predictiva a priori\nAsi como es posible generar datos sintéticos desde el a posteriori. Es posible hacerlo desde el prior. En este caso la distribución se llama distribución predictiva a priori. Y representa los datos \\(p(Y^\\ast)\\) que el modelo espera ver antes de haber visto los datos. O más formalmente antes de haber sido condicionado a los datos. Se calcula como:\n\\[\np(Y^\\ast) =  \\int_{\\Theta} p(Y^\\ast \\mid \\theta) \\; p(\\theta) \\; d\\theta\n\\]\nEs importante notar que la definición es muy similar a la distribución predictiva a posteriori, solo que ahora integramos a lo largo del prior en vez del posterior.\nLos datos generados son predictivos ya que son los datos que el modelo esperara ver, es decir son datos no observados pero potencialmente observables. Como veremos en el siguiente capítulo un uso muy común para la distribución predictiva a priori es compararla con nuestro conocimiento previo y así evaluar si el modelo es capaz de generar resultados razonable, incluso antes de haber incorporado los datos.\n\n\n1.4.3 Distribución predictiva a priori y a posterior para el problema de la moneda.\nEn el caso del modelo beta-binomial es posible obtener analíticamente tanto la distribución predictiva a priori como a posteriori y estas son:\n\\[\np(Y^\\ast) \\propto \\operatorname{Beta-binomial}(n=N, \\alpha_{a priori}, \\beta_{a priori})\n\\]\n\\[\np(\\tilde{Y}  \\mid  Y)  \\propto \\operatorname{Beta-binomial}(n=N, \\alpha_{a priori} + y, \\beta_{a priori} + N - y)\n\\]\nOmitiremos la discusión de como se obtienen estas distribuciones\n\n\n1.4.4 Cuarteto Bayesiano\nEl siguiente bloque de código computa las distribuciones a priori, a posteriori, predictiva a priori y predictiva a posteriori. En vez de usar la distribución \\(\\operatorname{Beta-binomial}\\) para las distribuciones predictivas hemos optado por usar una aproximación más computacional y muestrear primero de la distribuciones beta y luego de la binomial. Esperamos que esta decisión contribuya a comprender mejor que representan estas distribuciones.\nEs importante notar que mientras la distribuciones a priori y a posteriori son distribución sobre los parámetros en un modelo, la distribución predictivas a priori y a posteriori son distribuciones sobre los datos (predichos).\n\nfig, axes = plt.subplots(2, 2, figsize=(10, 8), sharex=\"row\", sharey=\"row\")\naxes = np.ravel(axes)\ndist = pz.Beta\na_prior = 1\nb_prior = 1\nN = 12\ny = 3\nx = np.linspace(0, 1, 100)\n\n\nprior = dist(a_prior, b_prior).pdf(x)\naxes[0].fill_between(x, 0, prior)\naxes[0].set_title(\"Prior\")\naxes[0].set_yticks([])\n\n\nposterior = dist(a_prior + y, b_prior + N - y).pdf(x)\naxes[1].fill_between(x, 0, posterior)\naxes[1].set_title(\"Posterior\")\n\n\nprior = dist(a_prior, b_prior).rvs(500)\nprior_predictive = np.hstack([pz.Binomial(n=N, p=p).rvs(N) for p in prior])\naxes[2].hist(prior_predictive, bins=range(0, N+2), rwidth=0.9, align=\"left\", density=True)\naxes[2].set_xlim(-0.5, N+0.5)  \naxes[2].set_title(\"Predictiva a priori\")\n\nposterior = dist(a_prior + y, b_prior + N - y).rvs(500)\nprior_predictive = np.hstack([pz.Binomial(n=N, p=p).rvs(N) for p in posterior])\naxes[3].hist(prior_predictive, bins=range(0, N+2), rwidth=0.9, align=\"left\", density=True)\naxes[3].set_xlim(-0.5, N+0.5)  \naxes[3].set_title(\"Predictiva a posteriori\");\n\nfig.suptitle(\"Cuarteto Bayesiano\", fontweight=\"bold\", fontsize=16);",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Inferencia Bayesiana</span>"
    ]
  },
  {
    "objectID": "01_Inferencia_Bayesiana.html#ejercicios",
    "href": "01_Inferencia_Bayesiana.html#ejercicios",
    "title": "1  Inferencia Bayesiana",
    "section": "1.5 Ejercicios",
    "text": "1.5 Ejercicios\n\nEl estadístico Bruno de Finetti declaró que “Las probabilidades no existen”. Edwin Jaynes, físico, declaró que la teoría de probabilidad es la lógica de la ciencia. Discutí estos enunciados.\nUsá pz.Beta().plot_interactive() para explorar distintas combinaciones de parámetros de la distribución Beta. Cuál es el efecto de los parámetros \\(\\alpha\\) y \\(\\beta\\)?\nLa media de la distribución Beta es \\(\\frac{\\alpha}{\\alpha+\\beta}\\). Cuál es la media de la distribución a posteriori para un modelo Beta-Binomial, con prior Beta(2, 5) y 10 experimentos con 6 caras?\nLa varianza de la distribución Beta es \\(\\frac{\\alpha \\beta}{(\\alpha+\\beta)^2(\\alpha+\\beta+1)}\\). Cuál es la varianza de la distribución a posteriori para un modelo Beta-Binomial, con prior Beta(2, 5) y 10 experimentos con 6 caras?\nContrastá los resultados de los puntos anteriores usando la función mean y var de la distribución Beta de PreliZ.\nEl valor esperado, o media, de una variable aleatoria continua con densidad \\(f(p)\\) se define como:\n\n\\[\\mathbb{E}(p) = \\int p f(p) dp\\]\nSi \\(p \\sim \\text{Beta}(\\alpha, \\beta)\\) encuentra la expresión para el valor esperado de \\(p\\).\n\nPreliZ tiene una función llamada maxent. Explicá que hace y generá un par de ejemplos\nUsá la siguiente función para explorar diversas combinaciones de priors y likelihoods. Enunciá las conclusiones que consideres más relevantes.\n\n\ndef a_posteriori_grilla(grilla=10, a=1, b=1, caras=6, tiradas=9):\n    grid = np.linspace(0, 1, grilla)\n    prior = pz.Beta(a, b).pdf(grid)\n    likelihood = pz.Binomial(n=tiradas, p=grid).pdf(caras)\n    posterior = likelihood * prior\n    posterior /= posterior.sum()\n    _, ax = plt.subplots(1, 3, sharex=True, figsize=(16, 4))\n    ax[0].set_title('caras = {}\\ntiradas = {}'.format(caras, tiradas))\n    for i, (e, e_n) in enumerate(zip([prior, likelihood, posterior], ['a priori', 'likelihood', 'a posteriori'])):\n        ax[i].set_yticks([])\n        ax[i].plot(grid, e, 'o-', label=e_n)\n        ax[i].legend(fontsize=14)\n\n\ninteract(a_posteriori_grilla, grilla=ipyw.IntSlider(min=2, max=100, step=1, value=15), a=ipyw.FloatSlider(min=1, max=7, step=1, value=1), b=ipyw.FloatSlider(\n    min=1, max=7, step=1, value=1), caras=ipyw.IntSlider(min=0, max=20, step=1, value=6), tiradas=ipyw.IntSlider(min=0, max=20, step=1, value=9));",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Inferencia Bayesiana</span>"
    ]
  },
  {
    "objectID": "02_Programación_probabilística.html",
    "href": "02_Programación_probabilística.html",
    "title": "2  Programación probabilista",
    "section": "",
    "text": "2.1 Introducción a PyMC\nPyMC es un paquete para programación probabilística bajo Python. PyMC es lo suficientemente madura para resolver muchos problemas estadísticos. PyMC permite crear modelos probabilísticos usando una sintaxis intuitiva y fácil de leer que es muy similar a la sintaxis usada para describir modelos probabilísticos.\nLa mayoría de las funciones de PyMC están escritas en Python. Mientras que las partes computacionalmente demandantes están escritas en NumPy y PyTensor. Pytensor es una biblioteca de Python que permite definir, optimizar y evaluar expresiones matemáticas que involucran matrices multidimensionales de manera eficiente. PyTensor es hija de Theano una librería de Python originalmente desarrollada para deep learning (que es a su vez la antecesora de TensorFlow, PyTorch, etc).",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Programación probabilista</span>"
    ]
  },
  {
    "objectID": "02_Programación_probabilística.html#introducción-a-pymc",
    "href": "02_Programación_probabilística.html#introducción-a-pymc",
    "title": "2  Programación probabilista",
    "section": "",
    "text": "2.1.1 El problema de la moneda, ahora usando PyMC y ArviZ\nA continuación revistaremos el problema de la moneda visto en el capítulo anterior, usando esta vez PyMC para definir nuestro modelo y hacer inferencia. Luego usaremos ArviZ para analizar el a posterori.\nA continuación generaremos datos sintéticos, en este caso asumiremos que conocemos el valor the \\(\\theta\\) y lo llamaremos theta_real, y luego intentaremos averiguar este valor como si no lo conociéramos. En un problema real theta_real sería desconocido y realizaríamos un proceso de inferencia precisamente para averiguar su valor.\n\nnp.random.seed(123)\nn_experimentos = 4\ntheta_real = 0.35  # en una situación real este valor es desconocido\ndatos = pz.Binomial(n=1, p=theta_real).rvs(size=n_experimentos)\ndatos\n\narray([1, 0, 0, 0])\n\n\n\n\n2.1.2 Creación del modelo\nAhora que tenemos nuestros datos es necesario especificar el modelo. Para ello usaremos una distribución beta (con parámetros \\(\\alpha=\\beta=1\\)) como a priori y la distribución de Bernoulli como likelihood. Usando la notación usual en estadística tenemos:\n\\[\\begin{align}\n\\theta &\\sim \\operatorname{Beta}(\\alpha=1, \\beta=1)\\\\\nY &\\sim \\operatorname{Bin}(n=1, p=\\theta)\n\\end{align}\\]\n\nCada uno de los elementos del array datos es un experimento de Bernoulli, es decir un experimento donde solo es posible obtener dos valores (0 o 1) si en cambio tuviera el número total de “caras” obtenidas en varios experimentos de Bernoulli podríamos modelar el likelihood como una distribución Binomial.\n\nEsto modelo se traduce casi literalmente a PyMC, veamos:\n\nwith pm.Model() as nuestro_primer_modelo:\n    θ = pm.Beta(\"θ\", alpha=1, beta=1)  # a priori\n    y = pm.Bernoulli(\"y\", p=θ, observed=datos)  # likelihood\n    # y = pm.Binomial('y',n=n_experimentos, p=θ, observed=sum(datos))\n\nEn la primer linea hemos creado un nuevo objeto llamado nuestro_primer_modelo. Este objeto contiene información sobre el modelo y las variables que lo conforman. PyMC usa el bloque with para indicar que todas las lineas que están dentro de él hacen referencia al mismo modelo (que en este caso se llama nuestro_primer_modelo).\nLa segunda linea de código, especifica el a priori, como pueden ver la sintaxis sigue de cerca a la notación matemática, la única diferencia es que el primer argumento es siempre una cadena que especifica el nombre de la variable aleatoria (el nombre es usado internamente por PyMC), este nombre siempre deberá coincidir con el nombre de la variable de Python a la que se le asigna. De no ser así el código correrá igual, pero puede conducir a errores y confusiones al analizar el modelo.\n\nEs importante recalcar que las variables de PyMC, como \\(\\theta\\), no son números sino objetos que representan distribuciones. Es decir objetos a partir de los cuales es posible calcular probabilidades y generar números aleatorios.\n\nEn la tercer linea de código se especifica el likelihood, que como verán es similar a la linea anterior con la diferencia que hemos agregado un argumento llamado observed al cual le asignamos nuestros datos. Esta es la forma de indicarle a PyMC cuales son los datos. Los datos pueden ser números, listas de Python, arrays de NumPy o data_frames de Pandas.\n\n\n2.1.3 Inferencia\nNuestro modelo ya está completamente especificado, lo único que nos resta hacer es obtener el a posteriori. En el capítulo anterior vimos como hacerlo de forma analítica, ahora lo haremos con métodos numéricos.\nEn PyMC la inferencia se realiza escribiendo las siguientes lineas:\n\nwith nuestro_primer_modelo:\n    idata = pm.sample(1000)\n\nAuto-assigning NUTS sampler...\nInitializing NUTS using jitter+adapt_diag...\nMultiprocess sampling (4 chains in 4 jobs)\nNUTS: [θ]\nSampling 4 chains for 1_000 tune and 1_000 draw iterations (4_000 + 4_000 draws total) took 1 seconds.\n\n\n\n\n\n\n\n    \n      \n      100.00% [8000/8000 00:00&lt;00:00 Sampling 4 chains, 0 divergences]\n    \n    \n\n\nPrimero llamamos al objeto que definimos como nuestro modelo (nuestro_primer_modelo), indicando de esta forma que es sobre ese objeto que queremos realizar la inferencia. En la segunda linea le indicamos a PyMC que deseamos 1000 muestras. Esta linea luce inocente, pero internamente PyMC está haciendo muchas cosas por nosotros. Algunas de las cuales son detalladas en el mensaje que se imprime en pantalla.\nVeamos este mensaje:\n\nLa primer linea indica que PyMC ha asignado el método de muestreo NUTS, el cual es un muy buen método para variables continuas.\nLa segunda linea nos da información sobre cómo se inicializaron los valores de NUTS. Un detalle que por ahora no nos preocupa.\nLa tercer linea indica que PyMC correrá cuatro cadenas en paralelo, es decir generará cuatro muestras independientes del a posteriori. Esta cantidad puede ser diferente en sus computadoras ya que es determinada automáticamente en función de los procesadores disponibles (que en mi caso, 4). sample tiene un argumento chains que permite modificar este comportamiento.\nLa cuarta linea indica qué variable ha sido asignada a cual método de muestreo. En este caso la información es redundante, ya que tenemos una sola variable, pero esto no siempre es así. PyMC permite combinar métodos de muestreo, ya sea de forma automática basado en propiedades de las variables a muestrear o especificado por el usuario usando el argumento step.\nLa quinta linea es una barra de progreso con varias métricas sobre la velocidad del muestreo, que en este caso (y para referencia futura) es muy alta. También indica la cantidad de cadenas usadas y la cantidad de divergencias. Tener 0 divergencias es ideal, más adelante discutiremos la razón.\nPor último tenemos un detalle de la cantidad de muestras generadas, aunque pedimos 1000 obtuvimos 8000, la razón es que es son 1000 por cadena (4 cadenas en mi caso), es decir 4000. Todavía nos queda explicar 4000 muestras extras, estas se corresponden a 1000 por cadena y son muestras que PyMC utiliza para auto-tunear el método de muestreo. Estás muestras son luego descartadas automáticamente ya que no son muestras representativas del posterior. La cantidad de pasos que se usan para tunear el algoritmo de muestro se puede cambiar con el argumento tune de la función pm.sample(.).\n\n\n\n2.1.4 Resumiendo el a posteriori\nPor lo general, la primer tarea a realizar luego de haber realizado un muestreo es evaluar como lucen los resultados. La función plot_forestplot de ArviZ es muy útil para esta tarea.\n\naz.plot_forest(idata, combined=True, figsize=(6, 2));\n\n\n\n\n\n\n\n\nEl punto indica la media, la linea gruesa el rango intercuartil y las lineas finas el HDI 94%\n\nEs importante notar que la variable y es una variable observada, es decir conocida. Mientras que en gráfico anterior estamos dibujando solo \\(\\theta\\) que es la única variables desconocida, y por lo tanto muestreada.\n\nSi quisiéramos un resumen numérico de los resultados podemos usar:\n\naz.summary(idata, kind=\"stats\")\n\n\n\n\n\n\n\n\n\nmean\nsd\nhdi_3%\nhdi_97%\n\n\n\n\nθ\n0.332\n0.176\n0.035\n0.646\n\n\n\n\n\n\n\n\nComo resultado obtenemos un DataFrame con los valores de la media, la desviación estándar y el intervalo HDI 94% (hdi_3 hdi_97).\nOtra forma de resumir visualmente el a posteriori es usar la función plot_posterior que viene con ArviZ, ya hemos utilizado esta distribución en el capítulo anterior para un falso a posteriori. Vamos a usarlo ahora con un posterior real. Por defecto, esta función muestra un histograma para variables discretas y KDEs para variables continuas. También obtenemos la media de la distribución (podemos preguntar por la mediana o moda usando el argumento point_estimate) y el 94% HDI como una línea negra en la parte inferior de la gráfica. Se pueden establecer diferentes valores de intervalo para el HDI con el argumento hdi_prob. Este tipo de gráfica fue presentado por John K. Kruschke en su gran libro “Doing Bayesian Data Analysis”.\n\naz.plot_posterior(idata);\n\n\n\n\n\n\n\n\n\n\n2.1.5 Accidentes mineros\nEste ejemplo está tomado del tutorial de PyMC.\nEl problema es el siguiente, tenemos un registro del número de accidentes en minas de carbón, ubicadas en el Reino Unido, que ocurrieron entre 1851 y 1962 (Jarrett, 1979). Se sospecha que la aplicación de ciertas regulaciones de seguridad tuvo como efecto una disminución en la cantidad de catástrofes. Por lo tanto nos interesa averiguar el año en que la tasa cambió y nos interesa estimar ambas tasas.\nLos datos son los siguientes, por un lado tenemos la variable accidentes que contiene la cantidad de accidentes por año y por el otro la variable años conteniendo el rango de años para los cuales tenemos datos. Si prestan atención verán que accidentes es un arreglo enmascarado (o masked array). Esto es un tipo especial de arreglo de NumPy donde cada elemento del arreglo contiene asociado un valor True o False el cual indica si el elemento debe o no ser usado durante cualquier tipo de operación. En este caso como faltan datos para dos años lo que se ha hecho es marcar esa falta de datos con un valor centinela de -999, esta es la forma de indicarle a PyMC la presencia de datos faltantes, alternativamente se pueden pasar los datos como un dataframe de Pandas conteniendo el valor especial NAN (que es el valor por defecto en Pandas para lidiar con datos faltantes).\nBien, pero para que molestarse con datos faltantes si en general es más fácil eliminarlos. una de las razones es que esto puede conducir a pérdida de información cuando por cada observación tenemos más de una variable o cantidad de interés. Por ejemplo si tenemos 50 sujetos a los que les hemos medido la presión, la temperatura y el ritmo cardíaco, pero sucede que para 4 de ellos no contamos con el datos de la presión (porque alguien se olvidó de medirlo o registrarlo, o porque el tensiómetro se rompió, o por lo que sea). Podemos eliminar esos cuatro sujetos del análisis y perder por lo tanto información sobre la presión y ritmo cardíaco, o podemos usar todos los datos disponibles y además estimar los valores de temperatura faltantes. En el contexto de la estadística Bayesiana los datos faltantes se tratan como un parámetro desconocido del modelo que puede ser estimado.\n\naccidentes = pd.Series([4, 5, 4, 0, 1, 4, 3, 4, 0, 6, 3, 3, 4, 0, 2, 6,\n                       3, 3, 5, 4, 5, 3, 1, 4, 4, 1, 5, 5, 3, 4, 2, 5,\n                       2, 2, 3, 4, 2, 1, 3, np.nan, 2, 1, 1, 1, 1, 3, 0, 0,\n                       1, 0, 1, 1, 0, 0, 3, 1, 0, 3, 2, 2, 0, 1, 1, 1,\n                       0, 1, 0, 1, 0, 0, 0, 2, 1, 0, 0, 0, 1, 1, 0, 2,\n                       3, 3, 1, np.nan, 2, 1, 1, 1, 1, 2, 4, 2, 0, 0, 1, 4,\n                       0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1])\naños = np.arange(1851, 1962)\n\n\nplt.plot(años, accidentes, \".\")\nplt.ylabel(\"Número de accidentes\")\nplt.xlabel(\"Año\");\n\n\n\n\n\n\n\n\nPara modelar los accidentes usaremos una distribución de Poisson. Como creemos que la cantidad media de accidentes es distinta antes y después de la introducción de regulaciones de seguridad usaremos dos valores de tasas medias de accidentes (\\(t_0\\) y \\(t_1\\)). Además deberemos estimar un punto de corte (\\(pc\\)) que dividirá los años para los cuales se aplica la tasa de accidentes \\(t_0\\) de los cuales se aplica la tasa \\(t_1\\):\n\\[\\begin{equation}\nA_t \\sim Poisson(tasa)\n\\end{equation}\\]\n\\[\\begin{equation}\ntasa = \\begin{cases}\nt_0, \\text{si } t \\ge pc,\\\\\nt_1, \\text{si } t \\lt pc\n\\end{cases}\n\\end{equation}\\]\nLos a prioris que usaremos serán:\n\\[\\begin{align}\nt_0 \\sim Expon(1) \\\\\nt_1 \\sim Expon(1) \\\\\npc \\sim U(A_0, A_1)\n\\end{align}\\]\nDonde la distribución uniforme es discreta y \\(A_0\\) y \\(A_1\\) corresponden al primer y último año considerado en el análisis respectivamente.\nGráficamente el modelo es:\n\nUna peculiaridad de la implementación de este modelo en PyMC es el uso de la función pm.switch (linea 10). Esta es en realidad una función de PyMC y equivale a un if else de Python. Si el primer argumento es True entonces devuelve el segundo argumento caso contrario el tercer argumento. Como resultado tenemos que tasa es un vector de longitud igual a la de años y cuyos elementos corresponden a una repetición \\(t_0\\) seguida de una repetición \\(t_1\\), la cantidad exacta de repeticiones de \\(t_0\\) y \\(t_1\\) está controlada por la condición \\(pc \\ge\\) años. De esta forma, podemos al muestrear \\(pc\\), modificar que años reciben cual tasa para el cálculo del likelihood.\n\nwith pm.Model() as modelo_cat:\n\n    pc = pm.DiscreteUniform(\"pc\", lower=años.min(), upper=años.max())\n\n    # Priors para las tasas antes y después del cambio.\n    t_0 = pm.Exponential(\"t_0\", 1)\n    t_1 = pm.Exponential(\"t_1\", 1)\n\n    # Asignamos las tasas a los años de acuerdo a pc\n    tasa = pm.Deterministic(\"tasa\", pm.math.switch(pc &gt;= años, t_0, t_1))\n\n    acc = pm.Poisson(\"acc\", tasa, observed=accidentes)\n    idata_cat = pm.sample(1000, random_seed=1791, idata_kwargs={\"log_likelihood\": True})\n\n/home/osvaldo/anaconda3/envs/bayes/lib/python3.10/site-packages/pymc/model.py:1384: RuntimeWarning: invalid value encountered in cast\n  data = convert_observed_data(data).astype(rv_var.dtype)\n/home/osvaldo/anaconda3/envs/bayes/lib/python3.10/site-packages/pymc/model.py:1407: ImputationWarning: Data in acc contains missing values and will be automatically imputed from the sampling distribution.\n  warnings.warn(impute_message, ImputationWarning)\nMultiprocess sampling (4 chains in 4 jobs)\nCompoundStep\n&gt;CompoundStep\n&gt;&gt;Metropolis: [pc]\n&gt;&gt;Metropolis: [acc_missing]\n&gt;NUTS: [t_0, t_1]\nSampling 4 chains for 1_000 tune and 1_000 draw iterations (4_000 + 4_000 draws total) took 2 seconds.\nThe rhat statistic is larger than 1.01 for some parameters. This indicates problems during sampling. See https://arxiv.org/abs/1903.08008 for details\nThe effective sample size per chain is smaller than 100 for some parameters.  A higher number is needed for reliable rhat and ess computation. See https://arxiv.org/abs/1903.08008 for details\n\n\n\n\n\n\n\n    \n      \n      100.00% [8000/8000 00:02&lt;00:00 Sampling 4 chains, 0 divergences]\n    \n    \n\n\n\nidata_cat\n\n\n            \n              \n                arviz.InferenceData\n              \n              \n              \n            \n                  \n                  posterior\n                  \n                  \n                      \n                          \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;xarray.Dataset&gt;\nDimensions:            (chain: 4, draw: 1000, acc_missing_dim_0: 2,\n                        tasa_dim_0: 111, acc_dim_0: 111)\nCoordinates:\n  * chain              (chain) int64 0 1 2 3\n  * draw               (draw) int64 0 1 2 3 4 5 6 ... 994 995 996 997 998 999\n  * acc_missing_dim_0  (acc_missing_dim_0) int64 0 1\n  * tasa_dim_0         (tasa_dim_0) int64 0 1 2 3 4 5 ... 106 107 108 109 110\n  * acc_dim_0          (acc_dim_0) int64 0 1 2 3 4 5 ... 105 106 107 108 109 110\nData variables:\n    pc                 (chain, draw) int64 1889 1886 1886 ... 1889 1892 1892\n    acc_missing        (chain, draw, acc_missing_dim_0) int64 0 0 0 1 ... 1 3 1\n    t_0                (chain, draw) float64 3.273 3.31 2.694 ... 2.881 2.946\n    t_1                (chain, draw) float64 0.8727 0.8444 ... 1.008 0.8809\n    tasa               (chain, draw, tasa_dim_0) float64 3.273 3.273 ... 0.8809\n    acc                (chain, draw, acc_dim_0) int64 4 5 4 0 1 4 ... 0 0 1 0 1\nAttributes:\n    created_at:                 2023-04-20T21:18:39.548868\n    arviz_version:              0.15.1\n    inference_library:          pymc\n    inference_library_version:  5.3.0\n    sampling_time:              2.365055799484253\n    tuning_steps:               1000xarray.DatasetDimensions:chain: 4draw: 1000acc_missing_dim_0: 2tasa_dim_0: 111acc_dim_0: 111Coordinates: (5)chain(chain)int640 1 2 3array([0, 1, 2, 3])draw(draw)int640 1 2 3 4 5 ... 995 996 997 998 999array([  0,   1,   2, ..., 997, 998, 999])acc_missing_dim_0(acc_missing_dim_0)int640 1array([0, 1])tasa_dim_0(tasa_dim_0)int640 1 2 3 4 5 ... 106 107 108 109 110array([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,  13,\n        14,  15,  16,  17,  18,  19,  20,  21,  22,  23,  24,  25,  26,  27,\n        28,  29,  30,  31,  32,  33,  34,  35,  36,  37,  38,  39,  40,  41,\n        42,  43,  44,  45,  46,  47,  48,  49,  50,  51,  52,  53,  54,  55,\n        56,  57,  58,  59,  60,  61,  62,  63,  64,  65,  66,  67,  68,  69,\n        70,  71,  72,  73,  74,  75,  76,  77,  78,  79,  80,  81,  82,  83,\n        84,  85,  86,  87,  88,  89,  90,  91,  92,  93,  94,  95,  96,  97,\n        98,  99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110])acc_dim_0(acc_dim_0)int640 1 2 3 4 5 ... 106 107 108 109 110array([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,  13,\n        14,  15,  16,  17,  18,  19,  20,  21,  22,  23,  24,  25,  26,  27,\n        28,  29,  30,  31,  32,  33,  34,  35,  36,  37,  38,  39,  40,  41,\n        42,  43,  44,  45,  46,  47,  48,  49,  50,  51,  52,  53,  54,  55,\n        56,  57,  58,  59,  60,  61,  62,  63,  64,  65,  66,  67,  68,  69,\n        70,  71,  72,  73,  74,  75,  76,  77,  78,  79,  80,  81,  82,  83,\n        84,  85,  86,  87,  88,  89,  90,  91,  92,  93,  94,  95,  96,  97,\n        98,  99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110])Data variables: (6)pc(chain, draw)int641889 1886 1886 ... 1889 1892 1892array([[1889, 1886, 1886, ..., 1888, 1887, 1888],\n       [1892, 1891, 1891, ..., 1890, 1888, 1890],\n       [1889, 1889, 1889, ..., 1890, 1890, 1890],\n       [1890, 1889, 1889, ..., 1889, 1892, 1892]])acc_missing(chain, draw, acc_missing_dim_0)int640 0 0 1 0 1 0 0 ... 2 0 2 0 2 1 3 1array([[[0, 0],\n        [0, 1],\n        [0, 1],\n        ...,\n        [3, 1],\n        [3, 1],\n        [3, 1]],\n\n       [[6, 1],\n        [6, 1],\n        [6, 3],\n        ...,\n        [2, 1],\n        [2, 1],\n        [2, 1]],\n\n       [[1, 1],\n        [1, 2],\n        [1, 0],\n        ...,\n        [4, 2],\n        [2, 2],\n        [2, 2]],\n\n       [[1, 0],\n        [1, 0],\n        [1, 0],\n        ...,\n        [2, 0],\n        [2, 1],\n        [3, 1]]])t_0(chain, draw)float643.273 3.31 2.694 ... 2.881 2.946array([[3.27300145, 3.31004382, 2.69385652, ..., 3.361489  , 3.40956827,\n        3.40956827],\n       [3.10592073, 3.36085222, 3.00534019, ..., 2.54307457, 2.51920588,\n        3.5372262 ],\n       [2.71227824, 2.71227824, 3.0856883 , ..., 2.86019813, 2.99960294,\n        2.89937231],\n       [2.86208697, 3.20730475, 2.95804455, ..., 3.36485285, 2.88077941,\n        2.94647035]])t_1(chain, draw)float640.8727 0.8444 ... 1.008 0.8809array([[0.87270139, 0.84441641, 1.0630722 , ..., 1.00885421, 1.00662329,\n        1.00662329],\n       [1.05576985, 0.91125974, 0.85414828, ..., 1.04231662, 1.05555764,\n        0.77176679],\n       [0.89805765, 0.89805765, 0.79840082, ..., 0.6720661 , 0.7872776 ,\n        0.72575752],\n       [0.75190232, 1.20079714, 0.77526223, ..., 0.73696447, 1.00761759,\n        0.88086743]])tasa(chain, draw, tasa_dim_0)float643.273 3.273 3.273 ... 0.8809 0.8809array([[[3.27300145, 3.27300145, 3.27300145, ..., 0.87270139,\n         0.87270139, 0.87270139],\n        [3.31004382, 3.31004382, 3.31004382, ..., 0.84441641,\n         0.84441641, 0.84441641],\n        [2.69385652, 2.69385652, 2.69385652, ..., 1.0630722 ,\n         1.0630722 , 1.0630722 ],\n        ...,\n        [3.361489  , 3.361489  , 3.361489  , ..., 1.00885421,\n         1.00885421, 1.00885421],\n        [3.40956827, 3.40956827, 3.40956827, ..., 1.00662329,\n         1.00662329, 1.00662329],\n        [3.40956827, 3.40956827, 3.40956827, ..., 1.00662329,\n         1.00662329, 1.00662329]],\n\n       [[3.10592073, 3.10592073, 3.10592073, ..., 1.05576985,\n         1.05576985, 1.05576985],\n        [3.36085222, 3.36085222, 3.36085222, ..., 0.91125974,\n         0.91125974, 0.91125974],\n        [3.00534019, 3.00534019, 3.00534019, ..., 0.85414828,\n         0.85414828, 0.85414828],\n...\n        [2.86019813, 2.86019813, 2.86019813, ..., 0.6720661 ,\n         0.6720661 , 0.6720661 ],\n        [2.99960294, 2.99960294, 2.99960294, ..., 0.7872776 ,\n         0.7872776 , 0.7872776 ],\n        [2.89937231, 2.89937231, 2.89937231, ..., 0.72575752,\n         0.72575752, 0.72575752]],\n\n       [[2.86208697, 2.86208697, 2.86208697, ..., 0.75190232,\n         0.75190232, 0.75190232],\n        [3.20730475, 3.20730475, 3.20730475, ..., 1.20079714,\n         1.20079714, 1.20079714],\n        [2.95804455, 2.95804455, 2.95804455, ..., 0.77526223,\n         0.77526223, 0.77526223],\n        ...,\n        [3.36485285, 3.36485285, 3.36485285, ..., 0.73696447,\n         0.73696447, 0.73696447],\n        [2.88077941, 2.88077941, 2.88077941, ..., 1.00761759,\n         1.00761759, 1.00761759],\n        [2.94647035, 2.94647035, 2.94647035, ..., 0.88086743,\n         0.88086743, 0.88086743]]])acc(chain, draw, acc_dim_0)int644 5 4 0 1 4 3 4 ... 0 0 1 0 0 1 0 1array([[[4, 5, 4, ..., 1, 0, 1],\n        [4, 5, 4, ..., 1, 0, 1],\n        [4, 5, 4, ..., 1, 0, 1],\n        ...,\n        [4, 5, 4, ..., 1, 0, 1],\n        [4, 5, 4, ..., 1, 0, 1],\n        [4, 5, 4, ..., 1, 0, 1]],\n\n       [[4, 5, 4, ..., 1, 0, 1],\n        [4, 5, 4, ..., 1, 0, 1],\n        [4, 5, 4, ..., 1, 0, 1],\n        ...,\n        [4, 5, 4, ..., 1, 0, 1],\n        [4, 5, 4, ..., 1, 0, 1],\n        [4, 5, 4, ..., 1, 0, 1]],\n\n       [[4, 5, 4, ..., 1, 0, 1],\n        [4, 5, 4, ..., 1, 0, 1],\n        [4, 5, 4, ..., 1, 0, 1],\n        ...,\n        [4, 5, 4, ..., 1, 0, 1],\n        [4, 5, 4, ..., 1, 0, 1],\n        [4, 5, 4, ..., 1, 0, 1]],\n\n       [[4, 5, 4, ..., 1, 0, 1],\n        [4, 5, 4, ..., 1, 0, 1],\n        [4, 5, 4, ..., 1, 0, 1],\n        ...,\n        [4, 5, 4, ..., 1, 0, 1],\n        [4, 5, 4, ..., 1, 0, 1],\n        [4, 5, 4, ..., 1, 0, 1]]])Indexes: (5)chainPandasIndexPandasIndex(Index([0, 1, 2, 3], dtype='int64', name='chain'))drawPandasIndexPandasIndex(Index([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,\n       ...\n       990, 991, 992, 993, 994, 995, 996, 997, 998, 999],\n      dtype='int64', name='draw', length=1000))acc_missing_dim_0PandasIndexPandasIndex(Index([0, 1], dtype='int64', name='acc_missing_dim_0'))tasa_dim_0PandasIndexPandasIndex(Index([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,\n       ...\n       101, 102, 103, 104, 105, 106, 107, 108, 109, 110],\n      dtype='int64', name='tasa_dim_0', length=111))acc_dim_0PandasIndexPandasIndex(Index([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,\n       ...\n       101, 102, 103, 104, 105, 106, 107, 108, 109, 110],\n      dtype='int64', name='acc_dim_0', length=111))Attributes: (6)created_at :2023-04-20T21:18:39.548868arviz_version :0.15.1inference_library :pymcinference_library_version :5.3.0sampling_time :2.365055799484253tuning_steps :1000\n                      \n                  \n            \n            \n            \n                  \n                  log_likelihood\n                  \n                  \n                      \n                          \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;xarray.Dataset&gt;\nDimensions:             (chain: 4, draw: 1000, acc_observed_dim_0: 109)\nCoordinates:\n  * chain               (chain) int64 0 1 2 3\n  * draw                (draw) int64 0 1 2 3 4 5 6 ... 994 995 996 997 998 999\n  * acc_observed_dim_0  (acc_observed_dim_0) int64 0 1 2 3 4 ... 105 106 107 108\nData variables:\n    acc_observed        (chain, draw, acc_observed_dim_0) float64 -1.708 ... ...\nAttributes:\n    created_at:                 2023-04-20T21:18:39.746180\n    arviz_version:              0.15.1\n    inference_library:          pymc\n    inference_library_version:  5.3.0xarray.DatasetDimensions:chain: 4draw: 1000acc_observed_dim_0: 109Coordinates: (3)chain(chain)int640 1 2 3array([0, 1, 2, 3])draw(draw)int640 1 2 3 4 5 ... 995 996 997 998 999array([  0,   1,   2, ..., 997, 998, 999])acc_observed_dim_0(acc_observed_dim_0)int640 1 2 3 4 5 ... 104 105 106 107 108array([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,  13,\n        14,  15,  16,  17,  18,  19,  20,  21,  22,  23,  24,  25,  26,  27,\n        28,  29,  30,  31,  32,  33,  34,  35,  36,  37,  38,  39,  40,  41,\n        42,  43,  44,  45,  46,  47,  48,  49,  50,  51,  52,  53,  54,  55,\n        56,  57,  58,  59,  60,  61,  62,  63,  64,  65,  66,  67,  68,  69,\n        70,  71,  72,  73,  74,  75,  76,  77,  78,  79,  80,  81,  82,  83,\n        84,  85,  86,  87,  88,  89,  90,  91,  92,  93,  94,  95,  96,  97,\n        98,  99, 100, 101, 102, 103, 104, 105, 106, 107, 108])Data variables: (1)acc_observed(chain, draw, acc_observed_dim_0)float64-1.708 -2.132 ... -0.8809 -1.008array([[[-1.70822553, -2.131956  , -1.70822553, ..., -1.00886322,\n         -0.87270139, -1.00886322],\n        [-1.70025194, -2.11272842, -1.70025194, ..., -1.01352594,\n         -0.84441641, -1.01352594],\n        [-1.90801508, -2.52647918, -1.90801508, ..., -1.00190918,\n         -1.0630722 , -1.00190918],\n        ...,\n        [-1.69000671, -2.08706059, -1.69000671, ..., -1.00003897,\n         -1.00885421, -1.00003897],\n        [-1.6812794 , -2.06413163, -1.6812794 , ..., -1.00002184,\n         -1.00662329, -1.00002184],\n        [-1.6812794 , -2.06413163, -1.6812794 , ..., -1.00002184,\n         -1.00662329, -1.00002184]],\n\n       [[-1.75073375, -2.22686146, -1.75073375, ..., -1.00149963,\n         -1.05576985, -1.00149963],\n        [-1.69012774, -2.08737107, -1.69012774, ..., -1.00418705,\n         -0.91125974, -1.00418705],\n        [-1.78183094, -2.29087809, -1.78183094, ..., -1.01179875,\n         -0.85414828, -1.01179875],\n...\n        [-1.83468837, -2.39323538, -1.83468837, ..., -1.06946468,\n         -0.6720661 , -1.06946468],\n        [-1.78373707, -2.29469505, -1.78373707, ..., -1.02645196,\n         -0.7872776 , -1.02645196],\n        [-1.81944907, -2.36439271, -1.81944907, ..., -1.04629684,\n         -0.72575752, -1.04629684]],\n\n       [[-1.83393653, -2.39182337, -1.83393653, ..., -1.03705118,\n         -0.75190232, -1.03705118],\n        [-1.72363481, -2.16764178, -1.72363481, ..., -1.01781152,\n         -1.20079714, -1.01781152],\n        [-1.79798468, -2.32289417, -1.79798468, ..., -1.02981618,\n         -0.77526223, -1.02981618],\n        ...,\n        [-1.68936975, -2.08542343, -1.68936975, ..., -1.04218007,\n         -0.73696447, -1.04218007],\n        [-1.8265897 , -2.37796672, -1.8265897 , ..., -1.00002887,\n         -1.00761759, -1.00002887],\n        [-1.80209233, -2.33092228, -1.80209233, ..., -1.00771557,\n         -0.88086743, -1.00771557]]])Indexes: (3)chainPandasIndexPandasIndex(Index([0, 1, 2, 3], dtype='int64', name='chain'))drawPandasIndexPandasIndex(Index([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,\n       ...\n       990, 991, 992, 993, 994, 995, 996, 997, 998, 999],\n      dtype='int64', name='draw', length=1000))acc_observed_dim_0PandasIndexPandasIndex(Index([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,\n       ...\n        99, 100, 101, 102, 103, 104, 105, 106, 107, 108],\n      dtype='int64', name='acc_observed_dim_0', length=109))Attributes: (4)created_at :2023-04-20T21:18:39.746180arviz_version :0.15.1inference_library :pymcinference_library_version :5.3.0\n                      \n                  \n            \n            \n            \n                  \n                  sample_stats\n                  \n                  \n                      \n                          \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;xarray.Dataset&gt;\nDimensions:                (chain: 4, draw: 1000, scaling_dim_0: 2,\n                            accept_dim_0: 2, accepted_dim_0: 2)\nCoordinates:\n  * chain                  (chain) int64 0 1 2 3\n  * draw                   (draw) int64 0 1 2 3 4 5 ... 994 995 996 997 998 999\n  * scaling_dim_0          (scaling_dim_0) int64 0 1\n  * accept_dim_0           (accept_dim_0) int64 0 1\n  * accepted_dim_0         (accepted_dim_0) int64 0 1\nData variables: (12/20)\n    scaling                (chain, draw, scaling_dim_0) float64 2.358 ... 2.585\n    reached_max_treedepth  (chain, draw) bool False False False ... False False\n    max_energy_error       (chain, draw) float64 0.265 0.09046 ... 0.4624\n    perf_counter_start     (chain, draw) float64 2.601e+04 ... 2.602e+04\n    energy                 (chain, draw) float64 176.4 176.6 ... 178.5 178.2\n    index_in_trajectory    (chain, draw) int64 -3 1 -2 -1 -2 -1 ... 0 2 2 2 2 -3\n    ...                     ...\n    n_steps                (chain, draw) float64 3.0 1.0 3.0 1.0 ... 3.0 3.0 3.0\n    accept                 (chain, draw, accept_dim_0) float64 0.007396 ... 0...\n    accepted               (chain, draw, accepted_dim_0) float64 0.0 0.0 ... 0.5\n    energy_error           (chain, draw) float64 -0.06509 0.09046 ... -0.104\n    largest_eigval         (chain, draw) float64 nan nan nan nan ... nan nan nan\n    step_size              (chain, draw) float64 0.8418 0.8418 ... 1.204 1.204\nAttributes:\n    created_at:                 2023-04-20T21:18:39.558212\n    arviz_version:              0.15.1\n    inference_library:          pymc\n    inference_library_version:  5.3.0\n    sampling_time:              2.365055799484253\n    tuning_steps:               1000xarray.DatasetDimensions:chain: 4draw: 1000scaling_dim_0: 2accept_dim_0: 2accepted_dim_0: 2Coordinates: (5)chain(chain)int640 1 2 3array([0, 1, 2, 3])draw(draw)int640 1 2 3 4 5 ... 995 996 997 998 999array([  0,   1,   2, ..., 997, 998, 999])scaling_dim_0(scaling_dim_0)int640 1array([0, 1])accept_dim_0(accept_dim_0)int640 1array([0, 1])accepted_dim_0(accepted_dim_0)int640 1array([0, 1])Data variables: (20)scaling(chain, draw, scaling_dim_0)float642.358 2.438 2.358 ... 2.144 2.585array([[[2.35794769, 2.43845855],\n        [2.35794769, 2.43845855],\n        [2.35794769, 2.43845855],\n        ...,\n        [2.35794769, 2.43845855],\n        [2.35794769, 2.43845855],\n        [2.35794769, 2.43845855]],\n\n       [[1.9487171 , 2.58486855],\n        [1.9487171 , 2.58486855],\n        [1.9487171 , 2.58486855],\n        ...,\n        [1.9487171 , 2.58486855],\n        [1.9487171 , 2.58486855],\n        [1.9487171 , 2.58486855]],\n\n       [[2.9282    , 2.30535855],\n        [2.9282    , 2.30535855],\n        [2.9282    , 2.30535855],\n        ...,\n        [2.9282    , 2.30535855],\n        [2.9282    , 2.30535855],\n        [2.9282    , 2.30535855]],\n\n       [[2.14358881, 2.58486855],\n        [2.14358881, 2.58486855],\n        [2.14358881, 2.58486855],\n        ...,\n        [2.14358881, 2.58486855],\n        [2.14358881, 2.58486855],\n        [2.14358881, 2.58486855]]])reached_max_treedepth(chain, draw)boolFalse False False ... False Falsearray([[False, False, False, ..., False, False, False],\n       [False, False, False, ..., False, False, False],\n       [False, False, False, ..., False, False, False],\n       [False, False, False, ..., False, False, False]])max_energy_error(chain, draw)float640.265 0.09046 ... -0.6745 0.4624array([[ 0.26504584,  0.09046142,  0.46798465, ...,  0.22325168,\n        -0.07519011,  1.3507017 ],\n       [-0.11731527,  0.6444133 ,  0.17705949, ..., -0.4767042 ,\n         0.07733244, -0.50792413],\n       [-0.46983184,  1.33427506,  0.41174189, ...,  1.70651699,\n        -0.76661906, -0.39112682],\n       [ 0.32258973,  0.35653055,  1.51618935, ...,  0.38004319,\n        -0.67445668,  0.46237617]])perf_counter_start(chain, draw)float642.601e+04 2.601e+04 ... 2.602e+04array([[26014.91047292, 26014.91148382, 26014.91232575, ...,\n        26015.87911283, 26015.87987525, 26015.88066041],\n       [26015.06253877, 26015.06348128, 26015.06439578, ...,\n        26016.17831508, 26016.17899803, 26016.17949004],\n       [26014.83830859, 26014.83938188, 26014.84018346, ...,\n        26015.825392  , 26015.826208  , 26015.82687544],\n       [26015.02727908, 26015.02858552, 26015.02973348, ...,\n        26016.12544131, 26016.12618101, 26016.12687694]])energy(chain, draw)float64176.4 176.6 178.5 ... 178.5 178.2array([[176.36555001, 176.6459368 , 178.51243729, ..., 179.84406587,\n        178.07559104, 181.37051764],\n       [178.78937655, 178.97206313, 179.92765349, ..., 178.60148204,\n        180.77395892, 179.25473734],\n       [177.06746079, 180.29252714, 177.8214689 , ..., 182.01800491,\n        179.75269857, 179.16297612],\n       [179.09410751, 178.91538598, 180.7228587 , ..., 178.93246036,\n        178.51951985, 178.20431289]])index_in_trajectory(chain, draw)int64-3 1 -2 -1 -2 -1 0 ... 0 2 2 2 2 -3array([[-3,  1, -2, ...,  2, -1,  0],\n       [-3, -2,  2, ..., -3, -1,  3],\n       [ 2,  0, -3, ..., -3,  1, -1],\n       [-3,  3, -3, ...,  2,  2, -3]])smallest_eigval(chain, draw)float64nan nan nan nan ... nan nan nan nanarray([[nan, nan, nan, ..., nan, nan, nan],\n       [nan, nan, nan, ..., nan, nan, nan],\n       [nan, nan, nan, ..., nan, nan, nan],\n       [nan, nan, nan, ..., nan, nan, nan]])lp(chain, draw)float64-175.4 -176.6 ... -176.9 -176.6array([[-175.41139297, -176.57599061, -177.89154262, ..., -179.02823666,\n        -177.99848681, -179.18144757],\n       [-178.65935626, -177.29025153, -179.37600142, ..., -178.44921053,\n        -180.18844401, -178.28848647],\n       [-176.46011085, -177.26077905, -175.90058188, ..., -180.86232685,\n        -177.73456282, -179.01067105],\n       [-177.68364881, -178.00208278, -176.31468939, ..., -178.20585816,\n        -176.94953554, -176.61512549]])step_size_bar(chain, draw)float641.007 1.007 1.007 ... 1.172 1.172array([[1.00729023, 1.00729023, 1.00729023, ..., 1.00729023, 1.00729023,\n        1.00729023],\n       [1.08286555, 1.08286555, 1.08286555, ..., 1.08286555, 1.08286555,\n        1.08286555],\n       [1.05223362, 1.05223362, 1.05223362, ..., 1.05223362, 1.05223362,\n        1.05223362],\n       [1.1719586 , 1.1719586 , 1.1719586 , ..., 1.1719586 , 1.1719586 ,\n        1.1719586 ]])diverging(chain, draw)boolFalse False False ... False Falsearray([[False, False, False, ..., False, False, False],\n       [False, False, False, ..., False, False, False],\n       [False, False, False, ..., False, False, False],\n       [False, False, False, ..., False, False, False]])process_time_diff(chain, draw)float640.0005549 0.0002336 ... 0.0003363array([[0.00055489, 0.00023365, 0.00047276, ..., 0.00038232, 0.0004018 ,\n        0.00020246],\n       [0.00049923, 0.00044608, 0.00043372, ..., 0.00033792, 0.00017472,\n        0.00037314],\n       [0.00054747, 0.00021153, 0.00051525, ..., 0.00040727, 0.00022437,\n        0.00045394],\n       [0.00067074, 0.00051242, 0.00061125, ..., 0.00037821, 0.00034662,\n        0.00033631]])perf_counter_diff(chain, draw)float640.0005654 0.0002338 ... 0.0003364array([[0.00056544, 0.00023377, 0.00047288, ..., 0.0003825 , 0.00040203,\n        0.00020259],\n       [0.0004992 , 0.00044613, 0.00043394, ..., 0.00033805, 0.00017483,\n        0.00037316],\n       [0.00054733, 0.0002119 , 0.00051491, ..., 0.00040753, 0.00022451,\n        0.00045401],\n       [0.00067041, 0.00051257, 0.00061126, ..., 0.00037832, 0.00034671,\n        0.00033643]])acceptance_rate(chain, draw)float640.9064 0.9135 0.7718 ... 1.0 0.8124array([[0.90639157, 0.91350957, 0.77182768, ..., 0.88353586, 0.98583535,\n        0.25905842],\n       [0.98955886, 0.76850148, 0.91835521, ..., 0.98134166, 0.9255821 ,\n        1.        ],\n       [1.        , 0.26334902, 0.82588129, ..., 0.45805148, 1.        ,\n        0.89739528],\n       [0.83775282, 0.81221993, 0.61178363, ..., 0.83165062, 1.        ,\n        0.81235333]])tree_depth(chain, draw)int642 1 2 1 2 2 2 2 ... 2 2 1 2 2 2 2 2array([[2, 1, 2, ..., 2, 2, 1],\n       [2, 2, 2, ..., 2, 1, 2],\n       [2, 1, 2, ..., 2, 1, 2],\n       [2, 2, 2, ..., 2, 2, 2]])n_steps(chain, draw)float643.0 1.0 3.0 1.0 ... 3.0 3.0 3.0 3.0array([[3., 1., 3., ..., 3., 3., 1.],\n       [3., 3., 3., ..., 3., 1., 3.],\n       [3., 1., 3., ..., 3., 1., 3.],\n       [3., 3., 3., ..., 3., 3., 3.]])accept(chain, draw, accept_dim_0)float640.007396 0.0 ... 0.1929 0.732array([[[7.39612819e-03, 0.00000000e+00],\n        [4.81757005e-01, 4.36350696e-01],\n        [1.00000000e+00, 0.00000000e+00],\n        ...,\n        [3.02994122e-02, 5.00000000e-01],\n        [3.15524455e+00, 3.78320328e-01],\n        [3.06370307e-01, 1.47077949e-01]],\n\n       [[3.74914457e-01, 7.26775219e+00],\n        [2.64087657e+00, 5.42411823e-02],\n        [1.01373740e-01, 8.18567118e-02],\n        ...,\n        [1.09606993e+00, 1.23133365e+00],\n        [2.32672273e-01, 5.45267664e-01],\n        [4.14571869e+00, 5.28322671e-01]],\n\n       [[1.00000000e+00, 5.22871476e-01],\n        [8.72268475e-02, 3.25327855e-01],\n        [4.92180078e-01, 2.47982802e+00],\n        ...,\n        [2.13478920e-01, 2.97234574e-01],\n        [6.59186005e-03, 7.33430580e-01],\n        [7.07448678e-04, 4.06632834e-03]],\n\n       [[2.13928968e-01, 1.49531966e+00],\n        [2.16730557e+00, 5.00000000e-01],\n        [1.00000000e+00, 5.00000000e-01],\n        ...,\n        [1.20579091e+00, 6.69148835e-01],\n        [7.47765382e-01, 3.68482237e-01],\n        [1.92939365e-01, 7.32034299e-01]]])accepted(chain, draw, accepted_dim_0)float640.0 0.0 1.0 0.5 ... 1.0 0.5 0.0 0.5array([[[0. , 0. ],\n        [1. , 0.5],\n        [1. , 0. ],\n        ...,\n        [1. , 0.5],\n        [1. , 0. ],\n        [1. , 0. ]],\n\n       [[1. , 1. ],\n        [1. , 0. ],\n        [0. , 0.5],\n        ...,\n        [1. , 0.5],\n        [1. , 0.5],\n        [1. , 0. ]],\n\n       [[1. , 0.5],\n        [0. , 0.5],\n        [0. , 1. ],\n        ...,\n        [0. , 0.5],\n        [0. , 0.5],\n        [0. , 0. ]],\n\n       [[1. , 0.5],\n        [1. , 0.5],\n        [1. , 0.5],\n        ...,\n        [1. , 0.5],\n        [1. , 0.5],\n        [0. , 0.5]]])energy_error(chain, draw)float64-0.06509 0.09046 ... -0.594 -0.104array([[-6.50865282e-02,  9.04614228e-02,  4.67984653e-01, ...,\n        -6.90994557e-02,  4.34232260e-02,  0.00000000e+00],\n       [ 3.18244988e-02, -6.90837604e-02,  3.89116988e-02, ...,\n         5.76026427e-02,  7.73324387e-02, -1.49912676e-01],\n       [-2.39403787e-01,  0.00000000e+00, -1.80410173e-01, ...,\n         1.15200873e+00, -7.66619061e-01,  3.67900783e-01],\n       [ 1.26164191e-01,  3.56530549e-01, -8.28935220e-05, ...,\n         3.80043189e-01, -5.94038760e-01, -1.04008828e-01]])largest_eigval(chain, draw)float64nan nan nan nan ... nan nan nan nanarray([[nan, nan, nan, ..., nan, nan, nan],\n       [nan, nan, nan, ..., nan, nan, nan],\n       [nan, nan, nan, ..., nan, nan, nan],\n       [nan, nan, nan, ..., nan, nan, nan]])step_size(chain, draw)float640.8418 0.8418 ... 1.204 1.204array([[0.84177244, 0.84177244, 0.84177244, ..., 0.84177244, 0.84177244,\n        0.84177244],\n       [1.09823017, 1.09823017, 1.09823017, ..., 1.09823017, 1.09823017,\n        1.09823017],\n       [1.09244341, 1.09244341, 1.09244341, ..., 1.09244341, 1.09244341,\n        1.09244341],\n       [1.2040221 , 1.2040221 , 1.2040221 , ..., 1.2040221 , 1.2040221 ,\n        1.2040221 ]])Indexes: (5)chainPandasIndexPandasIndex(Index([0, 1, 2, 3], dtype='int64', name='chain'))drawPandasIndexPandasIndex(Index([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,\n       ...\n       990, 991, 992, 993, 994, 995, 996, 997, 998, 999],\n      dtype='int64', name='draw', length=1000))scaling_dim_0PandasIndexPandasIndex(Index([0, 1], dtype='int64', name='scaling_dim_0'))accept_dim_0PandasIndexPandasIndex(Index([0, 1], dtype='int64', name='accept_dim_0'))accepted_dim_0PandasIndexPandasIndex(Index([0, 1], dtype='int64', name='accepted_dim_0'))Attributes: (6)created_at :2023-04-20T21:18:39.558212arviz_version :0.15.1inference_library :pymcinference_library_version :5.3.0sampling_time :2.365055799484253tuning_steps :1000\n                      \n                  \n            \n            \n            \n                  \n                  observed_data\n                  \n                  \n                      \n                          \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;xarray.Dataset&gt;\nDimensions:             (acc_observed_dim_0: 109)\nCoordinates:\n  * acc_observed_dim_0  (acc_observed_dim_0) int64 0 1 2 3 4 ... 105 106 107 108\nData variables:\n    acc_observed        (acc_observed_dim_0) int64 4 5 4 0 1 4 3 ... 1 0 0 1 0 1\nAttributes:\n    created_at:                 2023-04-20T21:18:39.562843\n    arviz_version:              0.15.1\n    inference_library:          pymc\n    inference_library_version:  5.3.0xarray.DatasetDimensions:acc_observed_dim_0: 109Coordinates: (1)acc_observed_dim_0(acc_observed_dim_0)int640 1 2 3 4 5 ... 104 105 106 107 108array([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,  13,\n        14,  15,  16,  17,  18,  19,  20,  21,  22,  23,  24,  25,  26,  27,\n        28,  29,  30,  31,  32,  33,  34,  35,  36,  37,  38,  39,  40,  41,\n        42,  43,  44,  45,  46,  47,  48,  49,  50,  51,  52,  53,  54,  55,\n        56,  57,  58,  59,  60,  61,  62,  63,  64,  65,  66,  67,  68,  69,\n        70,  71,  72,  73,  74,  75,  76,  77,  78,  79,  80,  81,  82,  83,\n        84,  85,  86,  87,  88,  89,  90,  91,  92,  93,  94,  95,  96,  97,\n        98,  99, 100, 101, 102, 103, 104, 105, 106, 107, 108])Data variables: (1)acc_observed(acc_observed_dim_0)int644 5 4 0 1 4 3 4 ... 0 0 1 0 0 1 0 1array([4, 5, 4, 0, 1, 4, 3, 4, 0, 6, 3, 3, 4, 0, 2, 6, 3, 3, 5, 4, 5, 3,\n       1, 4, 4, 1, 5, 5, 3, 4, 2, 5, 2, 2, 3, 4, 2, 1, 3, 2, 1, 1, 1, 1,\n       3, 0, 0, 1, 0, 1, 1, 0, 0, 3, 1, 0, 3, 2, 2, 0, 1, 1, 1, 0, 1, 0,\n       1, 0, 0, 0, 2, 1, 0, 0, 0, 1, 1, 0, 2, 3, 3, 1, 2, 1, 1, 1, 1, 2,\n       4, 2, 0, 0, 1, 4, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1])Indexes: (1)acc_observed_dim_0PandasIndexPandasIndex(Index([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,\n       ...\n        99, 100, 101, 102, 103, 104, 105, 106, 107, 108],\n      dtype='int64', name='acc_observed_dim_0', length=109))Attributes: (4)created_at :2023-04-20T21:18:39.562843arviz_version :0.15.1inference_library :pymcinference_library_version :5.3.0\n                      \n                  \n            \n            \n              \n            \n            \n\n\n\nax = az.plot_posterior(idata_cat, var_names=[\"~tasa\", \"~acc\"], figsize=(12, 6));\n\n\n\n\n\n\n\n\n\naz.summary(idata_cat, var_names=[\"~tasa\", \"~acc\"])\n\n\n\n\n\n\n\n\n\nmean\nsd\nhdi_3%\nhdi_97%\nmcse_mean\nmcse_sd\ness_bulk\ness_tail\nr_hat\n\n\n\n\npc\n1889.971\n2.430\n1885.000\n1894.000\n0.168\n0.119\n224.0\n232.0\n1.01\n\n\nacc_missing[0]\n2.379\n1.879\n0.000\n6.000\n0.097\n0.069\n364.0\n467.0\n1.01\n\n\nacc_missing[1]\n0.931\n0.983\n0.000\n3.000\n0.038\n0.028\n704.0\n827.0\n1.00\n\n\nt_0\n3.080\n0.286\n2.558\n3.625\n0.006\n0.004\n2130.0\n2528.0\n1.00\n\n\nt_1\n0.929\n0.118\n0.712\n1.151\n0.002\n0.002\n3090.0\n2802.0\n1.00\n\n\n\n\n\n\n\n\n\ntasa_mean = idata_cat.posterior[\"tasa\"].mean((\"chain\", \"draw\"))\ntasa_hdi = az.hdi(idata_cat.posterior[\"tasa\"].values)\npc_hdi = az.hdi(idata_cat.posterior[\"pc\"])[\"pc\"]\n\n_, ax = plt.subplots(figsize=(10, 5), sharey=True)\nax.plot(años, accidentes, \".\")\n\nax.set_ylabel(\"Número de accidentes\")\nax.set_xlabel(\"Año\")\n\nax.vlines(\n    idata_cat.posterior[\"pc\"].mean((\"chain\", \"draw\")),\n    accidentes.min(),\n    accidentes.max(),\n    color=\"C1\",\n    lw=2,\n)\n\n\nax.fill_betweenx(\n    [accidentes.min(), accidentes.max()], pc_hdi[0], pc_hdi[1], alpha=0.3, color=\"C1\"\n)\nax.plot(años, tasa_mean, \"k\", lw=2)\nax.fill_between(años, tasa_hdi[:, 0], tasa_hdi[:, 1], alpha=0.3, color=\"k\")\n\nfaltante0 = (\n    idata_cat.posterior[\"acc_missing\"].sel(acc_missing_dim_0=0).mean((\"chain\", \"draw\"))\n)\nfaltante1 = (\n    idata_cat.posterior[\"acc_missing\"].sel(acc_missing_dim_0=1).mean((\"chain\", \"draw\"))\n)\n\nax.plot(años[np.isnan(accidentes)], [faltante0, faltante1], \"C2s\");",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Programación probabilista</span>"
    ]
  },
  {
    "objectID": "02_Programación_probabilística.html#comparando-grupos",
    "href": "02_Programación_probabilística.html#comparando-grupos",
    "title": "2  Programación probabilista",
    "section": "2.2 Comparando grupos",
    "text": "2.2 Comparando grupos\nUna tarea común al analizar datos es comparar grupos. Podríamos estar interesados en analizar los resultados de un ensayo clínico donde se busca medir la efectividad de una droga, o la reducción de la cantidad de accidentes de tránsito al introducir un cambio en las regulaciones de tránsito, o el desempeño de estudiantes bajo diferentes aproximaciones pedagógicas, etc. Este tipo de preguntas se suele resolver en el marco de lo que se conoce como pruebas de hipótesis que busca declarar si una observación es estadísticamente significativa o no. Nosotros tomaremos una ruta alternativa.\nAl comparar grupos debemos decidir que característica(s) vamos a usar. Una característica común es la media de cada grupo. En ese caso podemos calcular la distribución a posteriori de la diferencia entre medias. Para ayudarnos a entender este posterior usaremos 3 herramientas:\n\nUn posteriorplot con un valor de referencia\nUna medida llamada d de Cohen\nLa probabilidad de superioridad\n\nEn el capítulo anterior ya vimos un ejemplo de cómo usar posteriorplot con un valor de referencia, pronto veremos otro ejemplo. Las novedades aquí son el d de Cohen y la probabilidad de superioridad, dos maneras populares de expresar el tamaño del efecto.\n\n2.2.1 d de Cohen\nUna medida muy común, al menos en ciertas disciplinas, para cuantificar el tamaño del efecto es el d de Cohen\n\\[\n\\frac{\\mu_2 - \\mu_1}{\\sqrt{\\frac{\\sigma_2^2 + \\sigma_1^2}{2}}}\n\\]\nDe acuerdo con esta expresión, el tamaño del efecto es la diferencia de las medias con respecto a la desviación estándar combinada de ambos grupos. Ya que es posible obtener una distribución a posteriori de medias y de desviaciones estándar, también es posible calcular una distribución a posteriori de los valores d de Cohen. Por supuesto, si sólo necesitamos o queremos una estimación puntual, podríamos calcular la media de esa distribución a posteriori. En general, al calcular una desviación estándar combinada, se toma en cuenta el tamaño de la muestra de cada grupo explícitamente, pero la ecuación de d de Cohen omite el tamaño de la muestra, la razón es que tomamos estos valores del posterior (por lo que ya estamos considerando la incertidumbre de las desviaciones estándar).\n\nUn d de Cohen es una forma de medir el tamaño del efecto donde la diferencia de las medias se estandariza al considerar las desviaciones estándar de ambos grupos.\n\nCohen introduce la variabilidad de cada grupo al usar sus desviaciones estándar. Esto es realmente importante, una diferencia de 1 cuando la desviación estándar es de 0.1 es muy grande en comparación con la misma diferencia cuando la desviación estándar es 10. Además, un cambio de x unidades de un grupo respecto del otro podría explicarse por cada punto desplazándose exactamente x unidades o la mitad de los puntos sin cambiar mientras la otra mitad cambia 2x unidades, y así con otras combinaciones. Por lo tanto, incluir las variaciones intrínsecas de los grupos es una forma de poner las diferencias en contexto. Re-escalar (estandarizar) las diferencias nos ayuda a dar sentido a la diferencia entre grupos y facilita evaluar si el cambio es importante, incluso cuando no estamos muy familiarizados con la escala utilizada para las mediciones.\nUn d de Cohen se puede interpretar como un Z-score. Un Z-score es la cantidad de desviaciones estándar que un valor difiere del valor medio de lo que se está observando o midiendo, puede ser positivo o negativo dependiendo de si la diferencia es por exceso o por defecto. Por lo tanto, un d de Cohen de -1.2, indica que la media de un grupo está 1.2 desviación estándar por debajo de la media del otro grupo.\nIncluso con las diferencias de medias estandarizadas, puede ser necesario tener que calibrarnos en función del contexto de un problema determinado para poder decir si un valor de d de Cohen es grande, pequeño, mediano, importante, despreciable, etc. Afortunadamente, esta calibración se puede adquirir con la práctica, a modo de ejemplo si estamos acostumbrados a realizar varios análisis para más o menos el mismo tipo de problemas, podemos acostumbrarnos a un d de Cohen de entre 0.8 y 1.2, de modo que si obtenemos un valor de 2 podría ser que estamos frente a algo importante, inusual (¡o un error!). Una alternativa es consultar con expertos en el tema.\nUna muy buena página web para explorar cómo se ven los diferentes valores de Cohen’s es http://rpsychologist.com/d3/cohend. En esa página, también encontrarán otras formas de expresar el tamaño del efecto; algunas de ellos podrían ser más intuitivas, como la probabilidad de superioridad que analizaremos a continuación.\n\n\n2.2.2 Probabilidad de superioridad\nEsta es otra forma de informar el tamaño del efecto y se define como la probabilidad que un dato tomado al azar de un grupo tenga un valor mayor que un punto tomado al azar del otro grupo. Si suponemos que los datos que estamos utilizando se distribuyen de forma Gaussiana, podemos calcular la probabilidad de superioridad a partir de la d de Cohen usando la expresión:\n\\[\\begin{equation} \\label{eq_ps}\nps = \\Phi \\left ( \\frac{\\delta}{\\sqrt{2}} \\right)\n\\end{equation}\\]\nDonde \\(\\Phi\\) es la distribución normal acumulada y \\(\\delta\\) es el d de Cohen. Podemos calcular una estimación puntual de la probabilidad de superioridad (lo que generalmente se informa) o podemos calcular la distribución a posteriori. Si no estamos de acuerdo con la suposición de normalidad, podemos descartar esta fórmula y calcularla directamente a partir del posterior sin necesidad de asumir ninguna distribución. Esta es una de las ventajas de usar métodos de muestreo para estimar el a posteriori, una vez obtenidas las muestras lo que podemos hacer con ellas es muy flexible.\n\n\n2.2.3 El conjunto de datos tips\nPara explorar el tema de esta sección, vamos a usar el conjunto de datos tips (propinas). Estos datos fueron informados por primera vez por Bryant, P. G. and Smith, M (1995) Practical Data Analysis: Case Studies in Business Statistics.\nQueremos estudiar el efecto del día de la semana sobre la cantidad de propinas en un restaurante. Para este ejemplo, los diferentes grupos son los días. Comencemos el análisis cargando el conjunto de datos como un DataFrame de Pandas usando solo una línea de código. Si no está familiarizado con Pandas, el comando tail se usa para mostrar las últimas filas de un DataFrame:\n\ntips = pd.read_csv(\"datos/propinas.csv\")\ntips.tail()\n\n\n\n\n\n\n\n\n\ntotal_bill\ntip\nsex\nsmoker\nday\ntime\nsize\n\n\n\n\n239\n29.03\n5.92\nMale\nNo\nSat\nDinner\n3\n\n\n240\n27.18\n2.00\nFemale\nYes\nSat\nDinner\n2\n\n\n241\n22.67\n2.00\nMale\nYes\nSat\nDinner\n2\n\n\n242\n17.82\n1.75\nMale\nNo\nSat\nDinner\n2\n\n\n243\n18.78\n3.00\nFemale\nNo\nThur\nDinner\n2\n\n\n\n\n\n\n\n\nPara este ejemplo solo vamos a usar las columnas day y tip y vamos a usar la función plot_forest de ArviZ. Aún cuando ArviZ está pensado para análisis de modelos Bayesianos algunos de sus funciones pueden ser útiles para analizar datos.\n\naz.plot_forest(\n    tips.pivot(columns=\"day\", values=\"tip\").to_dict(\"list\"),\n    kind=\"ridgeplot\",\n    hdi_prob=1,\n    figsize=(12, 4),\n);\n\n\n\n\n\n\n\n\nA fin de simplificar el análisis vamos a crear 2 variables: * La variable categories contiene los nombres de los días (abreviados y en inglés) * La variable idx codifica los días de la semana como enteros entre 0 y 3.\n\ncategories = np.array([\"Thur\", \"Fri\", \"Sat\", \"Sun\"])\n\ntip = tips[\"tip\"].values\nidx = pd.Categorical(tips[\"day\"], categories=categories).codes\n\nEl modelo para este problema es basicamente igual a model_g, con la diferencia que \\(\\mu\\) y \\(\\sigma\\) ahora serán vectores en vez de escalares. La sintáxis de PyMC es super-útil para estos caso, en vez de usar for loops escribimos el modelo de forma vectorizada, para ello especificamos el argumento shape para los priors \\(\\mu\\) y \\(\\sigma\\) y para el likelihood usamos la variable idx para indexar de forma adecuada \\(\\mu\\) y \\(\\sigma\\) para asegurar que usamos los parámetros correctos para cada grupo. En este ejemplo un \\(\\mu\\) para jueves, otra para viernes, otra para sábado y una cuarta para domingo, y lo mismo para \\(\\sigma\\).\n    with pm.Model() as comparing_groups:\n        μ = pm.Normal('μ', mu=0, sigma=10, shape=4)\n        σ = pm.HalfNormal('σ', sigma=10, shape=4)\n\n        y = pm.Normal('y', mu=μ[idx], sigma=σ[idx], observed=tip)\nPyMC provee una sintaxis alternativa, la cual consisten en especificar coordenadas y dimensiones. La ventaja de esta alternativa es que permite una mejor integración con ArviZ.\nVeamos, en este ejemplo tenemos 4 valores para las medias y 4 para las desviaciones estándar, y por eso usamos shape=4. El InferenceData tendrá 4 indices 0, 1, 2, 3 correspondientes a cada uno de los 4 días. Pero es trabajo del usuario asociar esos indices numéricos con los días.\nAl usar coordenadas y dimensiones nosotros podremos usar los rótulos 'Thur', 'Fri', 'Sat', 'Sun' para referirnos a los parámetros relacionados con cada uno de estos días. ArviZ también podrá hacer uso de estos rótulos. Vamos a especificar dos coordenadas days con las dimensiones 'Thur', 'Fri', 'Sat', 'Sun' y “days_flat” que contendrá los mismo rótulos pero repetidos según el orden y longitud que corresponda con cada observación. Esto último será útil para poder obtener pruebas predictivas a posteriori para cada día.\n\ncoords = {\"days\": categories, \"days_flat\": categories[idx]}\n\nwith pm.Model(coords=coords) as comparing_groups:\n    μ = pm.HalfNormal(\"μ\", sigma=5, dims=\"days\")\n    σ = pm.HalfNormal(\"σ\", sigma=1, dims=\"days\")\n\n    y = pm.Gamma(\"y\", mu=μ[idx], sigma=σ[idx], observed=tip, dims=\"days_flat\")\n\n    idata_cg = pm.sample()\n    idata_cg.extend(pm.sample_posterior_predictive(idata_cg))\n\nAuto-assigning NUTS sampler...\nInitializing NUTS using jitter+adapt_diag...\nMultiprocess sampling (4 chains in 4 jobs)\nNUTS: [μ, σ]\nSampling 4 chains for 1_000 tune and 1_000 draw iterations (4_000 + 4_000 draws total) took 3 seconds.\nSampling: [y]\n\n\n\n\n\n\n\n    \n      \n      100.00% [8000/8000 00:02&lt;00:00 Sampling 4 chains, 0 divergences]\n    \n    \n\n\n\n\n\n\n\n    \n      \n      100.00% [4000/4000 00:00&lt;00:00]\n    \n    \n\n\nUna vez obtenido un a posteriori podemos hacer todos los análisis que creamos pertinentes con el. Primero hagamos una prueba predictiva a posteriori. Vemos que en general somos capaces de capturar la forma general de las distribuciones, pero hay detalles que se nos escapan. Esto puede deberse al tamaño relativamente pequeño de la muestra, a que hay otros factores además del día que tienen influencia en las propinas o una combinación de ambas. Por ahora seguiremos con el análisis considerando que el modelo es lo suficientemente bueno\n\n_, axes = plt.subplots(2, 2)\naz.plot_ppc(\n    idata_cg,\n    num_pp_samples=100,\n    coords={\"days_flat\": [categories]},\n    flatten=[],\n    ax=axes,\n);\n\n\n\n\n\n\n\n\nPodemos ver la distribución de cada uno de los parámetros haciendo\n\naz.plot_posterior(idata_cg, var_names=\"μ\", figsize=(12, 3));\n\n\n\n\n\n\n\n\nLa figura anterior es bastante informativa, por ejemplo vemos que los valores medios de las propinas difieren en solo unos pocos centavos y que para los domingos el valor es ligeramente más alto que para el resto de los días analizados.\nPero quizá consideramos que puede ser mejor mostrar los datos de otra forma. Por ejemplo podemos calcular todas las diferencias de medias a posteriori entre si. Además podríamos querer usar alguna medida del tamaño del efecto que sea popular entre nuestra audiencia, como podrían ser la probabilidad de superioridad o d de Cohen.\nCohen’s d\n\\[\n\\frac{\\mu_2 - \\mu_1}{\\sqrt{\\frac{\\sigma_1^2 + \\sigma_2^2}{2}}}\n\\]\n\nSe puede interpretar como un z-score. Cuántas desviaciones estándar una media de un grupo está por encima (o por debajo) de la media del otro grupo\nEjemplo interactivo\n\nProbabilidad de superioridad\n\nLa probabilidad que un dato tomado de un grupo sea mayor que la de un dato tomado del otro grupo.\nSi suponemos que los datos se distribuyen normalmente, entonces:\n\n\\[\n\\text{ps} = \\Phi \\left ( \\frac{\\delta}{\\sqrt{2}} \\right)\n\\]\n\\(\\Phi\\) es la cdf de una distribución normal \\(\\delta\\) es el valor del Cohen’s d.\nCon el siguiente código usamos plot_posterior para graficar todas las diferencias no triviales o redundantes. Es decir evitamos las diferencias de un día con sigo mismo y evitamos calcular ‘Fri - Thur’ si ya hemos calculado ‘Thur- Fri’. Si lo viéramos como una matriz de diferencias solo estaríamos calculando la porción triangular superior.\n\ncg_posterior = az.extract(idata_cg)\n\ndist = pz.Normal(0, 1)\n\ncomparisons = [(categories[i], categories[j]) for i in range(4) for j in range(i+1, 4)]\n\n_, axes = plt.subplots(3, 2, figsize=(13, 9), sharex=True)\n\nfor (i, j), ax in zip(comparisons, axes.ravel()):\n    means_diff = cg_posterior[\"μ\"].sel(days=i) - cg_posterior['μ'].sel(days=j)\n    \n    d_cohen = (means_diff /\n               np.sqrt((cg_posterior[\"σ\"].sel(days=i)**2 + \n                        cg_posterior[\"σ\"].sel(days=j)**2) / 2)\n              ).mean().item()\n    \n    ps = dist.cdf(d_cohen/(2**0.5))\n    az.plot_posterior(means_diff.values, ref_val=0, ax=ax)\n    ax.set_title(f\"{i} - {j}\")\n    ax.plot(0, label=f\"Cohen's d = {d_cohen:.2f}\\nProb sup = {ps:.2f}\", alpha=0)\n    ax.legend(loc=1)\n\n\n\n\n\n\n\n\nUna forma de interpretar estos resultados es comparando el valor de referencia con el intervalo HDI. De acuerdo con la figura anterior, tenemos solo un caso cuando el 94% HDI excluye el valor de referencia de cero, la diferencia en las propinas entre el jueves y el domingo. Para todos los demás ejemplos, no podemos descartar una diferencia de cero (de acuerdo con los criterios de superposición de valores de referencia de HDI). Pero incluso para ese caso, ¿es una diferencia promedio de ≈0.5 dólares lo suficientemente grande? ¿Es suficiente esa diferencia para aceptar trabajar el domingo y perder la oportunidad de pasar tiempo con familiares o amigos? ¿Es suficiente esa diferencia para justificar promediar las propinas durante los cuatro días y dar a cada mozo/a la misma cantidad de dinero de propina? Este tipo de preguntas es crucial para interpretar los datos y/o tomar decisiones, pero las respuestas no las puede ofrecer la estadística de forma automática (ni ningún otro procedimiento). La estadística solo pueden ayudar en la interpretación y/o toma de decisiones.\nNota: Dependiendo del público el gráfico anterior puede que esté demasiado “cargado”, quizá es útil para una discusión dentro del equipo de trabajo, pero para un público en general quizá convenga sacar elementos o repartir la información entre una figura y una tabla o dos figuras.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Programación probabilista</span>"
    ]
  },
  {
    "objectID": "02_Programación_probabilística.html#resumen",
    "href": "02_Programación_probabilística.html#resumen",
    "title": "2  Programación probabilista",
    "section": "2.3 Resumen",
    "text": "2.3 Resumen\nAunque la estadística Bayesiana es conceptualmente simple, los modelos probabilísticos a menudo conducen a expresiones analíticamente intratables. Durante muchos años, esta fue una gran barrera que obstaculizó la adopción amplia de métodos Bayesianos. Afortunadamente, la matemática, la física y la informática vinieron al rescate en forma de métodos numéricos capaces, al menos en principio, de resolver cualquier inferencia. La posibilidad de automatizar el proceso de inferencia ha llevado al desarrollo de los lenguajes de programación probabilista que permiten una clara separación entre la definición del modelo y la inferencia.\nPyMC es una librería de Python para programación probabilística con una sintaxis simple, intuitiva y fácil de leer que también está muy cerca de la sintaxis estadística utilizada para describir modelos probabilísticos. En este capítulo introducimos PyMC revisando el problema de la moneda que vimos en el capítulo anterior. La diferencia es que no tuvimos que derivar analíticamente la distribución a posteriori. Los modelos en PyMC se definen dentro de un bloque with; para agregar una distribución de probabilidad a un modelo, solo necesitamos escribir una línea de código. Las distribuciones se pueden combinar y se pueden usar como priors (variables no observadas) o likelihoods (variables observadas). En la sintaxis de PyMC la única diferencia entre ambas es que para esta última debemos pasar los datos usando el argumento observed. Si todo va bien las muestras generadas por PyMC serán representativas de la distribución a posteriori y por lo tanto serán una representación de las consecuencias lógicas del modelo y los datos.\nArviZ es una librería que nos ayuda a explorar los modelos definidos por PyMC (u otras librerías como PyStan, TFP, BeanMachine, etc). Una forma de usar el posterior para ayudarnos a tomar decisiones es comparando la ROPE con el intervalo HDI. También mencionamos brevemente la noción de funciones de pérdida, una aproximación formal para cuantificar los costos y beneficios asociados a la toma de decisiones. Aprendimos que las funciones de pérdida y las estimaciones puntuales están íntimamente asociadas.\nHasta este momento todos los ejemplos estuvieron basado en modelos con un solo parámetro. Sin embargo PyMC permite, en principiop, usar un número arbitrario de parámetros, esto lo ejemplificamos con un modelo Gaussiano y luego una generalización de este, el modelo t de Student. La distribución t de Student suele usarse como alternativa a la Gaussiana cuando queremos hacer inferencias robustas a valores aberrantes. Pronto veremos cómo se puede usar estos modelos como para construir regresiones lineales.\nFinalizamos comparando medias entre grupos, una tarea común en análisis de datos. Si bien esto a veces se enmarca en el contexto de las pruebas de hipótesis, tomamos otra ruta y trabajamos este problema como una inferencia del tamaño del efecto.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Programación probabilista</span>"
    ]
  },
  {
    "objectID": "02_Programación_probabilística.html#ejercicios",
    "href": "02_Programación_probabilística.html#ejercicios",
    "title": "2  Programación probabilista",
    "section": "2.4 Ejercicios",
    "text": "2.4 Ejercicios\n\nUsando PyMC reproducí los resultados del primer capítulo para el problema de la moneda (use los 3 priors usados en ese capítulo).\nReemplazá la distribución beta por una uniforme en el intervalo [0, 1] ¿Cómo cambia la velocidad del muestreo? ¿Y si se usas un intervalo más ámplio, como [-3, 3]?\nPara el modelo_g. Usá una Gaussiana para la media, centrada en la media empírica. Probá modificar la desviación estándard de ese prior ¿Cuán robusto/sensible son los resultados a la elección del prior?\nLa Gaussiana es una distribución sin límites es decir es válida en el intervalo \\([-\\infty, \\infty]\\), en el ejemplo anterior la usamos para modelar datos que sabemos tienen límites ¿Qué opinas de esta elección?\nCalculá la probabilidad de superioridad a partir de las muestras del posterior (sin usar la formula de probabilidad de superioridad a partir de la d de Cohen). Comparar los resultados con los valores obtenidos a analíticamente.\nAplica al menos uno de los modelos visto en este capítulo a datos propios o de tu interés.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Programación probabilista</span>"
    ]
  },
  {
    "objectID": "03_Modelos_jerárquicos.html",
    "href": "03_Modelos_jerárquicos.html",
    "title": "3  Modelado Jerárquico",
    "section": "",
    "text": "3.1 Jerarquías en los datos\nAl analizar datos es común encontrarse con situaciones como las siguientes:\nEn todos estos casos lo que tenemos es una agrupamiento o jerarquía “natural”. Los datos están agrupados en diferentes niveles. En el caso de los estudiantes, los datos están agrupados por escuela, ciudad y provincia. En el caso de los pacientes, los datos están agrupados por hospital. En el caso de los corredores tenemos mediciones repetidas de un mismo individuo a lo largo del tiempo, es decir los datos están agrupados por corredor.\nEs muy común que al analizar estos datos ignoremos la estructura jerárquica de los datos y decidamos o bien agrupar los datos (ej, todos los corredores en un mismo grupo) o bien analizarlos por separado (cada corredor, independiente del resto). Agrupar todos los datos nos permite obtener estimaciones más precisas, el costo es que perdemos los detalles de cada grupo. Al agrupar datos también corremos el riesgo de opacar diferencias o incluso obtener conclusiones diametralmente opuestas (ver paradoja de Simpson). Tratar a cada grupo por separado tiene la ventaja que podemos capturar las particularidades de caa grupo, pero las estimaciones de los parámetros serán menos precisas y corremos el riesgo de sobre-ajuste.\n¿Es posible construir un modelo que contemple que los datos están formados por grupos diferentes, pero que al mismo tiempo comparten información?\nSi, y ese es precisamente el tema principal de este capítulo. Este tipo de modelos se conocen como modelos jerárquicos. En la literatura es posible encontrar el mismo concepto bajo diferentes nombres; modelos multinivel, modelos de efectos mixtos, modelos de efectos aleatorios o modelos anidados. Lamentablemente algunos autores utilizan estos términos de forma intercambiable, mientras que otros los utilizan para marcar algunas diferencias. Nosotros utilizaremos el término modelo jerárquico para referirnos a cualquier modelo que contemple la estructura jerárquica de los datos.\nUn modelo jerárquico se construye asignado distribuciones a priori a las distribuciones a priori! Este nivel superior de distribuciones a priori se suelen denominar hiper-priors. Conceptualmente al utilizar un hiper-prior, estamos asumiendo que los priors de cada grupo no son nesariamente idénticos, pero si están vinculados al provenir de una población (o mecanismo generarador de datos) común. La siguiente figura muestra un diagrama con las diferencias entre un modelo agrupado (un solo grupo), un modelo no agrupado (todos los grupos separados) y un modelo jerárquico.\nEl introducir hiper-priors induce una distribución a posteriori donde las estimaciones de cad grupo estarán parcialmente agrupadas, es decir en algún punto entre el modelo agrupado y el modelo no agrupado. Para generar intuición sobre este punto construyamos un modelo no-jerarquico y uno jerárquico y compararemos los resultados.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Modelado Jerárquico</span>"
    ]
  },
  {
    "objectID": "03_Modelos_jerárquicos.html#jerarquías-en-los-datos",
    "href": "03_Modelos_jerárquicos.html#jerarquías-en-los-datos",
    "title": "3  Modelado Jerárquico",
    "section": "",
    "text": "Datos de desempeño deportivo. Por ejemplo datos de los mismos corredores en diferentes años.\nDatos de salud. Por ejemplo datos de pacientes en diferentes hospitales.\nDatos de rendimiento escolar. Podríamos tener información sobre el rendimiento de los estudiantes en diferentes escuelas, de diferentes ciudades y en diferentes provincias.\n\n\n\n\n\n\n\n\n\nDiagrama que muestra las diferencias entre un modelo agrupado, un modelo no agrupado y un modelo jerárquico.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Modelado Jerárquico</span>"
    ]
  },
  {
    "objectID": "03_Modelos_jerárquicos.html#desplazamientos-jerárquicos",
    "href": "03_Modelos_jerárquicos.html#desplazamientos-jerárquicos",
    "title": "3  Modelado Jerárquico",
    "section": "3.2 Desplazamientos jerárquicos",
    "text": "3.2 Desplazamientos jerárquicos\nLas proteínas son moléculas formadas por 20 unidades, llamadas amino ácidos, cada amino ácido puede aparecer en una proteína 0 o más veces. Así como una melodía está definida por una sucesión de notas musicales, una proteína está definida por una sucesión de amino ácidos. Algunas variaciones de notas pueden dar como resultados pequeñas variaciones sobre la misma melodía, otras variaciones pueden resultar en melodías completamente distintas, algo similar sucede con las proteínas. Una forma de estudiar proteínas es usando resonancia magnética nuclear (la misma técnica usada para imágenes médicas). Esta técnica permite medir diversos observables, uno de ellos se llama desplazamiento químico y para simplificar diremos que podemos medir tantos desplazamientos químicos como amino ácidos tenga una proteína. Los aminoácidos son una familia de compuestos químicos por lo que tendría sentido tratarlos a todos de igual forma, pero al mismo tiempo tienen diferentes propiedades químicas, las cuales de hecho son relevantes para comprender como funcionan las proteínas! Por lo que también tiene sentido tratarlos por separado. Como ya vimos una alternativa es construir un modelo jerárquico y hacer algo a mitad de camino.\nEl siguiente conjunto de datos contiene valores de desplazamientos químicos para un conjunto de proteínas. Si inspeccionan el DataFrame cs_data verán que tiene 4 columnas:\n\nLa primera es un código que identifica a la proteína (si tienen curiosidad pueden ingresar el identificador en esta base de datos https://www.rcsb.org).\nLa segunda columna tiene el nombre del amino ácido (pueden corroborar que hay tan solo 20 nombres únicos).\nLa tercera contiene valores teóricos de desplazamientos químicos (calculados usando métodos cuánticos).\nLa cuarta tiene valores experimentales.\n\nLa motivación de este ejemplo es comparar las diferencias entre valores teóricos y experimentales, entre otras razones para evaluar la capacidad de los métodos teóricos para reproducir valores experimentales.\n\ncs_data = pd.read_csv('datos/chemical_shifts_theo_exp.csv')\ndiff = cs_data.theo - cs_data.exp\ncat_encode = pd.Categorical(cs_data['aa'])\nidx = cat_encode.codes\ncoords = {\"aa\": cat_encode.categories}\n\nPara resaltar la diferencia entre un modelo jerárquico y uno no-jerárquico vamos a construir ambos. Primero el no-jerárquico. Este modelo es equivalente a haber ajustado cada uno de los aa grupos por separado.\n\nwith pm.Model(coords=coords) as cs_nh:         \n    μ = pm.Normal('μ', mu=0, sigma=1, dims=\"aa\") \n    σ = pm.HalfNormal('σ', sigma=2, dims=\"aa\") \n \n    y = pm.Normal('y', mu=μ[idx], sigma=σ[idx], observed=diff) \n     \n    idata_cs_nh = pm.sample()\n\nAuto-assigning NUTS sampler...\nInitializing NUTS using jitter+adapt_diag...\nMultiprocess sampling (4 chains in 4 jobs)\nNUTS: [μ, σ]\nSampling 4 chains for 1_000 tune and 1_000 draw iterations (4_000 + 4_000 draws total) took 20 seconds.\n\n\n\n\n\n\n\n\n\n\n\nY ahora la novedad. El modeloo jerárquico.\nEste modelo tiene un hiper-prior para la media de \\(\\mu\\) (μ_mu) y otro para la desviación estándar de \\(\\mu\\) (μ_sd). Para \\(\\sigma\\) no usamos un hiper-prior, asumimos un valor común del parámetro para todos los grupos. Esto es una decisión de modelado. En este caso la justificación es mantener el ejemplo simple, pero en principio no sería problemático usar un hiper-prior también para \\(\\sigma\\).\n\nwith pm.Model(coords=coords) as cs_h:\n    # hyper_priors\n    μ_mu = pm.Normal('μ_mu', mu=0, sigma=2)\n    μ_sd = pm.HalfNormal('μ_sd', 2)\n\n    # priors\n    μ = pm.Normal('μ', mu=μ_mu, sigma=μ_sd, dims=\"aa\") \n    σ = pm.HalfNormal('σ', sigma=2, dims=\"aa\") \n\n    y = pm.Normal('y', mu=μ[idx], sigma=σ[idx], observed=diff) \n\n    idata_cs_h = pm.sample()\n\nAuto-assigning NUTS sampler...\nInitializing NUTS using jitter+adapt_diag...\nMultiprocess sampling (4 chains in 4 jobs)\nNUTS: [μ_mu, μ_sd, μ, σ]\nSampling 4 chains for 1_000 tune and 1_000 draw iterations (4_000 + 4_000 draws total) took 26 seconds.\n\n\n\n\n\n\n\n\n\n\n\nAntes de observar los resultados, comparemos gráficamente ambos modelos, para estar seguros que entendemos en que difieren. La siguiente figura muestra una representación gráfica de los modelos cs_nh (no-jerárquico) y cs_h (jerárquico). Se puede ver como el modelo jerárquico tiene un nivel más.\n\n\n\nAhora que tenemos más claras las diferencias a nivel del modelo, vamos a explorar las consecuencias de esas diferencias. Para ello vamos a comparar los resultados usando un forest plot. ArviZ nos ofrece la función plot_forest que permite pasar más de un modelo, esto es útil cuando queremos comparar los valores de parámetros equivalentes entre modelos como en el presente ejemplo. Por defecto, plot_forest grafica las cadenas de MCMC por separado, esto no es relevante en este caso por lo que vamos a combinarlas usando combined=True. Los invito a explorar, por su cuenta, el significado del resto de los argumentos que pasamos en la siguiente celda.\n\naxes = az.plot_forest([idata_cs_nh, idata_cs_h], model_names=['no_jerárquico', 'jerárquico'],\n                      var_names='μ', combined=True, r_hat=False, ess=False, figsize=(10, 7),\n                      colors='cycle')\ny_lims = axes[0].get_ylim()\naxes[0].vlines(idata_cs_h.posterior['μ_mu'].mean(), *y_lims, color=\"k\", ls=\":\");\n\n\n\n\n\n\n\n\nBien, tenemos un gráfico para 38 valores medios estimados, uno por aminoácido (19 en este conjunto de datos) y esto duplicado ya que tenemos dos modelos. También tenemos los intervalos de credibilidad del 94% y el rango intercuartil (el intervalo que contiene el 50% central de la distribución). La línea vertical es la media parcialmente agrupada, es decir la media según el modelo jerárquico. El valor es cercano a cero, esto es parte de lo que esperaríamos ver si los valores teóricos son buenos reproduciendo los valores experimentales.\nLa parte más relevante de este gráfico es que las estimaciones del modelo jerárquico son atraídas hacia la media parcialmente agrupada o, de forma equivalente, se contraen con respecto a las estimaciones no agrupadas. Este efecto es más notorio para los grupos más alejados de la media (como PRO), además la incertidumbre es igual o menor que la del modelo no jerárquico. Decimos que las estimaciones están parcialmente agrupadas porque tenemos una estimación para cada grupo, pero las estimaciones para cada grupo se informan mutuamente mediante el hiper prior. Por lo tanto, se obtiene una situación intermedia entre tener un solo grupo (la media global), todos los aminoácidos juntos, y tener 20 grupos separados, uno por aminoácido (el modelo no jerárquico).\n\n\n\n\n\n\nNota\n\n\n\nEn un modelo jerárquico, los grupos que comparten un hiperprior común comparten información a través de ese hiperprior. Esto da como resultado una contracción de las estimaciones, respecto del modelo desagrupado. Es decir, las estimaciones individuales se contraen hacia la media común. Al agrupar parcialemente las estimaciones, estamos modelando los grupos en algún punto medio entre grupos independientes unos de otros y un solo gran grupo.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Modelado Jerárquico</span>"
    ]
  },
  {
    "objectID": "03_Modelos_jerárquicos.html#es-deseable-tener-contracción",
    "href": "03_Modelos_jerárquicos.html#es-deseable-tener-contracción",
    "title": "3  Modelado Jerárquico",
    "section": "3.3 Es deseable tener contracción?",
    "text": "3.3 Es deseable tener contracción?\nUn modelo jerárquico ofrece estimaciones más conservadoras y más robustas. Por ejemplo, si un grupo tiene un tamaño de muestra pequeño, la estimación de ese grupo será más incierta que la de un grupo con un tamaño de muestra grande. En un modelo jerárquico, la información de los grupos con mayor tamaño de muestra se comparte con los grupos con menor tamaño de muestra, lo que resulta en una estimación más precisa para los grupos con menor tamaño de muestra. La cantidad exacta de contracción dependerá de varios factores (ver ejercicios), pero en general, la contracción es deseable ya que reduce la varianza de las estimaciones y reduce las posibilidades de sobreajuste.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Modelado Jerárquico</span>"
    ]
  },
  {
    "objectID": "03_Modelos_jerárquicos.html#intercambiabilidad",
    "href": "03_Modelos_jerárquicos.html#intercambiabilidad",
    "title": "3  Modelado Jerárquico",
    "section": "3.4 Intercambiabilidad",
    "text": "3.4 Intercambiabilidad\nUna secuencia, finita o infinita de variables aleatorias \\(X_1, X_2, X_3, \\dots X_j\\) es intercambiable si su distribución de probabilidad conjunta no se ve modificada por permutaciones de los indices \\((1, \\dots, J)\\).\nUn ejemplo típico de variables intercambiables es el siguiente:\nTenemos una bolsa con una bola blanca y una negra. La probabilidad de sacar cualquiera de ellas es de 0.5. Si \\(Y_i = 1\\) indica que la iésima bola es blanca y muestreando sin reemplazo, tendremos que:\n\\[P(X_1=1, X_2=0) = 0.5\\] \\[P(X_1=0, X_2=1) = 0.5\\]\nEs decir la probabilidad de tomar primero la blanca y luego la negra es igual a tomar primero la blanca y luego la negra. Es decir \\(X_1\\) e \\(X_2\\) son intercambiables.\nAhora bien también es cierto que:\n\\[0 = P(X_2=1 \\mid X_1=1) \\not= P(X_2=1) = 0.5\\]\nEs decir la probabilidad marginal de que la segunda bola sea blanca no es la misma que la probabilidad (condicional) de que la segunda sea blanca dado que ya obtuvimos una blanca. Es decir \\(X_1\\) e \\(X_2\\) no son independientes.\n\nToda secuencia iid es también intercambiable, pero una secuencia intercambiable no es necesariamente iid. La condición de intercambialidad es más general (o menos estricta) que la de independencia.\n\nEste enunciado se puede demostrar de la siguiente forma:\nSea \\(x_i \\mathop{\\sim}\\limits^{iid} p(x)\\), tenemos que la probabilidad conjunta se calcula como el producto de las probabilidades marginales:\n\\[p(x_i, \\dots , x_n) = \\prod_i^n p(x_i)\\]\nDado que el producto es commutativo, tenemos que el resultado es invariante a permutaciones.\nOK, todo muy lindo, pero y que tiene que ver esto con modelos jerárquicos? Por ahora parece que nada, pero avancemos un poco más.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Modelado Jerárquico</span>"
    ]
  },
  {
    "objectID": "03_Modelos_jerárquicos.html#teorema-de-de-finetti",
    "href": "03_Modelos_jerárquicos.html#teorema-de-de-finetti",
    "title": "3  Modelado Jerárquico",
    "section": "3.5 Teorema de De Finetti",
    "text": "3.5 Teorema de De Finetti\nUna secuencia de variables aleatorias es intercambiable si y solo si para todo \\(n\\) podemos escribir:\n\\[p(X_1, X_2,  \\dots , X_n) = \\int \\prod_i^n p(X_i \\mid \\theta) \\; p(\\theta)\\]\nEs decir, que conjunto de variable aleatorias puede ser descrito por:\n\nun parámetro \\(\\theta\\)\nun likelihood \\(p(X \\mid \\theta)\\)\nun prior p()\n\nEntonces podemos ver al teorema de De Finetti como una justificación de la estadística Bayesiana y como una garantía de que si tenemos una secuencia de observaciones/variables aleatorias intercambiables entoces podremos describirlas adecuadamente usando estadística Bayesiana. La trampa, por que siempre hay una trampa, es que el teorema no nos dice nada sobre como elegir ni el parámetro, ni el likelihood ni el prior. Existen algunas justificaciones teóricas para elegir estos elementos, pero no lo discutiremos en este curso, principalmente por su limitada utilidad práctica.\nSupongamos ahora que \\(\\theta\\) representa un conjunto de parámetros, \\(\\theta = (\\theta_1, \\theta_2,  \\dots , \\theta_n)\\) y que este conjunto de parámetros es intercambiable. Entonces podemos escribir:\n\\[(\\theta_1, \\theta_2,  \\dots , \\theta_n) = \\int \\prod_i^n p(\\theta_i \\mid \\psi) \\; p(\\psi)\\]\nObteniendo un modelo jerárquico que podríamos reescribir como:\n\\[\\begin{align*}\nX_{ij} &\\sim p(X \\mid \\theta_i) \\\\\n\\theta_i &\\sim p(\\theta \\mid \\psi)  \\\\\n\\psi &\\sim p(\\psi)\n\\end{align*}\\]",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Modelado Jerárquico</span>"
    ]
  },
  {
    "objectID": "03_Modelos_jerárquicos.html#intercambiabilidad-e-inferencia",
    "href": "03_Modelos_jerárquicos.html#intercambiabilidad-e-inferencia",
    "title": "3  Modelado Jerárquico",
    "section": "3.6 Intercambiabilidad e inferencia",
    "text": "3.6 Intercambiabilidad e inferencia\nYo he seleccionado 10 valores de desplazamientos químicos \\(y_1, \\dots, y_{10}\\). Qué me pueden decir del valor \\(y_4\\)? Sin conocer algo de la química de aminoácidos es dificil arriesgar un rango razonable para \\(y_4\\), pero si asumen intercambialidad podrían decir \\(y_4\\) debe parecerse a los demás valores, ya que no hay razón para pensar que \\(y_4\\) sea especial, por ejemplo para decir algo como \\(y_4 &gt; y_3\\) necesitarían información adicional o supuestos adicionales, como asumir un orden dado. Es decir intercambialidad, implica cierto grado de ignorancia.\nBien, ahora supongamos que les muestro 9 valores, excepto \\(y_4\\), qué pueden decir de \\(y_4\\)?\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\(y_1\\)\n\\(y_2\\)\n\\(y_3\\)\n\\(y_5\\)\n\\(y_6\\)\n\\(y_7\\)\n\\(y_8\\)\n\\(y_9\\)\n\\(y_{10}\\)\n\n\n\n\n58.27\n56.18\n56.84\n54.64\n54.2\n57.51\n44.45\n61.46\n54.45\n\n\n\nPodrían decir que es posible que \\(y_4\\) esté comprendido en ese rango y si están dispuestos a asumir normalidad, entonces pueden decir algo más como que es probable que \\(y_4\\) tenga una valor de 54.9 (la media de los otros valores) y sería extraño que tenga una valor inferior a \\(\\approx 41\\) y superior a \\(\\approx 68\\) (\\(54.9 \\pm 3 \\text{ desvíos estándar}\\)). Esta estimación es válida incluso si reordenamos los valores de la tabla anterior. Estamos asumiendo que \\(y_4\\) es intercambiable, pero no independiente, ya que de alguna forma el valor de \\(y_4\\) está “informado” por los otros valores.\nAlguien podría objetar que este análisis no es válido, ya que existen distintos tipos de aminoácidos y en algunos casos los desplazamientos pueden ser muy diferentes para estos distintos tipos. En ese caso podemos responder de al menos 2 formas.\n\nSi solo tenemos los valores pero no los “rótulos” (es decir no sabemos a que aminoácido corresponde cada valor), entonces no tenemos otra opción que asumir intercambialidad.\nSi tenemos los rótulos, entonces podemos incorporar esa información y hacer un análisis jerárquico.\n\nAmbas opciones están justificadas teóricamente. La diferencia es la información disponible.\nLuego de estas discusión teórica y conceptual veamos algunos otros ejemplos prácticos de modelos jerárquicos.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Modelado Jerárquico</span>"
    ]
  },
  {
    "objectID": "03_Modelos_jerárquicos.html#renacuajos-multinivel",
    "href": "03_Modelos_jerárquicos.html#renacuajos-multinivel",
    "title": "3  Modelado Jerárquico",
    "section": "3.7 Renacuajos multinivel",
    "text": "3.7 Renacuajos multinivel\nEste ejemplo está tomado de statistical rethinking\n\nTenemos 48 tanques llenos de renacuajos\nQueremos modelar la probabilidad de supervivencia de los renacuajos\nLas condiciones como la temperatura, el pH, la luz, etc. varían ligeramente entre los tanques (pero no los estamos teniendo en cuenta explícitamente)\nPodemos pensar en cada tanque como un grupo\n\n\nd = pd.read_csv('datos/renacuajos.csv', sep=\",\")\ncoords = {\"tanks\": list(d.index)}\nd.head()\n\n\n\n\n\n\n\n\n\ndensity\npred\nsize\nsurv\npropsurv\n\n\n\n\n0\n10\nno\nbig\n9\n0.9\n\n\n1\n10\nno\nbig\n10\n1.0\n\n\n2\n10\nno\nbig\n7\n0.7\n\n\n3\n10\nno\nbig\n10\n1.0\n\n\n4\n10\nno\nsmall\n9\n0.9\n\n\n\n\n\n\n\n\n\nwith pm.Model(coords=coords) as modelo_renacuajos:\n    # Hiperpriors\n    μ = pm.Normal('μ', 0., 2.)\n    σ = pm.HalfNormal('σ', 2.)\n    # Prior\n    α_tanque = pm.Normal('α_tanque', μ, σ,  dims=\"tanks\")\n    p = pm.Deterministic('p', pm.math.sigmoid(α_tanque))  # transformación logística\n    #likelihood\n    surv = pm.Binomial('surv', n=d.density, p=p, observed=d.surv)\n    \n    idata_renacuajos = pm.sample(2000, tune=2000)\n\nAuto-assigning NUTS sampler...\nInitializing NUTS using jitter+adapt_diag...\nMultiprocess sampling (4 chains in 4 jobs)\nNUTS: [μ, σ, α_tanque]\nSampling 4 chains for 2_000 tune and 2_000 draw iterations (8_000 + 8_000 draws total) took 49 seconds.\n\n\n\n\n\n\n\n\n\n\n\n\npm.model_to_graphviz(modelo_renacuajos)\n\n\n\n\n\n\n\n\nEn la siguiente figura se muestran las proporciones empíricas de sobrevivientes en cada tanque de renacuajos (puntos azules) y las proporciones estimadas por el modelo (puntos turquesa). La línea discontinua indica la proporción promedio de sobrevivientes teniendo en cuenta todos los tanques. Las lineas verticales dividen los tanques de acuerdo a las diferentes densidades iniciales de renacuajos: tanques pequeños (10), tanques medianos (25) y tanques grandes (35). En cada tanque, la media a posteriori del modelo multinivel está más cerca de la línea punteada que la proporción empírica. Esto refleja la información compartida entre tanques y el efecto de contracción.\n\n_, ax = plt.subplots(1, 1, figsize=(12, 5))\n\npost_r = az.extract(idata_renacuajos)\n\nax.scatter(np.arange(0, 48), d.propsurv, color='C0')\nax.scatter(np.arange(0, 48), post_r['p'].mean(\"sample\"), color='C1')\nax.hlines(logistic(post_r['μ'].median(\"sample\")), -1, 49, linestyles='--')\n\nidx = d.density[d.density.diff() &gt; 0].index\nax.vlines(idx + 0.5, -.05, 1.05, lw=.5)\nfor i, t in zip(np.linspace(0, 48, 7)[1::2], ('pequeño', 'mediano', 'grande')):\n    ax.text(i, 0, t, horizontalalignment='center')\nax.set_xlabel('tanques')\nax.set_ylabel('proporción de survivencia')\nax.set_xlim(-1, 48)\nax.set_xticks([])\nax.set_ylim(-.05, 1.05)\nax.grid(False)",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Modelado Jerárquico</span>"
    ]
  },
  {
    "objectID": "03_Modelos_jerárquicos.html#jerarquías-futboleras",
    "href": "03_Modelos_jerárquicos.html#jerarquías-futboleras",
    "title": "3  Modelado Jerárquico",
    "section": "3.8 Jerarquías futboleras",
    "text": "3.8 Jerarquías futboleras\nVarias estructuras de datos se prestan a descripciones jerárquicas que pueden abarcar múltiples niveles. Por ejemplo, jugadores profesionales de fútbol. Como en muchos otros deportes, los jugadores tienen diferentes posiciones dentro de la cancha. Es posible que nos interese estimar algunas métricas de habilidad para cada jugador, para las posiciones y para el grupo general de jugadores de fútbol profesional. Este tipo de estructura jerárquica también se puede encontrar en muchos otros dominios, como:\n\nInvestigación médica: Supongamos que estamos interesados en estimar la eficacia de diferentes fármacos para el tratamiento de una determinada enfermedad. Podemos categorizar a los pacientes según su información demográfica, la gravedad de la enfermedad y otros factores relevantes y construir un modelo jerárquico para estimar la probabilidad de curación o el éxito del tratamiento para cada subgrupo. Luego podemos usar los parámetros de la distribución de subgrupos para estimar la probabilidad general de curación o éxito del tratamiento para toda la población de pacientes.\nCiencias ambientales: Supongamos que estamos interesados en estimar el impacto de un determinado contaminante en un ecosistema particular. Podemos categorizar diferentes hábitats dentro del ecosistema (p. ej., ríos, lagos, bosques, humedales) y construir un modelo jerárquico para estimar la distribución de los niveles de contaminantes dentro de cada hábitat. Luego podemos usar los parámetros de la distribución del hábitat para estimar la distribución general de los niveles de contaminantes dentro del ecosistema.\nInvestigación de mercado: supongamos que estamos interesados en comprender el comportamiento de compra de los consumidores de un producto en particular en diferentes regiones. Podemos categorizar a los consumidores según su información demográfica (por ejemplo, edad, sexo, ingresos, educación) y construir un modelo jerárquico para estimar la distribución del comportamiento de compra para cada subgrupo. Luego podemos usar los parámetros de la distribución del subgrupo para estimar la distribución del comportamiento de compra para el grupo general de consumidores.\n\nVolviendo a nuestros jugadores de fútbol, hemos recopilado datos de la Premier League, Ligue 1, Bundesliga, Serie A y La Liga, en el transcurso de cuatro años (2017 a 2020). Supongamos que estamos interesados en la métrica de goles por tiro. Esto es lo que los estadísticos suelen llamar tasa de éxito, y podemos estimarlo con un modelo Binomial donde el parámetro \\(n\\) es el número de tiros y las observaciones \\(y\\) es el número de goles. Esto nos deja con un valor desconocido para \\(p\\), en ejemplos anteriores hemos estado llamando a este parámetro \\(\\theta\\) y hemos usado una distribución Beta para modelarlo. Haremos lo mismo ahora, pero jerárquicamente. \\(\\theta\\) representa la “tasa de éxito” de cada jugador y, por lo tanto, es un vector de tamaño n_jugadores. Usamos una distribución Beta para modelar \\(\\theta\\). Los hiperparámetros de la distribución Beta serán los vectores \\(\\mu_p\\) y \\(\\nu_p\\), que son vectores de tamaño 4, que representan las cuatro posiciones en nuestro conjunto de datos (defensor DF, centrocampista MF, delantero FW y arquero GK). Tendremos que indexar correctamente los vectores \\(\\mu_p\\) y \\(\\nu_p\\) para que coincidan con el número total de jugadores. Finalmente, tendremos dos parámetros globales, \\(\\mu\\) y \\(\\nu\\), que representan a los futbolistas profesionales.\nEl modelo PyMC se define en el siguiente bloque de código. El pm.Beta('mu', 1.7, 5.8) fue elegido con la ayuda de PreliZ como prior con el 90% de la masa entre 0 y 0.5. Este es un ejemplo de un prior poco informativo, ya que no hay duda de que una tasa de éxito de 0,5 es un valor alto. Las estadísticas deportivas están bien estudiadas y hay mucha información previa que podría usarse para definir priors más fuertes. Para este ejemplo, nos conformaremos con este prior. Una justificación similar se puede hacer para el prior pm.Gamma('nu', mu=125, sigma=50), que definimos como la distribución Gamma de máxima entropía con el 90% de la masa entre 50 y 200.\n\nfutbol = pd.read_csv(\"datos/futbol.csv\", dtype={'posición':'category'})\npos_idx = futbol.posición.cat.codes.values\npos_codes = futbol.posición.cat.categories\nn_pos = pos_codes.size\nn_jugadores = futbol.index.size\n\n\ncoords = {\"pos\": pos_codes}\nwith pm.Model(coords=coords) as modelo_futbol:\n    # Hiper-parámetros\n    μ = pm.Beta('μ', 1.7, 5.8) \n    ν = pm.Gamma('ν', mu=125, sigma=50)\n\n    \n    # Parámetros por posición\n    μ_p = pm.Beta('μ_p',\n                       mu=μ,\n                       nu=ν,\n                       dims = \"pos\")\n    \n    ν_p = pm.Gamma('ν_p', mu=125, sigma=50, dims=\"pos\")\n \n    # Parámetros por jugador\n    θ = pm.Beta('θ', \n                    mu=μ_p[pos_idx],\n                    nu=ν_p[pos_idx])\n    \n    _ = pm.Binomial('gs', n=futbol.tiros.values, p=θ, observed=futbol.goles.values)\n\n    idata_futbol = pm.sample(4000, target_accept=0.98)\n\nAuto-assigning NUTS sampler...\nInitializing NUTS using jitter+adapt_diag...\nMultiprocess sampling (4 chains in 4 jobs)\nNUTS: [μ, ν, μ_p, ν_p, θ]\nSampling 4 chains for 1_000 tune and 4_000 draw iterations (4_000 + 16_000 draws total) took 358 seconds.\nThe rhat statistic is larger than 1.01 for some parameters. This indicates problems during sampling. See https://arxiv.org/abs/1903.08008 for details\nThe effective sample size per chain is smaller than 100 for some parameters.  A higher number is needed for reliable rhat and ess computation. See https://arxiv.org/abs/1903.08008 for details\n\n\n\n\n\n\n\n\n\n\n\n\npm.model_to_graphviz(modelo_futbol)\n\n\n\n\n\n\n\n\nEn el panel superior de la siguiente figura tenemos la distribución a posteriori del parámetro global \\(\\mu\\). La distribución a posteriori es cercana a 0.1. Lo que significa que, en general, para un jugador de fútbol profesional, la probabilidad de hacer un gol es en promedio del 10%. Este es un valor razonable, ya que hacer goles no es una tarea fácil y no estamos discriminando posiciones, es decir, estamos considerando jugadores cuyo papel principal no es el de hacer goles. En el panel central, tenemos el valor estimado de \\(mu_p\\) para la posición de defensa, como es de esperar, es más alto que el parámetro global \\(\\mu\\). En el panel inferior, tenemos el valor estimado de \\(\\theta\\) para Lionel Messi, con un valor de 0.17, que es más alto que el parámetro global \\(\\mu\\) y el valor de la posición delantera \\(\\mu_p\\). Esto también es de esperarse, ya que Lionel Messi es el mejor jugador de fútbol del mundo, y su rol principal es hacer goles.\n\n_, ax = plt.subplots(3, 1, figsize=(12, 7), sharex=True)\naz.plot_posterior(idata_futbol, var_names='μ', ax=ax[0])\nax[0].set_title(r\"Global mean\")\naz.plot_posterior(idata_futbol.posterior.sel(pos=\"FW\"), var_names='μ_p', ax=ax[1])\nax[1].set_title(r\"Forward position mean\")\naz.plot_posterior(idata_futbol.posterior.sel(θ_dim_0=1457), var_names='θ', ax=ax[2])\nax[2].set_title(r\"Messi mean\");\n\n\n\n\n\n\n\n\nLa siguiente figura muestra un forest plot para la distribución a posteriori del parámetro \\(\\mu_p\\). La distribución a posteriori para delanteros se centra en torno a 0.13, como ya vimos, y es la más alta de las cuatro. Esto tiene sentido ya que el papel de los jugadores en una posición delantera es hacer goles y asistencias. El valor más bajo de \\(\\mu_p\\) es para la posición de arquero. Esto esperable, ya que la función principal es evitar que el equipo contrario haga goles. El aspecto interesante es que la incertidumbre es muy alta, esto se debe a que tenemos muy pocos arqueros haciendo goles en nuestro conjunto de datos, 3 para ser precisos. Las distribuciones a posteriori para las posiciones de defensa y mediocampo están en el medio, siendo ligeramente más altas para los mediocampistas. Podemos explicar esto porque el papel principal de un mediocampista es defender y atacar, y por lo tanto la probabilidad de marcar un gol es mayor que la de un defensor pero menor que la de un delantero.\n\naz.plot_forest(idata_futbol, var_names=['μ_p'], combined=True, figsize=(12, 3));",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Modelado Jerárquico</span>"
    ]
  },
  {
    "objectID": "03_Modelos_jerárquicos.html#resumen",
    "href": "03_Modelos_jerárquicos.html#resumen",
    "title": "3  Modelado Jerárquico",
    "section": "3.9 Resumen",
    "text": "3.9 Resumen\nEn este capítulo hemos descrito uno de los conceptos más importantes de este curso: los modelos jerárquicos. Podemos construir modelos jerárquicos cada vez que podamos identificar subgrupos en nuestros datos. En tales casos, en lugar de tratar los subgrupos como entidades separadas o ignorar los subgrupos y tratarlos como un solo gran-grupo, podemos construir un modelo para agrupar-parcialmente la información entre los grupos.\nEl principal efecto de este agrupamiento-parcial es que las estimaciones de cada subgrupo estarán sesgadas por las estimaciones del resto de los subgrupos. Este efecto se conoce como contracción y, en general, es un truco muy útil que ayuda a mejorar las inferencias haciéndolas más conservadoras (ya que cada subgrupo informa a los demás acercando el resto de las estimaciones hacia él) y más informativas, obtenemos estimaciones a nivel de subgrupo y el nivel del grupo.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Modelado Jerárquico</span>"
    ]
  },
  {
    "objectID": "03_Modelos_jerárquicos.html#para-seguir-leyendo",
    "href": "03_Modelos_jerárquicos.html#para-seguir-leyendo",
    "title": "3  Modelado Jerárquico",
    "section": "3.10 Para seguir leyendo",
    "text": "3.10 Para seguir leyendo\n\nCapítulo 5 de Bayesian Data Analysis de Gelman et al. BDA3\nBernardo, J. M. (1996). The concept of exchangeability and its applications. Far East J. Mathematical Sciences 4, 111-121. Exchangeability",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Modelado Jerárquico</span>"
    ]
  },
  {
    "objectID": "03_Modelos_jerárquicos.html#ejercicios",
    "href": "03_Modelos_jerárquicos.html#ejercicios",
    "title": "3  Modelado Jerárquico",
    "section": "3.11 Ejercicios",
    "text": "3.11 Ejercicios\n\n(Borrador). Escribir el teorema de bayes para un modelo jerárquico. La idea es que escriban, likelihood, prior e hyperprior.\n(Borrador). Escribir enunciado tuberías jerarquicas (ver ejemplo en repo privado).\n(Borrador, este ejericio asume que ya vimos el ejemplo de las propinas, si no es así podríamos introducir el ejemplo y pedir que ajusten el modelo jerarquico y no jerarquico). Creá una versión jerárquica para el ejemplo de las propinas agrupando parcialmente los días de la semana.\n(Borrador), Cuando se utilizan distribuciones a priori débilmente informativos, las predicciones medias a posteriori de un modelo normal-normal jerárquico son (aproximadamente) promedios pesado de la siguiente forma:\n\n\\[\n\\frac{\\sigma^2_y}{\\sigma^2_y + n_j \\sigma^2_\\mu} \\overline{y}_{\\text{global}} + \\frac{n_j\\sigma^2_\\mu}{\\sigma^2_y + n_j \\sigma^2_\\mu} \\overline{y}_j.\n\\]\ndonde \\(\\overline{y}_{\\text{global}}\\) es la media de todas las observaciones, \\(\\overline{y}_j\\) es la media de las observaciones en el grupo \\(j\\), \\(n_j\\) es el número de observaciones en el grupo \\(j\\), \\(\\sigma^2_y\\) es la varianza de las observaciones y \\(\\sigma^2_\\mu\\) es la varianza de las medias de los grupos. Indique en que condiciones las prediccions a nivel individual se contraeran más hacia la predicción global.\n\nAplicá al menos uno de los modelos visto en este capítulo a datos propios o de tu interés.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Modelado Jerárquico</span>"
    ]
  }
]